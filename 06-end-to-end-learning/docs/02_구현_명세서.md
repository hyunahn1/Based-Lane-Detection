# Module 06: End-to-End Learning - 구현 명세서 v1.0

**작성일:** 2026-01-30  
**버전:** 1.0  
**목표:** ViT 기반 Image-to-Control E2E 상세 구현

---

## 1. 디렉토리 구조

```
06-end-to-end-learning/
├── docs/
│   ├── 01_아키텍처_설계서.md
│   ├── 02_구현_명세서.md
│   └── 03_검증서.md
│
├── src/
│   ├── __init__.py
│   │
│   ├── models/
│   │   ├── __init__.py
│   │   ├── vit.py                # Vision Transformer
│   │   ├── control_head.py       # Control output head
│   │   └── e2e_model.py          # Complete E2E model
│   │
│   ├── data/
│   │   ├── __init__.py
│   │   ├── dataset.py            # Driving dataset
│   │   └── augmentation.py       # Data augmentation
│   │
│   ├── training/
│   │   ├── __init__.py
│   │   ├── trainer.py            # Training loop
│   │   └── metrics.py            # Evaluation metrics
│   │
│   └── inference/
│       ├── __init__.py
│       └── predictor.py          # Real-time inference
│
├── config/
│   └── train_config.yaml         # Training configuration
│
├── train.py                       # Training script
├── evaluate.py                    # Evaluation script
├── test_basic.py                  # Basic functionality test
├── requirements.txt
└── README.md
```

---

## 2. 핵심 컴포넌트 구현

### 2.1 Vision Transformer (`vit.py`)

```python
import torch
import torch.nn as nn
from einops import rearrange

class PatchEmbedding(nn.Module):
    """
    Image를 patch로 나누고 embedding
    """
    def __init__(
        self,
        img_size: int = 224,
        patch_size: int = 16,
        in_channels: int = 3,
        embed_dim: int = 768
    ):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2  # 196
        
        # Conv로 patch embedding (효율적)
        self.projection = nn.Conv2d(
            in_channels,
            embed_dim,
            kernel_size=patch_size,
            stride=patch_size
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: (B, 3, 224, 224)
        return: (B, 196, 768)
        """
        x = self.projection(x)  # (B, 768, 14, 14)
        x = rearrange(x, 'b c h w -> b (h w) c')  # (B, 196, 768)
        return x


class MultiHeadAttention(nn.Module):
    """
    Multi-Head Self-Attention
    """
    def __init__(
        self,
        embed_dim: int = 768,
        num_heads: int = 12,
        dropout: float = 0.0
    ):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        assert embed_dim % num_heads == 0
        
        self.qkv = nn.Linear(embed_dim, embed_dim * 3)
        self.proj = nn.Linear(embed_dim, embed_dim)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: (B, N, D)
        return: (B, N, D)
        """
        B, N, D = x.shape
        
        # QKV projection
        qkv = self.qkv(x)  # (B, N, 3*D)
        qkv = rearrange(qkv, 'b n (three h d) -> three b h n d',
                       three=3, h=self.num_heads)
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        # Attention
        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn = attn.softmax(dim=-1)
        attn = self.dropout(attn)
        
        # Aggregate
        out = attn @ v  # (B, H, N, D/H)
        out = rearrange(out, 'b h n d -> b n (h d)')
        out = self.proj(out)
        
        return out


class TransformerBlock(nn.Module):
    """
    Transformer Encoder Block
    """
    def __init__(
        self,
        embed_dim: int = 768,
        num_heads: int = 12,
        mlp_ratio: float = 4.0,
        dropout: float = 0.0
    ):
        super().__init__()
        
        self.norm1 = nn.LayerNorm(embed_dim)
        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)
        
        self.norm2 = nn.LayerNorm(embed_dim)
        mlp_hidden_dim = int(embed_dim * mlp_ratio)
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, mlp_hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_hidden_dim, embed_dim),
            nn.Dropout(dropout)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Attention with residual
        x = x + self.attn(self.norm1(x))
        
        # MLP with residual
        x = x + self.mlp(self.norm2(x))
        
        return x


class VisionTransformer(nn.Module):
    """
    Vision Transformer (ViT) for image encoding
    """
    def __init__(
        self,
        img_size: int = 224,
        patch_size: int = 16,
        in_channels: int = 3,
        embed_dim: int = 768,
        depth: int = 12,
        num_heads: int = 12,
        mlp_ratio: float = 4.0,
        dropout: float = 0.0
    ):
        super().__init__()
        
        # Patch embedding
        self.patch_embed = PatchEmbedding(
            img_size, patch_size, in_channels, embed_dim
        )
        num_patches = self.patch_embed.num_patches
        
        # CLS token
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        
        # Position embedding
        self.pos_embed = nn.Parameter(
            torch.zeros(1, num_patches + 1, embed_dim)
        )
        self.pos_drop = nn.Dropout(dropout)
        
        # Transformer blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)
            for _ in range(depth)
        ])
        
        self.norm = nn.LayerNorm(embed_dim)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: (B, 3, 224, 224)
        return: (B, 768) - CLS token features
        """
        B = x.shape[0]
        
        # Patch embedding
        x = self.patch_embed(x)  # (B, 196, 768)
        
        # Add CLS token
        cls_token = self.cls_token.expand(B, -1, -1)  # (B, 1, 768)
        x = torch.cat([cls_token, x], dim=1)  # (B, 197, 768)
        
        # Add position embedding
        x = x + self.pos_embed
        x = self.pos_drop(x)
        
        # Transformer blocks
        for block in self.blocks:
            x = block(x)
        
        x = self.norm(x)
        
        # Return CLS token
        return x[:, 0]  # (B, 768)
```

### 2.2 Control Head (`control_head.py`)

```python
import torch
import torch.nn as nn

class ControlHead(nn.Module):
    """
    Control output head
    Features → [Steering, Throttle]
    """
    def __init__(
        self,
        embed_dim: int = 768,
        hidden_dim: int = 256,
        dropout: float = 0.1
    ):
        super().__init__()
        
        self.head = nn.Sequential(
            nn.Linear(embed_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 2)  # [steering, throttle]
        )
    
    def forward(self, features: torch.Tensor) -> torch.Tensor:
        """
        features: (B, 768)
        return: (B, 2) - [steering, throttle]
        """
        output = self.head(features)  # (B, 2)
        
        # Steering: tanh [-1, 1]
        # Throttle: sigmoid [0, 1]
        steering = torch.tanh(output[:, 0:1])
        throttle = torch.sigmoid(output[:, 1:2])
        
        return torch.cat([steering, throttle], dim=1)
```

### 2.3 Complete E2E Model (`e2e_model.py`)

```python
import torch
import torch.nn as nn
from .vit import VisionTransformer
from .control_head import ControlHead

class EndToEndModel(nn.Module):
    """
    Complete End-to-End model
    Image → ViT → Control
    """
    def __init__(
        self,
        img_size: int = 224,
        patch_size: int = 16,
        embed_dim: int = 768,
        depth: int = 12,
        num_heads: int = 12,
        dropout: float = 0.1
    ):
        super().__init__()
        
        # Vision encoder
        self.encoder = VisionTransformer(
            img_size=img_size,
            patch_size=patch_size,
            embed_dim=embed_dim,
            depth=depth,
            num_heads=num_heads,
            dropout=dropout
        )
        
        # Control head
        self.control_head = ControlHead(
            embed_dim=embed_dim,
            dropout=dropout
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: (B, 3, 224, 224)
        return: (B, 2) - [steering, throttle]
        """
        features = self.encoder(x)  # (B, 768)
        control = self.control_head(features)  # (B, 2)
        return control
    
    def get_attention_maps(self, x: torch.Tensor) -> list:
        """
        Extract attention maps for visualization
        """
        attention_maps = []
        
        # Patch embedding
        x = self.encoder.patch_embed(x)
        B = x.shape[0]
        cls_token = self.encoder.cls_token.expand(B, -1, -1)
        x = torch.cat([cls_token, x], dim=1)
        x = x + self.encoder.pos_embed
        
        # Get attention from each block
        for block in self.encoder.blocks:
            attn = block.attn
            qkv = attn.qkv(block.norm1(x))
            qkv = qkv.reshape(B, -1, 3, attn.num_heads, attn.head_dim)
            qkv = qkv.permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]
            
            attn_weights = (q @ k.transpose(-2, -1)) / (attn.head_dim ** 0.5)
            attn_weights = attn_weights.softmax(dim=-1)
            
            attention_maps.append(attn_weights.detach())
            
            # Forward through block
            x = block(x)
        
        return attention_maps
```

---

## 3. 데이터 로더 (`dataset.py`)

```python
import torch
from torch.utils.data import Dataset
import numpy as np
from PIL import Image
import albumentations as A
from albumentations.pytorch import ToTensorV2

class DrivingDataset(Dataset):
    """
    Driving dataset: (image, steering, throttle)
    """
    def __init__(
        self,
        image_paths: list,
        steering_values: list,
        throttle_values: list,
        transform=None
    ):
        self.image_paths = image_paths
        self.steering = np.array(steering_values, dtype=np.float32)
        self.throttle = np.array(throttle_values, dtype=np.float32)
        self.transform = transform
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # Load image
        image = Image.open(self.image_paths[idx]).convert('RGB')
        image = np.array(image)
        
        # Transform
        if self.transform:
            transformed = self.transform(image=image)
            image = transformed['image']
        
        # Control
        steering = self.steering[idx]
        throttle = self.throttle[idx]
        control = np.array([steering, throttle], dtype=np.float32)
        
        return image, torch.from_numpy(control)


def get_transforms(mode='train'):
    """
    Get data augmentation transforms
    """
    if mode == 'train':
        return A.Compose([
            A.Resize(224, 224),
            A.RandomBrightnessContrast(
                brightness_limit=0.2,
                contrast_limit=0.2,
                p=0.5
            ),
            A.HorizontalFlip(p=0.5),
            A.GaussianBlur(blur_limit=(3, 5), p=0.3),
            A.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            ),
            ToTensorV2()
        ])
    else:
        return A.Compose([
            A.Resize(224, 224),
            A.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            ),
            ToTensorV2()
        ])
```

---

## 4. Training Script (`train.py`)

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path

from src.models.e2e_model import EndToEndModel
from src.data.dataset import DrivingDataset, get_transforms

def train_epoch(model, dataloader, optimizer, criterion, device):
    """Single training epoch"""
    model.train()
    total_loss = 0
    
    for images, controls in dataloader:
        images = images.to(device)
        controls = controls.to(device)
        
        # Forward
        predictions = model(images)
        
        # Loss (separate for steering and throttle)
        steering_loss = criterion(predictions[:, 0], controls[:, 0])
        throttle_loss = criterion(predictions[:, 1], controls[:, 1])
        loss = steering_loss + 0.5 * throttle_loss
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    return total_loss / len(dataloader)


def validate(model, dataloader, criterion, device):
    """Validation"""
    model.eval()
    total_loss = 0
    steering_errors = []
    throttle_errors = []
    
    with torch.no_grad():
        for images, controls in dataloader:
            images = images.to(device)
            controls = controls.to(device)
            
            predictions = model(images)
            
            # Loss
            steering_loss = criterion(predictions[:, 0], controls[:, 0])
            throttle_loss = criterion(predictions[:, 1], controls[:, 1])
            loss = steering_loss + 0.5 * throttle_loss
            
            total_loss += loss.item()
            
            # Errors
            steering_errors.extend(
                torch.abs(predictions[:, 0] - controls[:, 0]).cpu().tolist()
            )
            throttle_errors.extend(
                torch.abs(predictions[:, 1] - controls[:, 1]).cpu().tolist()
            )
    
    return {
        'loss': total_loss / len(dataloader),
        'steering_mae': sum(steering_errors) / len(steering_errors),
        'throttle_mae': sum(throttle_errors) / len(throttle_errors)
    }
```

---

## 5. 평가 메트릭 (`metrics.py`)

```python
import numpy as np

def calculate_metrics(predictions, targets):
    """
    Calculate control prediction metrics
    """
    pred_steering = predictions[:, 0]
    pred_throttle = predictions[:, 1]
    gt_steering = targets[:, 0]
    gt_throttle = targets[:, 1]
    
    metrics = {
        'steering_mae': np.mean(np.abs(pred_steering - gt_steering)),
        'steering_rmse': np.sqrt(np.mean((pred_steering - gt_steering) ** 2)),
        'throttle_mae': np.mean(np.abs(pred_throttle - gt_throttle)),
        'throttle_rmse': np.sqrt(np.mean((pred_throttle - gt_throttle) ** 2)),
    }
    
    return metrics
```

---

## 6. Configuration (`config/train_config.yaml`)

```yaml
model:
  img_size: 224
  patch_size: 16
  embed_dim: 768
  depth: 12
  num_heads: 12
  dropout: 0.1

training:
  epochs: 100
  batch_size: 32
  num_workers: 4
  
  optimizer:
    name: AdamW
    lr: 1e-4
    weight_decay: 0.05
  
  scheduler:
    name: CosineAnnealingLR
    T_max: 100
    eta_min: 1e-6
  
  loss:
    steering_weight: 1.0
    throttle_weight: 0.5

data:
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1

paths:
  data_dir: ./data
  checkpoint_dir: ./checkpoints
  log_dir: ./logs
```

---

## 7. 핵심 파라미터

### 7.1 Model Parameters

| Parameter | Value | Note |
|-----------|-------|------|
| Image Size | 224×224 | ViT standard |
| Patch Size | 16×16 | 196 patches |
| Embed Dim | 768 | ViT-Base |
| Depth | 12 layers | ViT-Base |
| Num Heads | 12 | Multi-head attention |
| MLP Ratio | 4.0 | FFN expansion |

**Total Parameters:** ~86M

### 7.2 Training Parameters

| Parameter | Value |
|-----------|-------|
| Batch Size | 32 |
| Learning Rate | 1e-4 |
| Weight Decay | 0.05 |
| Epochs | 100 |
| Optimizer | AdamW |
| Scheduler | CosineAnnealingLR |

---

## 8. 예상 성능

### 8.1 Control Accuracy

| Metric | Expected | Excellent |
|--------|----------|-----------|
| Steering MAE | < 0.10 rad | < 0.05 rad |
| Steering RMSE | < 0.15 rad | < 0.08 rad |
| Throttle MAE | < 0.10 | < 0.05 |
| Throttle RMSE | < 0.15 | < 0.08 |

### 8.2 Inference Speed

| Hardware | FPS | Latency |
|----------|-----|---------|
| RTX 5090 | 100+ | <10ms |
| RTX 3080 | 60+ | <17ms |
| Jetson AGX | 20+ | <50ms |

---

**작성자:** AI Development Team  
**날짜:** 2026-01-30  
**Status:** Implementation Spec Complete  
**Next:** Verification Plan
