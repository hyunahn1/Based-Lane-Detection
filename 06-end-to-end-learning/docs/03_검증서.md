# Module 06: End-to-End Learning - 검증서 v1.0

**작성일:** 2026-01-30  
**버전:** 1.0  
**목표:** ViT 기반 E2E 모델 검증 및 성능 평가

---

## 1. 검증 전략

### 1.1 검증 목표

1. **기능 검증** - 모든 컴포넌트가 정상 작동하는가?
2. **성능 검증** - Control accuracy가 목표치를 달성하는가?
3. **효율성 검증** - Real-time inference 가능한가?
4. **해석성 검증** - Attention visualization 작동하는가?

### 1.2 검증 레벨

```
Level 1: Unit Tests
  ├─ Vision Transformer
  ├─ Control Head
  ├─ Dataset Loader
  └─ Metrics

Level 2: Integration Tests
  ├─ End-to-End model
  ├─ Training loop
  └─ Inference pipeline

Level 3: Performance Tests
  ├─ Control accuracy
  ├─ Inference speed
  └─ Memory usage

Level 4: Qualitative Analysis
  ├─ Attention visualization
  ├─ Failure case analysis
  └─ Driving behavior
```

---

## 2. KPI (Key Performance Indicators)

### 2.1 Control Accuracy

| Metric | Minimum | Target | Excellent |
|--------|---------|--------|-----------|
| **Steering MAE** | < 0.15 rad | < 0.10 rad | < 0.05 rad |
| **Steering RMSE** | < 0.20 rad | < 0.15 rad | < 0.08 rad |
| **Throttle MAE** | < 0.15 | < 0.10 | < 0.05 |
| **Throttle RMSE** | < 0.20 | < 0.15 | < 0.08 |

### 2.2 Inference Performance

| Metric | Minimum | Target | Excellent |
|--------|---------|--------|-----------|
| **FPS (RTX 5090)** | > 50 | > 80 | > 100 |
| **Latency** | < 30ms | < 15ms | < 10ms |
| **GPU Memory** | < 6GB | < 4GB | < 2GB |

### 2.3 Training Efficiency

| Metric | Target |
|--------|--------|
| Convergence | < 50 epochs |
| Training Time | < 6 hours |
| Val Loss Stability | No divergence |

---

## 3. Unit Tests

### 3.1 Vision Transformer Test

```python
def test_vision_transformer():
    """
    ViT 기본 기능 테스트
    """
    from src.models.vit import VisionTransformer
    
    # Model
    model = VisionTransformer(
        img_size=224,
        patch_size=16,
        embed_dim=768,
        depth=12,
        num_heads=12
    )
    
    # Input
    x = torch.randn(2, 3, 224, 224)
    
    # Forward
    features = model(x)
    
    # Check
    assert features.shape == (2, 768)
    assert not torch.isnan(features).any()
    
    # Parameters
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total parameters: {total_params:,}")
    
    assert total_params > 80_000_000  # ~86M
    
    print("✅ PASS: Vision Transformer")
```

### 3.2 Control Head Test

```python
def test_control_head():
    """
    Control head 테스트
    """
    from src.models.control_head import ControlHead
    
    # Model
    head = ControlHead(embed_dim=768)
    
    # Input
    features = torch.randn(4, 768)
    
    # Forward
    control = head(features)
    
    # Check
    assert control.shape == (4, 2)
    
    # Steering: [-1, 1]
    assert (control[:, 0] >= -1).all() and (control[:, 0] <= 1).all()
    
    # Throttle: [0, 1]
    assert (control[:, 1] >= 0).all() and (control[:, 1] <= 1).all()
    
    print("✅ PASS: Control Head")
```

### 3.3 End-to-End Model Test

```python
def test_e2e_model():
    """
    Complete E2E 모델 테스트
    """
    from src.models.e2e_model import EndToEndModel
    
    # Model
    model = EndToEndModel()
    
    # Input
    images = torch.randn(2, 3, 224, 224)
    
    # Forward
    control = model(images)
    
    # Check
    assert control.shape == (2, 2)
    assert (control[:, 0] >= -1).all() and (control[:, 0] <= 1).all()
    assert (control[:, 1] >= 0).all() and (control[:, 1] <= 1).all()
    
    print("✅ PASS: End-to-End Model")
```

### 3.4 Dataset Test

```python
def test_dataset():
    """
    Dataset loader 테스트
    """
    from src.data.dataset import DrivingDataset, get_transforms
    
    # Dummy data
    image_paths = ['img1.jpg', 'img2.jpg']
    steering = [0.1, -0.2]
    throttle = [0.5, 0.7]
    
    # Dataset
    transform = get_transforms('train')
    dataset = DrivingDataset(
        image_paths,
        steering,
        throttle,
        transform=transform
    )
    
    # Check
    assert len(dataset) == 2
    
    # Sample
    image, control = dataset[0]
    assert image.shape == (3, 224, 224)
    assert control.shape == (2,)
    
    print("✅ PASS: Dataset")
```

---

## 4. Integration Tests

### 4.1 Training Loop Test

```python
def test_training_loop():
    """
    Training loop 통합 테스트
    """
    from src.models.e2e_model import EndToEndModel
    from torch.utils.data import TensorDataset, DataLoader
    import torch.optim as optim
    
    # Model
    model = EndToEndModel()
    optimizer = optim.AdamW(model.parameters(), lr=1e-4)
    criterion = nn.MSELoss()
    
    # Dummy data
    images = torch.randn(10, 3, 224, 224)
    controls = torch.randn(10, 2)
    controls[:, 0] = torch.tanh(controls[:, 0])
    controls[:, 1] = torch.sigmoid(controls[:, 1])
    
    dataset = TensorDataset(images, controls)
    dataloader = DataLoader(dataset, batch_size=2)
    
    # Train 1 epoch
    model.train()
    for batch_images, batch_controls in dataloader:
        predictions = model(batch_images)
        loss = criterion(predictions, batch_controls)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    print(f"✅ PASS: Training Loop (loss: {loss.item():.4f})")
```

### 4.2 Inference Pipeline Test

```python
def test_inference():
    """
    Inference pipeline 테스트
    """
    from src.models.e2e_model import EndToEndModel
    import time
    
    model = EndToEndModel()
    model.eval()
    
    # Dummy input
    images = torch.randn(1, 3, 224, 224)
    
    # Warmup
    for _ in range(10):
        _ = model(images)
    
    # Measure latency
    times = []
    with torch.no_grad():
        for _ in range(100):
            start = time.time()
            control = model(images)
            end = time.time()
            times.append((end - start) * 1000)
    
    avg_latency = sum(times) / len(times)
    fps = 1000 / avg_latency
    
    print(f"✅ PASS: Inference")
    print(f"   Latency: {avg_latency:.2f}ms")
    print(f"   FPS: {fps:.1f}")
    
    assert avg_latency < 50  # < 50ms
```

---

## 5. Performance Tests

### 5.1 Control Accuracy Test

```python
def test_control_accuracy(model, test_loader, device):
    """
    Control prediction accuracy 측정
    """
    model.eval()
    
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for images, controls in test_loader:
            images = images.to(device)
            predictions = model(images).cpu()
            
            all_predictions.append(predictions)
            all_targets.append(controls)
    
    predictions = torch.cat(all_predictions, dim=0).numpy()
    targets = torch.cat(all_targets, dim=0).numpy()
    
    # Metrics
    steering_mae = np.mean(np.abs(predictions[:, 0] - targets[:, 0]))
    steering_rmse = np.sqrt(np.mean((predictions[:, 0] - targets[:, 0]) ** 2))
    throttle_mae = np.mean(np.abs(predictions[:, 1] - targets[:, 1]))
    throttle_rmse = np.sqrt(np.mean((predictions[:, 1] - targets[:, 1]) ** 2))
    
    print("Control Accuracy:")
    print(f"  Steering MAE: {steering_mae:.4f}")
    print(f"  Steering RMSE: {steering_rmse:.4f}")
    print(f"  Throttle MAE: {throttle_mae:.4f}")
    print(f"  Throttle RMSE: {throttle_rmse:.4f}")
    
    # Assert targets
    assert steering_mae < 0.15, "Steering MAE too high!"
    assert throttle_mae < 0.15, "Throttle MAE too high!"
    
    print("✅ PASS: Control Accuracy")
```

### 5.2 Inference Speed Test

```python
def test_inference_speed(model, device):
    """
    Inference speed 측정
    """
    model.eval()
    model = model.to(device)
    
    # Dummy input
    images = torch.randn(1, 3, 224, 224).to(device)
    
    # Warmup
    for _ in range(50):
        with torch.no_grad():
            _ = model(images)
    
    # Measure
    torch.cuda.synchronize() if device == 'cuda' else None
    
    import time
    times = []
    
    for _ in range(200):
        start = time.time()
        with torch.no_grad():
            _ = model(images)
        torch.cuda.synchronize() if device == 'cuda' else None
        end = time.time()
        times.append((end - start) * 1000)
    
    avg_latency = sum(times) / len(times)
    fps = 1000 / avg_latency
    
    print("Inference Speed:")
    print(f"  Latency: {avg_latency:.2f}ms")
    print(f"  FPS: {fps:.1f}")
    
    assert fps > 20, "FPS too low!"
    
    print("✅ PASS: Inference Speed")
```

### 5.3 Memory Usage Test

```python
def test_memory_usage(model, device):
    """
    GPU memory usage 측정
    """
    if device != 'cuda':
        print("⚠️ SKIP: Memory test (CPU mode)")
        return
    
    model = model.to(device)
    
    torch.cuda.reset_peak_memory_stats()
    
    # Forward pass
    images = torch.randn(8, 3, 224, 224).to(device)
    with torch.no_grad():
        _ = model(images)
    
    memory_mb = torch.cuda.max_memory_allocated() / 1024 / 1024
    
    print(f"GPU Memory: {memory_mb:.2f}MB")
    
    assert memory_mb < 6000, "Memory usage too high!"
    
    print("✅ PASS: Memory Usage")
```

---

## 6. Attention Visualization Test

```python
def test_attention_visualization(model):
    """
    Attention map visualization 테스트
    """
    model.eval()
    
    # Input
    images = torch.randn(1, 3, 224, 224)
    
    # Get attention maps
    attention_maps = model.get_attention_maps(images)
    
    # Check
    assert len(attention_maps) == 12  # 12 layers
    
    # First layer attention: (B, num_heads, num_tokens, num_tokens)
    first_attn = attention_maps[0]
    assert first_attn.shape == (1, 12, 197, 197)
    
    # Visualize
    import matplotlib.pyplot as plt
    
    # Average over heads
    attn = first_attn[0].mean(dim=0)  # (197, 197)
    
    # CLS token attention to patches
    cls_attn = attn[0, 1:].reshape(14, 14)  # (14, 14)
    
    plt.figure(figsize=(6, 6))
    plt.imshow(cls_attn.numpy(), cmap='hot')
    plt.colorbar()
    plt.title('CLS Token Attention (Layer 1)')
    plt.savefig('attention_map.png')
    
    print("✅ PASS: Attention Visualization")
```

---

## 7. 예상 테스트 결과

### 7.1 Unit Tests

| Test | Expected Result |
|------|-----------------|
| Vision Transformer | ✅ Shape (B, 768), ~86M params |
| Control Head | ✅ Shape (B, 2), bounded outputs |
| E2E Model | ✅ End-to-end forward pass |
| Dataset | ✅ Correct data loading |

### 7.2 Performance Tests

| Metric | Expected Value |
|--------|----------------|
| Steering MAE | 0.08-0.12 rad |
| Throttle MAE | 0.05-0.10 |
| FPS (RTX 5090) | 80-120 |
| GPU Memory | 2-4 GB |

### 7.3 Attention Analysis

**Expected Behavior:**
- Early layers: Local texture patterns
- Middle layers: Object parts (lanes, edges)
- Late layers: Semantic concepts (track, obstacles)

---

## 8. Failure Cases

### 8.1 예상 실패 케이스

1. **Sharp Turns**
   - MAE > 0.20 rad
   - Model trained mostly on straight sections

2. **Low Lighting**
   - Prediction uncertainty increases
   - Attention spread over larger area

3. **Occlusions**
   - Model confused by blocked view
   - Attention shifts to irrelevant regions

### 8.2 완화 전략

1. ✅ Data augmentation (더 다양한 환경)
2. ✅ Ensemble models (여러 모델 평균)
3. ✅ Confidence estimation (불확실시 경고)

---

## 9. Ablation Study

### 9.1 Model Architecture

| Variant | Params | Expected MAE |
|---------|--------|--------------|
| ViT-Small (depth=8) | ~40M | 0.12 |
| **ViT-Base (depth=12)** | **~86M** | **0.10** |
| ViT-Large (depth=24) | ~300M | 0.09 |

### 9.2 Training Strategy

| Strategy | Expected Benefit |
|----------|------------------|
| + ImageNet Pre-training | -20% MAE |
| + MAE Pre-training | -15% MAE |
| + Data Augmentation | -10% MAE |
| + Ensemble (3 models) | -25% MAE |

---

## 10. 벤치마크 비교

| Method | Steering MAE | Throttle MAE | FPS |
|--------|--------------|--------------|-----|
| CNN (ResNet-18) | 0.15 | 0.12 | 150 |
| CNN (ResNet-50) | 0.12 | 0.10 | 100 |
| **ViT-Base (Ours)** | **0.10** | **0.08** | **80** |
| Swin-Base | 0.09 | 0.07 | 60 |

---

## 11. 체크리스트

### 11.1 구현 완성도

- [ ] Vision Transformer 구현
- [ ] Control Head 구현
- [ ] E2E Model 구현
- [ ] Dataset Loader 구현
- [ ] Training Script 구현
- [ ] Evaluation Script 구현

### 11.2 테스트 커버리지

- [ ] Unit Tests (4개)
- [ ] Integration Tests (2개)
- [ ] Performance Tests (3개)
- [ ] Attention Visualization

### 11.3 성능 검증

- [ ] Steering MAE < 0.15
- [ ] Throttle MAE < 0.15
- [ ] FPS > 50
- [ ] GPU Memory < 6GB

### 11.4 문서화

- [ ] Architecture Doc
- [ ] Implementation Spec
- [ ] Verification Plan
- [ ] README

---

## 12. 최종 검증 기준

### 12.1 Pass Criteria

**Minimum (합격):**
- ✅ All unit tests pass
- ✅ Steering MAE < 0.15
- ✅ FPS > 20
- ✅ No runtime errors

**Target (목표):**
- ✅ All tests pass
- ✅ Steering MAE < 0.10
- ✅ FPS > 50
- ✅ Attention visualization works

**Excellent (우수):**
- ✅ All tests pass
- ✅ Steering MAE < 0.05
- ✅ FPS > 80
- ✅ Interpretable attention patterns

---

## 13. 학습 곡선 분석

### 13.1 예상 학습 곡선

```
Epoch 1-10:  Rapid improvement (MAE 0.50 → 0.20)
Epoch 11-30: Steady improvement (MAE 0.20 → 0.12)
Epoch 31-50: Slow improvement (MAE 0.12 → 0.10)
Epoch 51+:   Plateau (MAE ~0.10)
```

### 13.2 조기 종료

```python
EarlyStopping(
    patience=15,
    min_delta=0.001,
    mode='min'
)
```

---

**작성자:** AI Development Team  
**날짜:** 2026-01-30  
**Status:** Verification Plan Complete  
**Next:** Implementation & Testing
