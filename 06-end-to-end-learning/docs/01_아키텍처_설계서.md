# Module 06: End-to-End Learning with Vision Transformer
## 아키텍처 설계서 v1.0

**작성일:** 2026-01-30  
**버전:** 1.0  
**목표:** 2026년 최신 ViT 기술로 Image-to-Control End-to-End 학습

---

## 1. 개요

### 1.1 목적
- **Image → Control 직접 매핑 학습**
- 중간 표현 없이 픽셀에서 조향/속도 직접 출력
- Vision Transformer로 spatial attention 학습

### 1.2 범위
- **입력:** Camera image (RGB)
- **출력:** Steering angle, Throttle
- **학습:** Imitation learning (expert demonstrations)
- **추론:** Real-time end-to-end control

### 1.3 핵심 차별화 (2026년 최신 기술)
1. **Vision Transformer (ViT)** - CNN 대신 순수 Transformer
2. **Hierarchical ViT (Swin)** - 효율적인 multi-scale feature
3. **MAE Pre-training** - Self-supervised 사전 학습
4. **Behavior Cloning** - Expert policy imitation
5. **Attention Visualization** - 어디를 보는지 해석 가능

---

## 2. 시스템 아키텍처

### 2.1 전체 구조

```
┌─────────────────────────────────────────────────────────────┐
│                End-to-End Learning System                    │
├─────────────────────────────────────────────────────────────┤
│                                                               │
│  ┌──────────────┐        ┌──────────────┐                   │
│  │   Camera     │───────►│  Vision      │                   │
│  │   Image      │  RGB   │  Transformer │                   │
│  │  (224×224)   │        │  (ViT/Swin)  │                   │
│  └──────────────┘        └──────┬───────┘                   │
│                                 │                            │
│                                 │ Features                   │
│                                 ▼                            │
│                          ┌──────────────┐                    │
│                          │   Control    │                    │
│                          │    Head      │                    │
│                          │  (MLP)       │                    │
│                          └──────┬───────┘                    │
│                                 │                            │
│                                 ▼                            │
│                          ┌──────────────┐                    │
│                          │   Output     │                    │
│                          │ • Steering   │                    │
│                          │ • Throttle   │                    │
│                          └──────────────┘                    │
│                                                               │
└─────────────────────────────────────────────────────────────┘
```

### 2.2 Vision Transformer Architecture

```
Input Image (3, 224, 224)
    ↓
┌─────────────────────────┐
│  Patch Embedding        │
│  (16×16 patches)        │
│  → 196 tokens           │
└─────────────────────────┘
    ↓
┌─────────────────────────┐
│  + Position Encoding    │
│  + [CLS] token          │
└─────────────────────────┘
    ↓
┌─────────────────────────┐
│  Transformer Encoder    │
│  × 12 layers            │
│  ├─ Multi-Head Attn     │
│  ├─ Layer Norm          │
│  ├─ MLP                 │
│  └─ Residual            │
└─────────────────────────┘
    ↓
┌─────────────────────────┐
│  [CLS] Token            │
│  (768-dim)              │
└─────────────────────────┘
    ↓
┌─────────────────────────┐
│  Control Head           │
│  MLP(768→256→64→2)      │
└─────────────────────────┘
    ↓
Output: [steering, throttle]
```

---

## 3. 핵심 컴포넌트

### 3.1 Vision Transformer (ViT)

**입력:**
- Image: (B, 3, 224, 224)

**Patch Embedding:**
```python
patches = image.unfold(2, 16, 16).unfold(3, 16, 16)
        # (B, 3, 14, 14, 16, 16)
patches = patches.reshape(B, 3, 196, 16*16)
        # (B, 3, 196, 256)
embedding = Linear(3*256, 768)(patches)
        # (B, 196, 768)
```

**Position Encoding:**
- Learnable positional embeddings
- Added to patch embeddings

**Transformer Encoder:**
- 12 layers
- 12 attention heads
- Hidden dim: 768
- MLP dim: 3072

**출력:**
- [CLS] token features: (B, 768)

### 3.2 Control Head

**구조:**
```python
ControlHead(
    Linear(768, 256),
    ReLU(),
    Dropout(0.1),
    Linear(256, 64),
    ReLU(),
    Linear(64, 2)
)
```

**출력:**
- Steering: [-1, 1] (tanh)
- Throttle: [0, 1] (sigmoid)

### 3.3 Alternative: Swin Transformer

**장점:**
- Hierarchical features
- Window-based attention (효율적)
- Better for dense prediction

**구조:**
```
Stage 1: 56×56, dim=96
Stage 2: 28×28, dim=192
Stage 3: 14×14, dim=384
Stage 4: 7×7, dim=768

Global Average Pooling → (768,) → Control Head
```

---

## 4. 학습 전략

### 4.1 Behavior Cloning (Imitation Learning)

**데이터:**
- Expert demonstrations
- (Image, Steering, Throttle) pairs
- 수집: Module 01, 02 사용한 자동 주행

**Loss Function:**
```python
# MSE for continuous control
steering_loss = MSE(pred_steering, gt_steering)
throttle_loss = MSE(pred_throttle, gt_throttle)

# Weighted combination
total_loss = λ1 * steering_loss + λ2 * throttle_loss
           = 1.0 * steering_loss + 0.5 * throttle_loss
```

### 4.2 데이터 수집 전략

**Option 1: Synthetic Data (Module 01 + 02)**
```python
# Use existing modules
lane_detector = Module01()
lane_keeper = Module02()

for image in track_images:
    lanes = lane_detector.detect(image)
    steering = lane_keeper.calculate_steering(lanes)
    
    dataset.append((image, steering, throttle))
```

**Option 2: Human Demonstrations**
- Manual driving
- Record (image, control) pairs

**Target:** 5,000-10,000 samples

### 4.3 Pre-training Strategy

**Option 1: MAE (Masked AutoEncoder)**
```
Self-supervised pre-training:
1. Mask 75% of patches
2. Reconstruct masked patches
3. Learn robust representations
```

**Option 2: ImageNet Pre-trained**
- Use pre-trained ViT-B/16
- Fine-tune on driving data

### 4.4 Training Configuration

```yaml
# Model
model: ViT-B/16
input_size: 224
patch_size: 16
embed_dim: 768
num_layers: 12
num_heads: 12

# Training
epochs: 100
batch_size: 32
optimizer: AdamW
lr: 1e-4
weight_decay: 0.05
scheduler: CosineAnnealingLR

# Data
train_split: 0.8
val_split: 0.1
test_split: 0.1

# Augmentation
random_brightness: 0.2
random_contrast: 0.2
random_flip: 0.5
```

---

## 5. 평가 지표

### 5.1 Control Accuracy

**Steering Error:**
```
MAE_steering = (1/N) * Σ|pred_steering - gt_steering|
RMSE_steering = sqrt((1/N) * Σ(pred_steering - gt_steering)²)
```

**Throttle Error:**
```
MAE_throttle = (1/N) * Σ|pred_throttle - gt_throttle|
```

### 5.2 Driving Performance

**On-track Rate:**
```
Success rate = # successful laps / # total laps
```

**Smoothness:**
```
Jerk = (1/T) * Σ|Δ²steering|
```

### 5.3 Target Performance

| Metric | Target | Excellent |
|--------|--------|-----------|
| Steering MAE | < 0.1 rad | < 0.05 rad |
| Throttle MAE | < 0.1 | < 0.05 |
| Success Rate | > 80% | > 95% |
| FPS | > 20 | > 30 |

---

## 6. 기술 스택

### 6.1 Core Libraries
```python
torch>=2.0.0
torchvision>=0.15.0
timm>=0.9.0  # Vision Transformer models
einops>=0.7.0  # Tensor operations
```

### 6.2 Data & Training
```python
opencv-python>=4.9.0
albumentations>=1.3.0
tensorboard>=2.15.0
wandb>=0.16.0
```

---

## 7. 시스템 요구사항

### 7.1 Hardware
- **GPU:** RTX 3080+ (12GB+)
- **RAM:** 32GB+
- **Storage:** 50GB+ (data + checkpoints)

### 7.2 Software
- Python 3.10+
- CUDA 12.0+
- PyTorch 2.0+

---

## 8. 비교: ViT vs CNN

| Aspect | CNN (ResNet) | ViT | Swin |
|--------|--------------|-----|------|
| **Receptive Field** | Local → Global | Global (all patches) | Hierarchical |
| **Inductive Bias** | Translation equivariance | None (learns from data) | Window locality |
| **Parameters** | ~44M | ~86M | ~88M |
| **FLOPs** | Lower | Higher | Medium |
| **Data Requirement** | Medium | High | Medium-High |
| **Interpretability** | Low | **High (attention)** | High |

**Why ViT for E2E?**
1. ✅ **Global context** - Sees entire image at once
2. ✅ **Attention visualization** - Understand what model looks at
3. ✅ **SOTA performance** - Best results on many tasks
4. ✅ **Transfer learning** - Strong pre-trained models

---

## 9. 리스크 및 완화 전략

### 9.1 Data Scarcity

**Risk:** ViT requires large datasets (typically 100K+ images)

**Mitigation:**
1. ✅ Strong data augmentation
2. ✅ Pre-trained models (ImageNet, MAE)
3. ✅ Synthetic data generation (Module 01+02)
4. ✅ Transfer learning

### 9.2 Overfitting

**Risk:** Model memorizes training tracks

**Mitigation:**
1. ✅ Dropout (0.1-0.2)
2. ✅ Weight decay (0.05)
3. ✅ Early stopping
4. ✅ Data augmentation

### 9.3 Computational Cost

**Risk:** ViT is computationally expensive

**Mitigation:**
1. ✅ Mixed precision training (FP16)
2. ✅ Gradient accumulation
3. ✅ Efficient inference (TorchScript)
4. ✅ Use Swin (more efficient)

---

## 10. 연구 기여

### 10.1 학술적 가치

1. **ViT for Autonomous Driving**
   - RC car scale에 ViT 적용 (신규)
   - Limited data에서 ViT 성능 검증

2. **Behavior Cloning with Transformers**
   - Transformer 기반 imitation learning
   - Attention으로 driving behavior 해석

3. **Sim-to-Real Transfer**
   - Synthetic data (Module 01+02) → Real driving
   - Domain adaptation 연구 가능

### 10.2 실무 가치

1. ✅ End-to-End 학습 경험
2. ✅ Vision Transformer 이해
3. ✅ Imitation Learning 실습
4. ✅ Real-time inference 최적화

---

## 11. 타임라인

### Phase 1: 데이터 수집 (3일)
- ✅ Module 01+02로 synthetic data 생성
- ✅ 5,000-10,000 samples 수집
- ✅ Train/Val/Test split

### Phase 2: 모델 구현 (2일)
- ✅ ViT backbone
- ✅ Control head
- ✅ Training pipeline

### Phase 3: 학습 (3일)
- ✅ Pre-training (optional)
- ✅ Fine-tuning
- ✅ Hyperparameter tuning

### Phase 4: 평가 (2일)
- ✅ Quantitative metrics
- ✅ Qualitative analysis
- ✅ Attention visualization

### Phase 5: 최적화 (2일)
- ✅ Inference speed
- ✅ Model compression
- ✅ Real-time deployment

**Total: 12일 (2주)**

---

## 12. 참고 문헌

1. **Dosovitskiy et al.** (2021). *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.* ICLR.

2. **Liu et al.** (2021). *Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.* ICCV.

3. **He et al.** (2022). *Masked Autoencoders Are Scalable Vision Learners.* CVPR.

4. **Bojarski et al.** (2016). *End to End Learning for Self-Driving Cars.* NVIDIA.

5. **Codevilla et al.** (2018). *End-to-end Driving via Conditional Imitation Learning.* ICRA.

---

**작성자:** AI Development Team  
**날짜:** 2026-01-30  
**Status:** Architecture Design Complete  
**Next:** Implementation Specification
