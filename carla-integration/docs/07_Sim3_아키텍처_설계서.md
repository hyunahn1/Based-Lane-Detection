# Simulation 3: Reinforcement Learning - 아키텍처 설계서

**날짜:** 2026-01-30  
**버전:** 1.0  
**목표:** CARLA 기반 RL Agent with Curiosity Module

---

## 1. 개요

### 1.1 목적
- **연구급 포트폴리오**
- Module 08 (RL + Curiosity) CARLA 통합
- PPO Agent 실시간 제어
- Curiosity-driven Exploration

### 1.2 범위
- **환경:** CARLA Simulator (Gymnasium wrapper)
- **Agent:** PPO with Curiosity (ICM)
- **입력:** Image + Scalars (velocity, steering, etc.)
- **출력:** Steering + Throttle
- **특징:** Real-time RL control

### 1.3 핵심 차별점
```
Traditional (Sim 1):
  Rule-based (PID)
  
End-to-End (Sim 2):
  Supervised Learning
  
Reinforcement Learning (Sim 3):
  Trial-and-error learning ⭐
  Reward-driven
  Curiosity exploration
```

---

## 2. 시스템 아키텍처

### 2.1 전체 구조

```
┌─────────────────────────────────────────────────────────────┐
│                    CARLA Environment                         │
│  ┌────────────┐    ┌────────────┐                           │
│  │  Vehicle   │    │  Camera    │                           │
│  └──────┬─────┘    └──────┬─────┘                           │
└─────────┼────────────────┼─────────────────────────────────┘
          │                │
          │ Control        │ Observation
          │                │
┌─────────▼────────────────▼─────────────────────────────────┐
│              RL Agent (Module 08)                           │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌────────────────────────────────────────────┐            │
│  │         Observation Processing             │            │
│  │  - Image (84×84)                           │            │
│  │  - Velocity, Steering, Previous Actions    │            │
│  └────────────────┬───────────────────────────┘            │
│                   │                                         │
│  ┌────────────────▼───────────────────────────┐            │
│  │         PPO Actor-Critic Network           │            │
│  │  - CNN for image                           │            │
│  │  - MLP for scalars                         │            │
│  │  - Shared layers                           │            │
│  │  - Actor (mean, std)                       │            │
│  │  - Critic (value)                          │            │
│  └────────────────┬───────────────────────────┘            │
│                   │                                         │
│  ┌────────────────▼───────────────────────────┐            │
│  │         Intrinsic Curiosity Module         │            │
│  │  - Feature Network (CNN)                   │            │
│  │  - Forward Model                           │            │
│  │  - Inverse Model                           │            │
│  │  - Intrinsic Reward                        │            │
│  └────────────────┬───────────────────────────┘            │
│                   │                                         │
│                   ▼                                         │
│          Action: [Steering, Throttle]                      │
│          Reward: Extrinsic + Intrinsic                     │
└─────────────────────────────────────────────────────────────┘
```

### 2.2 데이터 플로우

```
CARLA Camera → RGB (640×480)
    ↓ Grayscale + Resize
Grayscale (84×84)
    ↓
Vehicle State → [velocity, steering, prev_action]
    ↓
Observation: {image, scalars}
    ↓ Actor-Critic
Action: [steering, throttle]
    ↓
CARLA Vehicle Control
    ↓
Reward + Next Observation
    ↓ Curiosity Module
Intrinsic Reward
    ↓
Total Reward = Extrinsic + Intrinsic
```

---

## 3. CARLA-Gymnasium Bridge

### 3.1 Gymnasium Environment Wrapper

```python
class CARLAGymEnv(gym.Env):
    """
    CARLA를 Gymnasium 환경으로 래핑
    """
    observation_space: Dict({
        'image': Box(0, 255, (84, 84), uint8),
        'velocity': Box(0, 10, (1,), float32),
        'steering': Box(-1, 1, (1,), float32),
        'prev_action': Box(-1, 1, (2,), float32)
    })
    
    action_space: Box(-1, 1, (2,), float32)
    # [steering, throttle]
```

### 3.2 Reward Function

```python
reward = (
    speed_reward       # 빠르게
    + centering_reward # 중앙 유지
    + smoothness       # 부드러운 제어
    - off_track_penalty # 이탈 방지
    + curiosity_bonus  # 탐험 장려
)
```

---

## 4. Module Integration

### 4.1 Module 08 통합 (RL Agent)

**입력:**
- Observation from CARLA-Gym

**처리:**
```python
1. Actor-Critic forward
2. Sample action (Gaussian)
3. Compute intrinsic reward (ICM)
4. Return action
```

**출력:**
```python
{
    'action': [steering, throttle],
    'value': float,
    'intrinsic_reward': float
}
```

---

## 5. 제어 로직

### 5.1 메인 루프 (Inference Mode)

```python
while simulation_running:
    # 1. Get observation
    obs = carla_gym_env.get_observation()
    
    # 2. RL agent action
    action, value, _ = rl_agent.select_action(
        obs, deterministic=True
    )
    
    # 3. Apply action
    next_obs, reward, done, truncated, info = carla_gym_env.step(action)
    
    # 4. Visualization
    display_value(value)
    display_reward(reward)
```

---

## 6. 성능 목표

### 6.1 Latency

| Component | Target | Max |
|-----------|--------|-----|
| PPO Inference | < 10ms | 20ms |
| Curiosity (optional) | < 5ms | 10ms |
| Environment step | < 20ms | 30ms |
| **Total** | **< 35ms** | **60ms** |

**Target FPS:** 20-30 Hz

---

## 7. 기술 스택

### 7.1 Core

```python
CARLA 0.9.15
Python 3.10+
PyTorch 2.0+
Gymnasium 0.29+
```

### 7.2 Existing Modules

```python
Module 08: /08-reinforcement-learning/src/
  - PPOAgent
  - ActorCritic
  - IntrinsicCuriosityModule
```

---

## 8. 디렉토리 구조

```
carla-integration/
├── docs/
│   ├── 07_Sim3_아키텍처_설계서.md    ← 이 파일
│   ├── 08_Sim3_구현_명세서.md
│   └── 09_Sim3_검증서.md
│
├── sim3-rl/
│   ├── __init__.py
│   ├── main.py                    # 메인 실행
│   ├── carla_gym_env.py           # CARLA-Gym wrapper
│   ├── rl_agent_node.py           # Module 08 wrapper
│   └── config.yaml
│
└── tests/
    └── test_sim3_integration.py
```

---

## 9. 개발 일정

### Phase 1: 문서 (토요일 밤)
- ✅ 아키텍처 설계서
- ✅ 구현 명세서
- ✅ 검증서

### Phase 2: 코드 (토요일 밤)
- ✅ CARLA-Gym bridge
- ✅ Module 08 통합
- ✅ Inference loop

### Phase 3: 실행 (월요일)
- ✅ CARLA 테스트
- ✅ RL control demo

---

## 10. 특징

### Sim 1 (Traditional):
```
Rule-based, PID
명확, 안정적
```

### Sim 2 (E2E):
```
Supervised learning
Image → Control
```

### Sim 3 (RL):
```
Reinforcement Learning ⭐
Trial-and-error
Curiosity-driven
Research-level
```

---

**작성자:** AI Development Team  
**날짜:** 2026-01-30  
**Status:** Architecture Design Complete  
**Next:** Implementation Specification
