# Simulation 3: Reinforcement Learning - Íµ¨ÌòÑ Î™ÖÏÑ∏ÏÑú

**ÎÇ†Ïßú:** 2026-01-30  
**Î≤ÑÏ†Ñ:** 1.0  
**Î™©Ìëú:** RL Agent CARLA ÌÜµÌï© ÏΩîÎìú

---

## 1. CARLA-Gymnasium Bridge (`carla_gym_env.py`)

```python
"""
CARLA-Gymnasium Environment Wrapper
"""
import carla
import gymnasium as gym
from gymnasium import spaces
import numpy as np
import cv2
from typing import Dict, Tuple


class CARLAGymEnv(gym.Env):
    """
    CARLAÎ•º Gymnasium ÌôòÍ≤ΩÏúºÎ°ú ÎûòÌïë
    """
    def __init__(self, carla_interface):
        super().__init__()
        
        self.carla = carla_interface
        
        # Observation space
        self.observation_space = spaces.Dict({
            'image': spaces.Box(0, 255, (84, 84), dtype=np.uint8),
            'velocity': spaces.Box(0, 10, (1,), dtype=np.float32),
            'steering': spaces.Box(-1, 1, (1,), dtype=np.float32),
            'prev_action': spaces.Box(-1, 1, (2,), dtype=np.float32)
        })
        
        # Action space: [steering, throttle]
        self.action_space = spaces.Box(-1, 1, (2,), dtype=np.float32)
        
        # State
        self.prev_action = np.zeros(2, dtype=np.float32)
        self.episode_reward = 0.0
        self.episode_steps = 0
        
        print("‚úÖ CARLA-Gym Environment initialized")
    
    def reset(self, seed=None, options=None):
        """Reset environment"""
        super().reset(seed=seed)
        
        # Reset state
        self.prev_action = np.zeros(2)
        self.episode_reward = 0.0
        self.episode_steps = 0
        
        # Get initial observation
        obs = self._get_observation()
        info = {}
        
        return obs, info
    
    def step(self, action: np.ndarray) -> Tuple[Dict, float, bool, bool, Dict]:
        """
        Execute action in CARLA
        
        Args:
            action: [steering, throttle] in [-1, 1]
        
        Returns:
            obs, reward, terminated, truncated, info
        """
        # Apply action to CARLA
        steering = float(action[0]) * 45.0  # Scale to degrees
        throttle = float(np.clip(action[1], 0, 1))  # Only positive
        
        self.carla.apply_control(steering, throttle)
        
        # Get next observation
        next_obs = self._get_observation()
        
        # Calculate reward
        reward = self._calculate_reward(next_obs)
        
        # Check termination
        terminated = False  # Continuous task
        truncated = self.episode_steps >= 1000  # Max episode length
        
        # Update state
        self.prev_action = action
        self.episode_reward += reward
        self.episode_steps += 1
        
        info = {
            'episode_reward': self.episode_reward,
            'episode_steps': self.episode_steps
        }
        
        return next_obs, reward, terminated, truncated, info
    
    def _get_observation(self) -> Dict:
        """Get current observation"""
        # Get image
        image = self.carla.get_latest_image()
        if image is None:
            image = np.zeros((480, 640, 3), dtype=np.uint8)
        
        # Preprocess: Grayscale + Resize
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        resized = cv2.resize(gray, (84, 84))
        
        # Get vehicle state
        vehicle_state = self.carla.get_vehicle_state()
        velocity = vehicle_state.get('velocity', 0.0)
        
        # Get current steering (from CARLA)
        # For simplicity, use prev_action
        current_steering = self.prev_action[0] if len(self.prev_action) > 0 else 0.0
        
        obs = {
            'image': resized,
            'velocity': np.array([velocity], dtype=np.float32),
            'steering': np.array([current_steering], dtype=np.float32),
            'prev_action': self.prev_action.copy()
        }
        
        return obs
    
    def _calculate_reward(self, obs: Dict) -> float:
        """Calculate reward"""
        # Speed reward
        velocity = obs['velocity'][0]
        speed_reward = velocity / 3.0  # Target: 3 m/s
        
        # Smoothness penalty
        action_diff = np.abs(self.prev_action).sum()
        smoothness_penalty = -0.1 * action_diff
        
        # Total reward
        reward = speed_reward + smoothness_penalty
        
        return float(reward)
```

---

## 2. RL Agent Node (`rl_agent_node.py`)

```python
"""
RL Agent Node (Module 08 Integration)
"""
import sys
from pathlib import Path

# Add Module 08 to path
module08_path = Path(__file__).parent.parent.parent / '08-reinforcement-learning'
sys.path.insert(0, str(module08_path))

import torch
import numpy as np
from typing import Dict, Tuple
import time


class RLAgentNode:
    """
    Module 08 wrapper for CARLA integration
    PPO Agent with Curiosity
    """
    def __init__(
        self,
        checkpoint_path: str,
        device: str = 'cuda'
    ):
        self.device = device
        
        # Load Module 08 agent
        from src.agent.ppo_agent import PPOAgent
        from src.environment.rc_track_env import RCTrackEnv
        
        # Create dummy env for obs/action spaces
        dummy_env = RCTrackEnv()
        
        self.agent = PPOAgent(
            obs_space=dummy_env.observation_space,
            action_space=dummy_env.action_space,
            device=device
        )
        
        if Path(checkpoint_path).exists():
            self.agent.policy.load_state_dict(
                torch.load(checkpoint_path, map_location=device)
            )
            print(f"‚úÖ RL Agent loaded from {checkpoint_path}")
        else:
            print(f"‚ö†Ô∏è Checkpoint not found: {checkpoint_path}")
            print(f"   Using untrained agent")
        
        print(f"‚úÖ RL Agent initialized ({device})")
    
    def select_action(
        self,
        obs: Dict,
        deterministic: bool = True
    ) -> Tuple[np.ndarray, float, float]:
        """
        Select action using PPO policy
        
        Args:
            obs: Observation dict
            deterministic: Use mean action (no exploration)
        
        Returns:
            action, value, log_prob
        """
        start = time.time()
        
        action, log_prob, value = self.agent.select_action(
            obs, deterministic=deterministic
        )
        
        processing_time = (time.time() - start) * 1000
        
        return action, value, processing_time
```

---

## 3. Main Loop (`main.py`)

```python
#!/usr/bin/env python3
"""
Simulation 3: Reinforcement Learning with PPO + Curiosity
Module 08 Integration with CARLA
"""
import sys
from pathlib import Path

# Add Sim1 to reuse CarlaInterface
sys.path.insert(0, str(Path(__file__).parent.parent / 'sim1-traditional'))

import time
import numpy as np

from carla_interface import CarlaInterface  # Reuse!
from carla_gym_env import CARLAGymEnv
from rl_agent_node import RLAgentNode


def main():
    """Main execution"""
    print("="*80)
    print("Simulation 3: Reinforcement Learning with PPO + Curiosity")
    print("Module 08 (RL + ICM)")
    print("="*80)
    
    # Configuration
    CHECKPOINT_PATH = Path(__file__).parent.parent.parent / '08-reinforcement-learning' / 'checkpoints' / 'best_ppo.pth'
    DEVICE = 'cuda'
    
    # Initialize CARLA
    carla = CarlaInterface()
    
    try:
        # 1. Connect to CARLA
        print("\n[Step 1] Connecting to CARLA...")
        carla.connect()
        
        # 2. Spawn vehicle
        print("\n[Step 2] Spawning vehicle...")
        vehicle = carla.spawn_vehicle()
        
        # 3. Spawn camera
        print("\n[Step 3] Spawning camera...")
        camera = carla.spawn_camera()
        
        # Wait for camera
        print("\n[Step 4] Waiting for camera stream...")
        time.sleep(3.0)
        
        # 4. Initialize RL components
        print("\n[Step 5] Initializing RL agent...")
        
        # CARLA-Gym wrapper
        carla_gym = CARLAGymEnv(carla)
        
        # RL agent
        rl_agent = RLAgentNode(
            checkpoint_path=str(CHECKPOINT_PATH),
            device=DEVICE
        )
        
        print("\n‚úÖ All modules initialized!")
        print("\n" + "="*80)
        print("Starting RL control (30Hz)")
        print("PPO Agent with Curiosity")
        print("Press Ctrl+C to stop")
        print("="*80 + "\n")
        
        # Reset environment
        obs, info = carla_gym.reset()
        
        # Main loop
        frame_count = 0
        episode_reward = 0.0
        total_latency = []
        
        while True:
            loop_start = time.time()
            
            # RL agent action selection
            action, value, agent_time = rl_agent.select_action(
                obs, deterministic=True
            )
            
            # Step environment
            next_obs, reward, terminated, truncated, info = carla_gym.step(action)
            
            episode_reward += reward
            
            # Update obs
            obs = next_obs
            
            # Reset if done
            if terminated or truncated:
                print(f"\nüìä Episode finished!")
                print(f"  Total reward: {episode_reward:.2f}")
                print(f"  Steps: {frame_count}\n")
                
                obs, info = carla_gym.reset()
                episode_reward = 0.0
                frame_count = 0
            
            # Logging
            loop_time = (time.time() - loop_start) * 1000
            total_latency.append(loop_time)
            
            if frame_count % 30 == 0:
                avg_latency = np.mean(total_latency[-30:]) if total_latency else 0
                fps = 1000 / avg_latency if avg_latency > 0 else 0
                
                steering = action[0] * 45.0
                throttle = action[1]
                
                print(f"[Frame {frame_count:04d}] FPS: {fps:.1f}")
                print(f"  Action (RL): Steer={steering:+.2f}¬∞, Throttle={throttle:.2f}")
                print(f"  Value (V): {value:.3f}")
                print(f"  Reward: {reward:+.3f}")
                print(f"  Episode Reward: {episode_reward:+.2f}")
                print(f"  Latency: {avg_latency:.1f}ms")
                print()
            
            frame_count += 1
            
            # Target 30Hz
            sleep_time = max(0, 0.033 - (time.time() - loop_start))
            time.sleep(sleep_time)
    
    except KeyboardInterrupt:
        print("\n" + "="*80)
        print("‚èπÔ∏è Stopped by user")
        print("="*80)
        
        if total_latency:
            print(f"\nStatistics:")
            print(f"  Total frames: {frame_count}")
            print(f"  Avg latency: {np.mean(total_latency):.1f}ms")
            print(f"  Avg FPS: {1000/np.mean(total_latency):.1f}")
            print(f"  Episode reward: {episode_reward:.2f}")
    
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        print("\nCleaning up...")
        carla.cleanup()
        print("‚úÖ Done!")


if __name__ == '__main__':
    main()
```

---

**ÏûëÏÑ±Ïûê:** AI Development Team  
**ÎÇ†Ïßú:** 2026-01-30  
**Status:** Implementation Spec Complete  
**Next:** Verification Plan & Code
