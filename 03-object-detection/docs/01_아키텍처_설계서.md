# Module 03: Object Detection System - ì•„í‚¤í…ì²˜ ì„¤ê³„ì„œ

**ë²„ì „:** 1.0  
**ì‘ì„±ì¼:** 2026-01-30  
**ìƒíƒœ:** ì„¤ê³„ ë‹¨ê³„  
**ì´ˆì :** ê³ ì •í™•ë„ ê°ì²´ ê°ì§€ (Accuracy-First)

---

## ğŸ“‹ ëª©ì°¨

1. [ì‹œìŠ¤í…œ ê°œìš”](#1-ì‹œìŠ¤í…œ-ê°œìš”)
2. [ì„¤ê³„ ì² í•™](#2-ì„¤ê³„-ì² í•™)
3. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#3-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
4. [í•µì‹¬ ì»´í¬ë„ŒíŠ¸](#4-í•µì‹¬-ì»´í¬ë„ŒíŠ¸)
5. [ë°ì´í„° í”Œë¡œìš°](#5-ë°ì´í„°-í”Œë¡œìš°)
6. [ì¸í„°í˜ì´ìŠ¤ ì„¤ê³„](#6-ì¸í„°í˜ì´ìŠ¤-ì„¤ê³„)
7. [ê¸°ìˆ  ìŠ¤íƒ](#7-ê¸°ìˆ -ìŠ¤íƒ)
8. [ì„±ëŠ¥ ëª©í‘œ](#8-ì„±ëŠ¥-ëª©í‘œ)
9. [ë³´ì•ˆ ë° ì•ˆì „](#9-ë³´ì•ˆ-ë°-ì•ˆì „)

---

## 1. ì‹œìŠ¤í…œ ê°œìš”

### 1.1 ëª©ì 

RC ììœ¨ì£¼í–‰ í™˜ê²½ì—ì„œ **ì‹¤ì‹œê°„ ê°ì²´ ê°ì§€ ë° ë¶„ë¥˜**ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. íŠ¸ë˜í”½ ì½˜, ì¥ì• ë¬¼, ë‹¤ë¥¸ ì°¨ëŸ‰, í‘œì§€íŒ ë“±ì„ ë†’ì€ ì •í™•ë„ë¡œ ê°ì§€í•˜ì—¬ ì•ˆì „í•œ ì£¼í–‰ì„ ì§€ì›í•©ë‹ˆë‹¤.

### 1.2 ë²”ìœ„

**In Scope:**
- âœ… ë‹¤ì¤‘ í´ë˜ìŠ¤ ê°ì²´ ê°ì§€ (5 classes)
- âœ… ì‹¤ì‹œê°„ ì¶”ë¡  (30+ FPS)
- âœ… ê³ ì •í™•ë„ ê°ì§€ (mAP@0.5 > 0.90)
- âœ… ì‘ì€ ê°ì²´ ê°ì§€ (5cm ì½˜)
- âœ… Module 01, 02ì™€ í†µí•©

**Out of Scope:**
- âŒ 3D ê°ì²´ ê°ì§€ (2Dë§Œ)
- âŒ ê°ì²´ ì¶”ì  (Tracking) - ë‹¨ìˆœ ê°ì§€ë§Œ
- âŒ ì˜ë¯¸ë¡ ì  ë¶„í•  (Segmentation)
- âŒ ì‹¤ì™¸ í™˜ê²½ (ì‹¤ë‚´ RC íŠ¸ë™ ì „ìš©)

### 1.3 ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ (RC íŠ¸ë™ í™˜ê²½ ê¸°ì¤€)

| ì¹´í…Œê³ ë¦¬ | ìš”êµ¬ì‚¬í•­ | ëª©í‘œ ê°’ | ë¹„ê³  |
|---------|---------|---------|------|
| **ì •í™•ë„** | mAP@0.5 | > 0.90 | COCO metric |
| **ì •ë°€ë„** | Precision | > 0.92 | False Positive ìµœì†Œí™” |
| **ì¬í˜„ìœ¨** | Recall | > 0.88 | ê°ì²´ ëˆ„ë½ ìµœì†Œí™” |
| **ì†ë„** | FPS | > 30 | RTX 5090 ê¸°ì¤€ |
| **ë©”ëª¨ë¦¬** | GPU Memory | < 4GB | ì¶”ë¡  ì‹œ |
| **ì§€ì—°ì‹œê°„** | Inference Latency | < 33ms | 30 FPS = 33ms/frame |

**âš ï¸ ì¤‘ìš” ì œì•½ì‚¬í•­:**
- **í™˜ê²½**: ì‹¤ë‚´ RC íŠ¸ë™ ì „ìš© (3Ã—4m ê³µê°„)
- **ê°ì²´ í¬ê¸°**: 5cm (ì½˜) ~ 30cm (ì°¨ëŸ‰)
- **ì¹´ë©”ë¼**: Pi Camera V2 (640Ã—480 â†’ 640Ã—640 crop)
- **ì¡°ëª…**: ì‹¤ë‚´ ì¡°ëª… (ì¼ì •í•œ í™˜ê²½)
- **ì •í™•ë„ ìš°ì„ **: ì†ë„ë³´ë‹¤ ì •í™•ë„ ìš°ì„ 

---

## 2. ì„¤ê³„ ì² í•™

### 2.1 í•µì‹¬ ì›ì¹™

1. **ì •í™•ë„ ìµœìš°ì„  (Accuracy First)**
   - False Positive ìµœì†Œí™” (ì•ˆì „ ì£¼í–‰)
   - ì‘ì€ ê°ì²´ ê°ì§€ ëŠ¥ë ¥
   - ë†’ì€ Precision & Recall
   - ëª¨ë¸ í¬ê¸° < ì†ë„ trade-off

2. **ì‹¤ì‹œê°„ì„± ë³´ì¥ (Real-time Capable)**
   - 30+ FPS ìœ ì§€
   - ë‚®ì€ ì§€ì—°ì‹œê°„
   - GPU ìµœì í™”

3. **ê°•ê±´ì„± (Robustness)**
   - ë‹¤ì–‘í•œ ì¡°ëª… ì¡°ê±´
   - ê°€ë¦¼ ìƒí™© (Occlusion)
   - ë‹¤ì–‘í•œ ê°ì²´ ë°°ì¹˜

4. **í†µí•©ì„± (Integration)**
   - Module 01, 02ì™€ ì›í™œí•œ ì—°ë™
   - í‘œì¤€ ì¶œë ¥ í˜•ì‹
   - ë…ë¦½ ì‹¤í–‰ ê°€ëŠ¥

---

## 3. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### 3.1 ì „ì²´ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Object Detection System (Module 03)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Camera     â”‚
â”‚   Input      â”‚
â”‚  (640Ã—480)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Preprocessing                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ - Resize to 640Ã—640                                  â”‚   â”‚
â”‚  â”‚ - Normalize (0-1)                                    â”‚   â”‚
â”‚  â”‚ - Convert to tensor                                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  YOLOv8l Backbone + Neck + Head                             â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  CSPDarknet53 Backbone                                 â”‚ â”‚
â”‚  â”‚  - Feature extraction                                  â”‚ â”‚
â”‚  â”‚  - Multi-scale features (P3, P4, P5)                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                   â”‚                                          â”‚
â”‚                   â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  PANet Neck                                            â”‚ â”‚
â”‚  â”‚  - Feature Pyramid Network                            â”‚ â”‚
â”‚  â”‚  - Top-down + Bottom-up paths                         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                   â”‚                                          â”‚
â”‚                   â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Detection Head (Anchor-free)                          â”‚ â”‚
â”‚  â”‚  - Classification                                      â”‚ â”‚
â”‚  â”‚  - Bounding Box Regression (DFL)                      â”‚ â”‚
â”‚  â”‚  - Objectness Score                                    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Post-processing                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ - Non-Maximum Suppression (NMS)                      â”‚   â”‚
â”‚  â”‚ - Confidence filtering (> 0.25)                      â”‚   â”‚
â”‚  â”‚ - Class-wise NMS (IoU > 0.45)                        â”‚   â”‚
â”‚  â”‚ - Small object filtering (min_size > 10px)           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Output                                                      â”‚
â”‚  {                                                           â”‚
â”‚    "boxes": [[x1, y1, x2, y2], ...],                        â”‚
â”‚    "classes": [0, 1, 2, ...],                               â”‚
â”‚    "confidences": [0.95, 0.87, ...],                        â”‚
â”‚    "class_names": ["cone", "obstacle", ...]                 â”‚
â”‚  }                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 ë ˆì´ì–´ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Application Layer                       â”‚
â”‚  - main.py                               â”‚
â”‚  - Integration with Module 01, 02       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Business Logic Layer                    â”‚
â”‚  - ObjectDetector (Main class)          â”‚
â”‚  - Detection pipeline orchestration     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Layer                             â”‚
â”‚  - YOLOv8l wrapper                       â”‚
â”‚  - Model loading & inference            â”‚
â”‚  - Output parsing                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Utility Layer                           â”‚
â”‚  - Preprocessing                         â”‚
â”‚  - Postprocessing (NMS)                  â”‚
â”‚  - Visualization                         â”‚
â”‚  - Metrics computation                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. í•µì‹¬ ì»´í¬ë„ŒíŠ¸

### 4.1 YOLOv8l Model

**ì„ íƒ ì´ìœ : ì •í™•ë„ ìš°ì„ **

| Model | Parameters | mAP@0.5 | mAP@0.5:0.95 | Speed (ms) | ì„ íƒ ì´ìœ  |
|-------|-----------|---------|--------------|-----------|----------|
| YOLOv8n | 3.2M | 0.527 | 0.375 | 8.7 | âŒ ì •í™•ë„ ë¶€ì¡± |
| YOLOv8s | 11.2M | 0.629 | 0.446 | 12.4 | âŒ ì—¬ì „íˆ ë‚®ìŒ |
| YOLOv8m | 25.9M | 0.676 | 0.501 | 17.3 | ğŸŸ¡ ê´œì°®ìœ¼ë‚˜ ë” ê°€ëŠ¥ |
| **YOLOv8l** | **43.7M** | **0.698** | **0.531** | **23.7** | âœ… **ìµœì  ì„ íƒ** |
| YOLOv8x | 68.2M | 0.706 | 0.541 | 35.1 | âŒ ë¯¸ë¯¸í•œ í–¥ìƒ, ëŠë¦¼ |

**YOLOv8l íŠ¹ì§•:**
- **Backbone**: CSPDarknet53 (Large)
- **Neck**: PANet (Path Aggregation Network)
- **Head**: Anchor-free + DFL (Distribution Focal Loss)
- **Input**: 640Ã—640 (default)
- **Output**: 8400 predictions per image

**ì •í™•ë„ ìš°ì„  ì„¤ì •:**
```yaml
# config/yolov8l.yaml
model: yolov8l.pt
imgsz: 640  # Higher resolution for small objects
conf: 0.25  # Confidence threshold (lower = more detections)
iou: 0.45   # NMS IoU threshold
max_det: 100  # Maximum detections per image
```

### 4.2 Dataset & Classes

**í´ë˜ìŠ¤ ì •ì˜ (5 classes):**

```yaml
# config/dataset.yaml
names:
  0: traffic_cone     # ì£¼í™©ìƒ‰/ë…¸ë€ìƒ‰ íŠ¸ë˜í”½ ì½˜
  1: obstacle         # ì¼ë°˜ ì¥ì• ë¬¼ (ë°•ìŠ¤, ì¥ë²½)
  2: robot_car        # ë‹¤ë¥¸ RC ì¹´
  3: traffic_sign     # ë¯¸ë‹ˆ êµí†µ í‘œì§€íŒ
  4: pedestrian       # ë¯¸ë‹ˆ í”¼ê·œì–´ (ì„ íƒì )

# Class distribution strategy
distribution:
  traffic_cone: 40%   # ê°€ì¥ ë§ìŒ (ì£¼ìš” ê°ì²´)
  obstacle: 30%
  robot_car: 15%
  traffic_sign: 10%
  pedestrian: 5%      # ë“œë¬¼ê²Œ
```

**ë°ì´í„°ì…‹ êµ¬ì„±:**

| Split | Images | Annotations | ë¹„ìœ¨ |
|-------|--------|-------------|------|
| **Train** | 700 | ~3500 | 70% |
| **Val** | 150 | ~750 | 15% |
| **Test** | 150 | ~750 | 15% |
| **Total** | 1000 | ~5000 | 100% |

**ìˆ˜ì§‘ ì „ëµ:**
1. **ë‹¤ì–‘í•œ ì¡°ëª…**: ë°ìŒ, ì–´ë‘ì›€, ê·¸ë¦¼ì
2. **ë‹¤ì–‘í•œ ê±°ë¦¬**: 0.5m ~ 3m
3. **ë‹¤ì–‘í•œ ê°ë„**: ì •ë©´, ì¸¡ë©´, ëŒ€ê°ì„ 
4. **ê°€ë¦¼ ìƒí™©**: ë¶€ë¶„ ê°€ë¦¼, ì¤‘ì²©
5. **ë‹¤ì–‘í•œ ë°°ê²½**: ë‹¤ë¥¸ íŠ¸ë™ êµ¬ì„±

### 4.3 Data Augmentation

**ëª©í‘œ: ë°ì´í„° 5ë°° ì¦ê°•**

```python
# Training augmentation (Ultralytics built-in)
augmentation:
  # Geometric
  - degrees: 10.0          # Rotation (Â±10Â°)
  - translate: 0.1         # Translation (Â±10%)
  - scale: 0.5             # Scale (0.5Ã— ~ 1.5Ã—)
  - shear: 0.0             # No shear (objects stay upright)
  - perspective: 0.0       # No perspective (camera fixed)
  - flipud: 0.0            # No vertical flip
  - fliplr: 0.5            # Horizontal flip (50%)
  
  # Photometric
  - hsv_h: 0.015           # Hue shift
  - hsv_s: 0.7             # Saturation
  - hsv_v: 0.4             # Brightness
  - mosaic: 1.0            # Mosaic augmentation
  - mixup: 0.1             # Mixup augmentation
  
  # Noise
  - blur: 0.01             # Gaussian blur
  - noise: 0.02            # Gaussian noise
```

**ê²°ê³¼:** 700 images â†’ 3500 effective samples

### 4.4 Training Strategy

**ì •í™•ë„ ìµœëŒ€í™” ì „ëµ:**

```python
# config/hyperparameters.yaml
epochs: 200               # ì¶©ë¶„í•œ í•™ìŠµ (ì¼ë°˜ì : 100)
batch_size: 16            # ì•ˆì •ì  í•™ìŠµ
imgsz: 640                # ê³ í•´ìƒë„ (ì‘ì€ ê°ì²´ ê°ì§€)
patience: 50              # Early stopping patience

# Optimizer: AdamW
optimizer: AdamW
lr0: 0.001                # Initial learning rate
lrf: 0.01                 # Final learning rate (lr0 * lrf)
momentum: 0.937           # SGD momentum
weight_decay: 0.0005      # L2 regularization

# Loss weights (ì •í™•ë„ ìš°ì„ )
box: 7.5                  # Box loss weight
cls: 0.5                  # Class loss weight
dfl: 1.5                  # DFL loss weight

# Fine-tuning strategy
freeze: 10                # Freeze first 10 layers (backbone)
pretrained: True          # Use COCO pre-trained weights
```

**í•™ìŠµ ë‹¨ê³„:**

1. **Phase 1: Transfer Learning (Epochs 1-50)**
   - Freeze backbone (first 10 layers)
   - Train only head & neck
   - High learning rate (1e-3)
   
2. **Phase 2: Fine-tuning (Epochs 51-150)**
   - Unfreeze all layers
   - Lower learning rate (1e-4)
   - Full model training

3. **Phase 3: Refinement (Epochs 151-200)**
   - Very low learning rate (1e-5)
   - Fine-tune for edge cases
   - Monitor overfitting

### 4.5 Post-processing Pipeline

**ì •í™•ë„ ë³´ì¥ì„ ìœ„í•œ í›„ì²˜ë¦¬:**

```python
# Post-processing configuration
postprocess:
  # Confidence filtering
  conf_thres: 0.25        # Confidence threshold
  
  # NMS (Non-Maximum Suppression)
  iou_thres: 0.45         # IoU threshold for NMS
  classes: null           # Filter by class (null = all)
  agnostic: False         # Class-agnostic NMS
  
  # Size filtering (ì‘ì€ ë…¸ì´ì¦ˆ ì œê±°)
  min_box_area: 100       # ìµœì†Œ ë°•ìŠ¤ ë©´ì  (10Ã—10 px)
  max_box_area: 200000    # ìµœëŒ€ ë°•ìŠ¤ ë©´ì  (ê±°ì˜ ì „ì²´ í™”ë©´)
  
  # Aspect ratio filtering (ë¹„ì •ìƒ ë°•ìŠ¤ ì œê±°)
  min_aspect_ratio: 0.2   # ìµœì†Œ ë¹„ìœ¨ (ë†’ì´/í­)
  max_aspect_ratio: 5.0   # ìµœëŒ€ ë¹„ìœ¨
  
  # Multi-scale NMS (ì •í™•ë„ í–¥ìƒ)
  multi_scale: True
```

**NMS ì•Œê³ ë¦¬ì¦˜:**
```python
1. Sort boxes by confidence (descending)
2. For each box:
   a. If IoU with any higher-confidence box > threshold:
      Remove (suppression)
   b. Else:
      Keep (max box for this region)
3. Return kept boxes
```

---

## 5. ë°ì´í„° í”Œë¡œìš°

### 5.1 Training Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Training Data Flow                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[Raw Images] (640Ã—480)
    â”‚
    â–¼
[Resize & Pad] â†’ 640Ã—640
    â”‚
    â–¼
[Augmentation]
    â”œâ”€ Mosaic (4 images)
    â”œâ”€ Mixup (2 images)
    â”œâ”€ Flip, Rotate, Scale
    â””â”€ HSV, Blur, Noise
    â”‚
    â–¼
[Normalize] (0-1)
    â”‚
    â–¼
[Batch] (16 images)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  YOLOv8l Model           â”‚
â”‚  - Forward pass          â”‚
â”‚  - Compute loss          â”‚
â”‚  - Backward pass         â”‚
â”‚  - Update weights        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
[Loss Computation]
    â”œâ”€ Box Loss (CIoU)
    â”œâ”€ Class Loss (BCE)
    â””â”€ DFL Loss
    â”‚
    â–¼
[Optimizer] AdamW
    â”‚
    â–¼
[Validation] (every epoch)
    â”‚
    â–¼
[Save Best Model] (highest mAP@0.5)
```

### 5.2 Inference Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Inference Data Flow                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[Input Frame] (640Ã—480)
    â”‚
    â–¼
[Preprocessing]
    â”œâ”€ Resize: 640Ã—640
    â”œâ”€ Normalize: [0, 1]
    â””â”€ To Tensor: (1, 3, 640, 640)
    â”‚
    â”‚  < 5ms
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  YOLOv8l Inference       â”‚
â”‚  - Backbone extraction   â”‚
â”‚  - PANet fusion          â”‚
â”‚  - Head prediction       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”‚  < 25ms (RTX 5090)
    â–¼
[Raw Predictions] (8400 boxes)
    â”‚
    â–¼
[Post-processing]
    â”œâ”€ Confidence filter (> 0.25)
    â”œâ”€ NMS (IoU < 0.45)
    â”œâ”€ Size filter (> 100 pxÂ²)
    â””â”€ Aspect ratio filter
    â”‚
    â”‚  < 3ms
    â–¼
[Final Detections] (~10-30 boxes)
    â”‚
    â–¼
[Output Format]
{
  "boxes": [[x1, y1, x2, y2], ...],
  "classes": [0, 1, ...],
  "confidences": [0.95, 0.87, ...],
  "class_names": ["cone", "obstacle", ...]
}

Total: < 33ms (30+ FPS)
```

---

## 6. ì¸í„°í˜ì´ìŠ¤ ì„¤ê³„

### 6.1 External Interface (Module ê°„ í†µí•©)

**Input Interface: ì¹´ë©”ë¼ í”„ë ˆì„**

```python
def detect_objects(
    frame: np.ndarray,  # (H, W, 3) RGB image
    conf_thres: float = 0.25,
    iou_thres: float = 0.45
) -> Dict:
    """
    ê°ì²´ ê°ì§€ ìˆ˜í–‰
    
    Parameters:
        frame: Input image (HÃ—WÃ—3)
        conf_thres: Confidence threshold
        iou_thres: NMS IoU threshold
    
    Returns:
        {
            "boxes": List[List[float]],      # [[x1, y1, x2, y2], ...]
            "classes": List[int],            # [0, 1, 2, ...]
            "confidences": List[float],      # [0.95, 0.87, ...]
            "class_names": List[str],        # ["cone", "obstacle", ...]
            "num_detections": int,           # Total detections
            "inference_time_ms": float       # Inference time
        }
    """
```

**Output to Module 02 (LKAS):**

```python
# Obstacle avoidance integration
detections = module_03.detect_objects(frame)

# Check if obstacles in lane
lane_mask = module_01.detect_lane(frame)
obstacles_in_lane = check_overlap(detections["boxes"], lane_mask)

if obstacles_in_lane:
    lkas.emergency_brake()
    # or lkas.steer_around_obstacle(detections)
```

### 6.2 Configuration Interface

**ë°ì´í„°ì…‹ ì„¤ì •: `config/dataset.yaml`**

```yaml
# Dataset paths
path: ./datasets
train: images/train
val: images/val
test: images/test

# Classes
nc: 5  # Number of classes
names:
  0: traffic_cone
  1: obstacle
  2: robot_car
  3: traffic_sign
  4: pedestrian

# Image settings
imgsz: 640
```

**ëª¨ë¸ ì„¤ì •: `config/yolov8l.yaml`**

```yaml
# Model
model: yolov8l  # Large model for accuracy

# Hyperparameters
epochs: 200
batch: 16
imgsz: 640
patience: 50

# Optimizer
optimizer: AdamW
lr0: 0.001
lrf: 0.01
momentum: 0.937
weight_decay: 0.0005

# Loss
box: 7.5
cls: 0.5
dfl: 1.5

# Augmentation
hsv_h: 0.015
hsv_s: 0.7
hsv_v: 0.4
degrees: 10.0
translate: 0.1
scale: 0.5
fliplr: 0.5
mosaic: 1.0
mixup: 0.1
```

### 6.3 API Interface

```python
class ObjectDetector:
    """YOLOv8 ê¸°ë°˜ ê°ì²´ ê°ì§€ê¸°"""
    
    def __init__(
        self,
        weights: str = 'yolov8l.pt',
        device: str = 'cuda',
        conf_thres: float = 0.25,
        iou_thres: float = 0.45
    ):
        """
        Parameters:
            weights: Model weights path
            device: Device ('cuda' or 'cpu')
            conf_thres: Confidence threshold
            iou_thres: NMS IoU threshold
        """
        pass
    
    def detect(
        self,
        image: np.ndarray,
        visualize: bool = False
    ) -> Dict:
        """
        ê°ì²´ ê°ì§€ ìˆ˜í–‰
        
        Parameters:
            image: Input image (HÃ—WÃ—3)
            visualize: Draw boxes on image
        
        Returns:
            Detection results
        """
        pass
    
    def detect_batch(
        self,
        images: List[np.ndarray]
    ) -> List[Dict]:
        """ë°°ì¹˜ ì²˜ë¦¬"""
        pass
    
    def export_onnx(self, output_path: str):
        """ONNX í¬ë§·ìœ¼ë¡œ export"""
        pass
    
    def get_performance_stats(self) -> Dict:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        pass
```

---

## 7. ê¸°ìˆ  ìŠ¤íƒ

### 7.1 í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬

| ì»´í¬ë„ŒíŠ¸ | ë¼ì´ë¸ŒëŸ¬ë¦¬ | ë²„ì „ | ìš©ë„ |
|---------|----------|------|------|
| **YOLO Framework** | Ultralytics | 8.0+ | YOLOv8 êµ¬í˜„ |
| **Deep Learning** | PyTorch | 2.0+ | ëª¨ë¸ í•™ìŠµ & ì¶”ë¡  |
| **Computer Vision** | OpenCV | 4.8+ | ì´ë¯¸ì§€ ì²˜ë¦¬ |
| **Annotation** | CVAT / Roboflow | - | ë°ì´í„° ë ˆì´ë¸”ë§ |
| **Augmentation** | Albumentations | 1.3+ | ë°ì´í„° ì¦ê°• |
| **Metrics** | torchmetrics | 1.0+ | í‰ê°€ ë©”íŠ¸ë¦­ |
| **Visualization** | Matplotlib | 3.7+ | ê²°ê³¼ ì‹œê°í™” |

### 7.2 ê°œë°œ í™˜ê²½

- **Python:** 3.10+
- **OS:** Linux (Ubuntu 22.04), macOS
- **GPU:** NVIDIA RTX 5090 (24GB VRAM)
- **CUDA:** 12.1
- **cuDNN:** 8.9
- **IDE:** VS Code, PyCharm

### 7.3 ì˜ì¡´ì„± ê´€ë¦¬

```txt
# requirements.txt

# Core
torch>=2.0.0
torchvision>=0.15.0
ultralytics>=8.0.0

# Computer Vision
opencv-python>=4.8.0
pillow>=10.0.0

# Data
numpy>=1.24.0
pandas>=2.0.0
albumentations>=1.3.0

# Metrics
torchmetrics>=1.0.0
scikit-learn>=1.3.0

# Visualization
matplotlib>=3.7.0
seaborn>=0.12.0

# Utils
tqdm>=4.65.0
pyyaml>=6.0.0
```

---

## 8. ì„±ëŠ¥ ëª©í‘œ

### 8.1 ì •ëŸ‰ì  ëª©í‘œ (KPI)

| ë©”íŠ¸ë¦­ | ëª©í‘œ | ì¸¡ì • ë°©ë²• | ìš°ì„ ìˆœìœ„ |
|--------|------|-----------|---------|
| **mAP@0.5** | > 0.90 | COCO metric | P0 (í•„ìˆ˜) |
| **mAP@0.5:0.95** | > 0.70 | COCO metric | P0 (í•„ìˆ˜) |
| **Precision** | > 0.92 | TP / (TP + FP) | P0 (í•„ìˆ˜) |
| **Recall** | > 0.88 | TP / (TP + FN) | P1 (ê¶Œì¥) |
| **F1-Score** | > 0.90 | 2 * P * R / (P + R) | P1 (ê¶Œì¥) |
| **FPS** | > 30 | Frames per second | P2 (ì„ íƒ) |
| **Inference Time** | < 33ms | Per frame | P2 (ì„ íƒ) |
| **GPU Memory** | < 4GB | During inference | P2 (ì„ íƒ) |

**ì„±ëŠ¥ ëª©í‘œ ì„¤ì • ê·¼ê±°:**

| í•­ëª© | ì´ìœ  |
|------|------|
| **mAP@0.5 > 0.90** | COCO baseline (0.70) ëŒ€ë¹„ ë†’ì€ ëª©í‘œ, RC í™˜ê²½ì€ ì œí•œì ì´ë¯€ë¡œ ë‹¬ì„± ê°€ëŠ¥ |
| **Precision > 0.92** | False Positive ìµœì†Œí™” (ì•ˆì „ ì£¼í–‰ ì¤‘ìš”) |
| **Recall > 0.88** | ëŒ€ë¶€ë¶„ì˜ ê°ì²´ ê°ì§€ (ë†“ì¹˜ëŠ” ê²ƒë³´ë‹¤ ê³¼ê²€ì¶œ ì„ í˜¸) |
| **FPS > 30** | ì‹¤ì‹œê°„ ì£¼í–‰ (30 FPS = ì¸ê°„ ì‹œê° ì‹œìŠ¤í…œ) |

### 8.2 í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ëª©í‘œ

| Class | mAP@0.5 | Precision | Recall | ë‚œì´ë„ |
|-------|---------|-----------|--------|--------|
| **traffic_cone** | > 0.92 | > 0.94 | > 0.90 | ë³´í†µ (ì‘ì€ ê°ì²´) |
| **obstacle** | > 0.90 | > 0.92 | > 0.88 | ì‰¬ì›€ (í° ê°ì²´) |
| **robot_car** | > 0.88 | > 0.90 | > 0.86 | ì–´ë ¤ì›€ (ë‹¤ì–‘í•œ ëª¨ì–‘) |
| **traffic_sign** | > 0.85 | > 0.88 | > 0.84 | ì–´ë ¤ì›€ (ì‘ê³  ë‹¤ì–‘) |
| **pedestrian** | > 0.80 | > 0.85 | > 0.80 | ë§¤ìš° ì–´ë ¤ì›€ (ë§¤ìš° ì‘ìŒ) |

### 8.3 ì •ì„±ì  ëª©í‘œ

- âœ… **ì‘ì€ ê°ì²´ ê°ì§€**: 5cm ì½˜ì„ 3m ê±°ë¦¬ì—ì„œ ê°ì§€
- âœ… **ì¡°ëª… ê°•ê±´ì„±**: ë°ìŒ/ì–´ë‘ì›€/ê·¸ë¦¼ìì—ì„œ ì¼ê´€ëœ ì„±ëŠ¥
- âœ… **ê°€ë¦¼ ì²˜ë¦¬**: 50% ê°€ë¦¼ê¹Œì§€ ê°ì§€
- âœ… **ë¶€ë“œëŸ¬ìš´ ê°ì§€**: ì—°ì† í”„ë ˆì„ì—ì„œ ì•ˆì •ì  ë°•ìŠ¤
- âœ… **ë‚®ì€ ì˜¤ê²€ì¶œ**: ë°°ê²½ì„ ê°ì²´ë¡œ ì˜¤ì¸ ìµœì†Œí™”

---

## 9. ë³´ì•ˆ ë° ì•ˆì „

### 9.1 ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜

**Fail-Safe ì„¤ê³„:**

```python
1. Input Validation
   - ì´ë¯¸ì§€ í¬ê¸° ê²€ì¦ (640Ã—640)
   - ì±„ë„ ìˆ˜ ê²€ì¦ (3 channels)
   - ê°’ ë²”ìœ„ ê²€ì¦ (0-255 or 0-1)

2. Confidence Filtering
   - ë‚®ì€ ì‹ ë¢°ë„ (<0.25) ì œê±°
   - ë§¤ìš° ì‘ì€ ë°•ìŠ¤ ì œê±° (<100 pxÂ²)
   - ë¹„ì •ìƒ ë¹„ìœ¨ ë°•ìŠ¤ ì œê±°

3. Graceful Degradation
   - ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œ: ë¹ˆ ê²°ê³¼ ë°˜í™˜
   - GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ: CPU fallback
   - ì¶”ë¡  ì‹¤íŒ¨ ì‹œ: ì´ì „ ê²°ê³¼ ì¬ì‚¬ìš©

4. Performance Monitoring
   - FPS ëª¨ë‹ˆí„°ë§ (< 15 FPSì´ë©´ ê²½ê³ )
   - GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
   - ì§€ì—°ì‹œê°„ tracking
```

### 9.2 í…ŒìŠ¤íŠ¸ ì „ëµ

**ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (Unit Test):**
- Preprocessing í•¨ìˆ˜
- Post-processing (NMS)
- ê° í´ë˜ìŠ¤ ê°ì§€
- Edge case ì»¤ë²„ë¦¬ì§€ > 90%

**í†µí•© í…ŒìŠ¤íŠ¸ (Integration Test):**
- ì „ì²´ íŒŒì´í”„ë¼ì¸ E2E
- Module 01, 02ì™€ í†µí•©
- ì‹¤ì‹œê°„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

**ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (Performance Test):**
- FPS ë²¤ì¹˜ë§ˆí¬
- ì§€ì—°ì‹œê°„ í”„ë¡œíŒŒì¼ë§
- GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- ì¥ì‹œê°„ ì•ˆì •ì„± (1ì‹œê°„ ì—°ì†)

**ì •í™•ë„ í…ŒìŠ¤íŠ¸ (Accuracy Test):**
- Test set mAP ì¸¡ì •
- í´ë˜ìŠ¤ë³„ ì„±ëŠ¥
- ì‘ì€ ê°ì²´ ê°ì§€ ëŠ¥ë ¥
- ê°€ë¦¼ ìƒí™© ì²˜ë¦¬

### 9.3 ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§

**ë¡œê·¸ ë ˆë²¨:**
```python
DEBUG:   ì„¸ë¶€ ì¶”ì  ì •ë³´
INFO:    ì¼ë°˜ ê°ì§€ ê²°ê³¼
WARNING: ë‚®ì€ FPS, ë†’ì€ ì§€ì—°ì‹œê°„
ERROR:   ì¶”ë¡  ì‹¤íŒ¨
CRITICAL: ì‹œìŠ¤í…œ ì¤‘ë‹¨
```

**ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ:**
- ì‹¤ì‹œê°„ ê°ì§€ ê²°ê³¼ (ë°•ìŠ¤)
- FPS & ì§€ì—°ì‹œê°„ ê·¸ë˜í”„
- í´ë˜ìŠ¤ë³„ ê°ì§€ ë¹ˆë„
- GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- Confidence ë¶„í¬

---

## 10. ë°ì´í„° ìˆ˜ì§‘ ì „ëµ

### 10.1 ìˆ˜ì§‘ ê³„íš

**ëª©í‘œ: 1,000 images (5,000+ annotations)**

**ìˆ˜ì§‘ í™˜ê²½:**
```
1. RC íŠ¸ë™ A (ì£¼ íŠ¸ë™)
   - í¬ê¸°: 3m Ã— 4m
   - ì¡°ëª…: ì²œì¥ LED
   - ìˆ˜ì§‘: 600 images

2. RC íŠ¸ë™ B (ë³´ì¡° íŠ¸ë™)
   - í¬ê¸°: 2.5m Ã— 3.5m
   - ì¡°ëª…: ìì—°ê´‘ + LED
   - ìˆ˜ì§‘: 400 images
```

**ìˆ˜ì§‘ ì¡°ê±´:**
```python
ì¡°ëª…:
  - ë°ìŒ (1000 lux): 30%
  - ë³´í†µ (500 lux): 40%
  - ì–´ë‘ì›€ (200 lux): 20%
  - ê·¸ë¦¼ì: 10%

ê±°ë¦¬:
  - ê°€ê¹Œì›€ (0.5-1m): 30%
  - ì¤‘ê°„ (1-2m): 40%
  - ë©€ë¦¬ (2-3m): 30%

ê°ë„:
  - ì •ë©´ (0-30Â°): 40%
  - ì¸¡ë©´ (30-60Â°): 35%
  - ëŒ€ê° (60-90Â°): 25%

ê°ì²´ ìˆ˜:
  - 1-2ê°œ: 20%
  - 3-5ê°œ: 50%
  - 6-10ê°œ: 25%
  - 10ê°œ ì´ìƒ: 5%
```

### 10.2 ë ˆì´ë¸”ë§ ì „ëµ

**ë„êµ¬: CVAT (Computer Vision Annotation Tool)**

**ë ˆì´ë¸”ë§ ê°€ì´ë“œë¼ì¸:**
```
1. Bounding Box
   - ê°ì²´ ì „ì²´ë¥¼ í¬í•¨ (margin ì•½ê°„)
   - ê°€ë¦¼ëœ ë¶€ë¶„ë„ ì¶”ì •í•˜ì—¬ í¬í•¨
   - ìµœì†Œ í¬ê¸°: 10Ã—10 px

2. í´ë˜ìŠ¤ í• ë‹¹
   - traffic_cone: ì£¼í™©/ë…¸ë‘ ì½˜
   - obstacle: ë°•ìŠ¤, ì¥ë²½, ê¸°íƒ€
   - robot_car: RC ì¹´ ì „ì²´
   - traffic_sign: í‘œì§€íŒ (ì „ì²´ í¬í•¨)
   - pedestrian: í”¼ê·œì–´ (ë¨¸ë¦¬ë¶€í„° ë°œê¹Œì§€)

3. í’ˆì§ˆ ê´€ë¦¬
   - ê° ì´ë¯¸ì§€ 2ëª…ì´ ê²€í†  (inter-annotator agreement)
   - ì• ë§¤í•œ ê²½ìš°: í¬í•¨ (recall ìš°ì„ )
   - ë¶€ë¶„ ê°€ë¦¼ (>50%): ë ˆì´ë¸”ë§
   - ë¶€ë¶„ ê°€ë¦¼ (<20%): ì œì™¸
```

### 10.3 ë°ì´í„° ê²€ì¦

**í’ˆì§ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] ëª¨ë“  ë°•ìŠ¤ê°€ ê°ì²´ í¬í•¨
- [ ] í´ë˜ìŠ¤ ì˜¬ë°”ë¥´ê²Œ í• ë‹¹
- [ ] ì¤‘ë³µ ë°•ìŠ¤ ì—†ìŒ
- [ ] ë„ˆë¬´ ì‘ì€ ë°•ìŠ¤ (<10px) ì œê±°
- [ ] ì´ë¯¸ì§€ í•´ìƒë„ ì¼ê´€ì„±
- [ ] Train/Val/Test ë¶„ë¦¬ í™•ì¸

---

## 11. ë§ˆì¼ìŠ¤í†¤ ë° ì¼ì •

### Phase 1: ì„¤ê³„ & ì¤€ë¹„ (Week 1, 2ì¼)
- [x] ì•„í‚¤í…ì²˜ ì„¤ê³„ì„œ
- [ ] êµ¬í˜„ ëª…ì„¸ì„œ
- [ ] ê²€ì¦ì„œ
- [ ] ë°ì´í„° ìˆ˜ì§‘ ê³„íš

### Phase 2: ë°ì´í„° ìˆ˜ì§‘ (Week 1-2, 5ì¼)
- [ ] ì´ë¯¸ì§€ ìˆ˜ì§‘ (1000ì¥)
- [ ] CVAT ë ˆì´ë¸”ë§
- [ ] í’ˆì§ˆ ê²€ì¦
- [ ] Train/Val/Test split

### Phase 3: êµ¬í˜„ (Week 2, 3ì¼)
- [ ] YOLOv8 í†µí•©
- [ ] Dataset loader
- [ ] Training pipeline
- [ ] Inference pipeline
- [ ] Post-processing

### Phase 4: í•™ìŠµ (Week 2-3, 4ì¼)
- [ ] Baseline training (100 epochs)
- [ ] Hyperparameter tuning
- [ ] Extended training (200 epochs)
- [ ] Model selection

### Phase 5: í‰ê°€ & ë¬¸ì„œí™” (Week 3-4, 2ì¼)
- [ ] Test set í‰ê°€
- [ ] Ablation studies
- [ ] Performance documentation
- [ ] Integration testing

**ì´ ì˜ˆìƒ ê¸°ê°„:** 2-3ì£¼

---

## 12. ì°¸ê³  ìë£Œ

### í•™ìˆ  ë…¼ë¬¸
1. **YOLOv8** (Ultralytics, 2023) - Official documentation
2. **YOLOv5** (Jocher et al., 2020) - Architecture foundation
3. **Focal Loss** (Lin et al., 2017) - Loss function for hard examples

### ê¸°ìˆ  ë¬¸ì„œ
- [Ultralytics YOLOv8 Docs](https://docs.ultralytics.com/)
- [YOLO GitHub](https://github.com/ultralytics/ultralytics)
- [CVAT Annotation Guide](https://opencv.github.io/cvat/)

### ì˜¤í”ˆì†ŒìŠ¤ ì°¸ê³ 
- [Roboflow Universe](https://universe.roboflow.com/)
- [COCO Dataset](https://cocodataset.org/)

---

**ë¬¸ì„œ ë²„ì „ ì´ë ¥:**
- v1.0 (2026-01-30): ì´ˆì•ˆ ì‘ì„± (ì •í™•ë„ ìš°ì„  ì „ëµ)

**ë¦¬ë·°ì–´:** [TBD]  
**ìŠ¹ì¸ì:** [TBD]

**ë‹¤ìŒ ë‹¨ê³„:** êµ¬í˜„ ëª…ì„¸ì„œ ì‘ì„±
