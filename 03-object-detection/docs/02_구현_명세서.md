# Module 03: Object Detection System - êµ¬í˜„ ëª…ì„¸ì„œ

**ë²„ì „:** 1.0  
**ì‘ì„±ì¼:** 2026-01-30  
**ìƒíƒœ:** êµ¬í˜„ ë‹¨ê³„  
**ì´ˆì :** YOLOv8l ê³ ì •í™•ë„ ê°ì²´ ê°ì§€

---

## ğŸ“‹ ëª©ì°¨

1. [ê°œë°œ í™˜ê²½ ì„¤ì •](#1-ê°œë°œ-í™˜ê²½-ì„¤ì •)
2. [ë””ë ‰í„°ë¦¬ êµ¬ì¡°](#2-ë””ë ‰í„°ë¦¬-êµ¬ì¡°)
3. [í•µì‹¬ í´ë˜ìŠ¤ ëª…ì„¸](#3-í•µì‹¬-í´ë˜ìŠ¤-ëª…ì„¸)
4. [ì•Œê³ ë¦¬ì¦˜ ìƒì„¸](#4-ì•Œê³ ë¦¬ì¦˜-ìƒì„¸)
5. [ì„¤ì • íŒŒì¼ ëª…ì„¸](#5-ì„¤ì •-íŒŒì¼-ëª…ì„¸)
6. [ë°ì´í„° íŒŒì´í”„ë¼ì¸](#6-ë°ì´í„°-íŒŒì´í”„ë¼ì¸)
7. [API ë¬¸ì„œ](#7-api-ë¬¸ì„œ)
8. [ì—ëŸ¬ ì²˜ë¦¬](#8-ì—ëŸ¬-ì²˜ë¦¬)

---

## 1. ê°œë°œ í™˜ê²½ ì„¤ì •

### 1.1 ì˜ì¡´ì„± ì„¤ì¹˜

```bash
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ
cd 03-object-detection

# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### 1.2 `requirements.txt`

```txt
# Core Dependencies
torch>=2.0.0
torchvision>=0.15.0
ultralytics>=8.0.0

# Computer Vision
opencv-python>=4.8.0
pillow>=10.0.0

# Data Processing
numpy>=1.24.0
pandas>=2.0.0
albumentations>=1.3.0

# Metrics & Evaluation
torchmetrics>=1.0.0
scikit-learn>=1.3.0
pycocotools>=2.0.0

# Visualization
matplotlib>=3.7.0
seaborn>=0.12.0

# Utils
tqdm>=4.65.0
pyyaml>=6.0.0
loguru>=0.7.0

# Development
pytest>=7.4.0
pytest-cov>=4.1.0
black>=23.0.0
flake8>=6.0.0

# Optional (Export)
onnx>=1.14.0
onnxruntime>=1.15.0
```

---

## 2. ë””ë ‰í„°ë¦¬ êµ¬ì¡°

```
03-object-detection/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ dataset.yaml             # ë°ì´í„°ì…‹ ì„¤ì •
â”‚   â”œâ”€â”€ yolov8l.yaml             # ëª¨ë¸ ì„¤ì •
â”‚   â””â”€â”€ hyperparameters.yaml     # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”œâ”€â”€ train/               # 700 images
â”‚   â”‚   â”œâ”€â”€ val/                 # 150 images
â”‚   â”‚   â””â”€â”€ test/                # 150 images
â”‚   â””â”€â”€ labels/
â”‚       â”œâ”€â”€ train/               # YOLO format .txt
â”‚       â”œâ”€â”€ val/
â”‚       â””â”€â”€ test/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ detector.py              # Main API
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ yolo_wrapper.py      # YOLOv8 wrapper
â”‚   â”‚   â””â”€â”€ postprocess.py       # NMS, filtering
â”‚   â”‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dataset.py           # Dataset loader
â”‚   â”‚   â”œâ”€â”€ augmentation.py      # Data augmentation
â”‚   â”‚   â””â”€â”€ validation.py        # Data quality check
â”‚   â”‚
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ trainer.py           # Training loop
â”‚   â”‚   â”œâ”€â”€ metrics.py           # mAP, Precision, Recall
â”‚   â”‚   â””â”€â”€ callbacks.py         # TensorBoard, checkpoints
â”‚   â”‚
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ predictor.py         # Inference pipeline
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ visualization.py     # Draw boxes
â”‚       â”œâ”€â”€ logger.py            # Logging setup
â”‚       â””â”€â”€ export.py            # Model export
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_detector.py
â”‚   â”œâ”€â”€ test_dataset.py
â”‚   â”œâ”€â”€ test_postprocess.py
â”‚   â””â”€â”€ test_integration.py
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ 01_ì•„í‚¤í…ì²˜_ì„¤ê³„ì„œ.md
â”‚   â”œâ”€â”€ 02_êµ¬í˜„_ëª…ì„¸ì„œ.md          # ì´ íŒŒì¼
â”‚   â”œâ”€â”€ 03_ê²€ì¦ì„œ.md
â”‚   â”œâ”€â”€ 04_êµ¬í˜„_ì¼ì¹˜ìœ¨_ë¶„ì„.md
â”‚   â””â”€â”€ 05_ì„±ëŠ¥_í‰ê°€.md
â”‚
â”œâ”€â”€ train.py                       # Training script
â”œâ”€â”€ detect.py                      # Inference script
â”œâ”€â”€ validate.py                    # Validation script
â”œâ”€â”€ export.py                      # Export to ONNX/TensorRT
â”‚
â”œâ”€â”€ runs/                          # Training outputs
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ exp1/
â”‚   â”‚   â”‚   â”œâ”€â”€ weights/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ best.pt
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ last.pt
â”‚   â”‚   â”‚   â”œâ”€â”€ results.csv
â”‚   â”‚   â”‚   â””â”€â”€ confusion_matrix.png
â”‚   â”‚   â””â”€â”€ exp2/
â”‚   â””â”€â”€ val/
â”‚
â””â”€â”€ results/                       # Test results
    â”œâ”€â”€ test_metrics.json
    â”œâ”€â”€ pr_curve.png
    â””â”€â”€ predictions/
```

---

## 3. í•µì‹¬ í´ë˜ìŠ¤ ëª…ì„¸

### 3.1 `ObjectDetector` (Main API)

**íŒŒì¼:** `src/detector.py`

```python
from typing import List, Dict, Optional, Tuple
import numpy as np
import torch
from ultralytics import YOLO


class ObjectDetector:
    """
    YOLOv8 ê¸°ë°˜ ê°ì²´ ê°ì§€ê¸°
    
    ì •í™•ë„ ìš°ì„  ì„¤ì •:
        - Model: YOLOv8l (Large)
        - Confidence: 0.25 (ë‚®ê²Œ ìœ ì§€ â†’ Recall í–¥ìƒ)
        - IoU NMS: 0.45 (ì¤‘ë³µ ì œê±°)
        - Image Size: 640Ã—640 (ê³ í•´ìƒë„)
    
    Attributes:
        model (YOLO): YOLOv8 model instance
        device (str): 'cuda' or 'cpu'
        conf_thres (float): Confidence threshold
        iou_thres (float): NMS IoU threshold
        class_names (Dict[int, str]): Class ID to name mapping
    """
    
    def __init__(
        self,
        weights: str = 'yolov8l.pt',
        device: str = 'cuda',
        conf_thres: float = 0.25,
        iou_thres: float = 0.45,
        imgsz: int = 640
    ):
        """
        Parameters:
            weights: Model weights path ('yolov8l.pt' or custom)
            device: Device for inference ('cuda' or 'cpu')
            conf_thres: Confidence threshold (0.0 ~ 1.0)
            iou_thres: NMS IoU threshold (0.0 ~ 1.0)
            imgsz: Input image size (default: 640)
        
        Note:
            - ì •í™•ë„ ìš°ì„ : conf_thresë¥¼ ë‚®ê²Œ (0.25) ì„¤ì •
            - False Positive ì œì–´: post-processingì—ì„œ ì¶”ê°€ í•„í„°ë§
        """
        self.device = device
        self.conf_thres = conf_thres
        self.iou_thres = iou_thres
        self.imgsz = imgsz
        
        # YOLOv8 ëª¨ë¸ ë¡œë“œ
        self.model = YOLO(weights)
        self.model.to(device)
        
        # í´ë˜ìŠ¤ ì´ë¦„
        self.class_names = {
            0: 'traffic_cone',
            1: 'obstacle',
            2: 'robot_car',
            3: 'traffic_sign',
            4: 'pedestrian'
        }
        
        # í†µê³„
        self._inference_times = []
        self._detection_counts = []
    
    def detect(
        self,
        image: np.ndarray,
        return_image: bool = False,
        filter_small: bool = True
    ) -> Dict:
        """
        ë‹¨ì¼ ì´ë¯¸ì§€ ê°ì²´ ê°ì§€
        
        Parameters:
            image: Input image (HÃ—WÃ—3), RGB format
            return_image: Return annotated image
            filter_small: Filter small boxes (< 100 pxÂ²)
        
        Returns:
            {
                "boxes": List[List[float]],       # [[x1, y1, x2, y2], ...]
                "classes": List[int],             # [0, 1, 2, ...]
                "confidences": List[float],       # [0.95, 0.87, ...]
                "class_names": List[str],         # ["cone", "obstacle", ...]
                "num_detections": int,            # Total detections
                "inference_time_ms": float,       # Inference time
                "image_annotated": np.ndarray     # Optional
            }
        """
        import time
        
        start_time = time.time()
        
        # ì¶”ë¡ 
        results = self.model.predict(
            source=image,
            conf=self.conf_thres,
            iou=self.iou_thres,
            imgsz=self.imgsz,
            device=self.device,
            verbose=False
        )
        
        inference_time = (time.time() - start_time) * 1000  # ms
        
        # ê²°ê³¼ íŒŒì‹±
        result = results[0]  # Single image
        boxes = result.boxes.xyxy.cpu().numpy()  # (N, 4)
        classes = result.boxes.cls.cpu().numpy().astype(int)  # (N,)
        confidences = result.boxes.conf.cpu().numpy()  # (N,)
        
        # Post-processing: ì‘ì€ ë°•ìŠ¤ í•„í„°ë§
        if filter_small:
            boxes, classes, confidences = self._filter_small_boxes(
                boxes, classes, confidences, min_area=100
            )
        
        # í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘
        class_names = [self.class_names.get(cls, 'unknown') for cls in classes]
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self._inference_times.append(inference_time)
        self._detection_counts.append(len(boxes))
        
        output = {
            "boxes": boxes.tolist(),
            "classes": classes.tolist(),
            "confidences": confidences.tolist(),
            "class_names": class_names,
            "num_detections": len(boxes),
            "inference_time_ms": inference_time
        }
        
        # Annotated image (ì„ íƒì )
        if return_image:
            output["image_annotated"] = self._draw_boxes(
                image, boxes, classes, confidences, class_names
            )
        
        return output
    
    def detect_batch(
        self,
        images: List[np.ndarray]
    ) -> List[Dict]:
        """
        ë°°ì¹˜ ì´ë¯¸ì§€ ì²˜ë¦¬
        
        Parameters:
            images: List of images (HÃ—WÃ—3)
        
        Returns:
            List of detection results
        """
        results_list = self.model.predict(
            source=images,
            conf=self.conf_thres,
            iou=self.iou_thres,
            imgsz=self.imgsz,
            device=self.device,
            verbose=False
        )
        
        outputs = []
        for result in results_list:
            boxes = result.boxes.xyxy.cpu().numpy()
            classes = result.boxes.cls.cpu().numpy().astype(int)
            confidences = result.boxes.conf.cpu().numpy()
            class_names = [self.class_names.get(cls, 'unknown') for cls in classes]
            
            outputs.append({
                "boxes": boxes.tolist(),
                "classes": classes.tolist(),
                "confidences": confidences.tolist(),
                "class_names": class_names,
                "num_detections": len(boxes)
            })
        
        return outputs
    
    def _filter_small_boxes(
        self,
        boxes: np.ndarray,
        classes: np.ndarray,
        confidences: np.ndarray,
        min_area: float = 100
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        ì‘ì€ ë°•ìŠ¤ í•„í„°ë§ (ë…¸ì´ì¦ˆ ì œê±°)
        
        Parameters:
            boxes: (N, 4) [x1, y1, x2, y2]
            classes: (N,)
            confidences: (N,)
            min_area: Minimum box area (pixelsÂ²)
        
        Returns:
            Filtered boxes, classes, confidences
        """
        # ë°•ìŠ¤ ë©´ì  ê³„ì‚°
        widths = boxes[:, 2] - boxes[:, 0]
        heights = boxes[:, 3] - boxes[:, 1]
        areas = widths * heights
        
        # í•„í„°ë§
        keep = areas >= min_area
        
        return boxes[keep], classes[keep], confidences[keep]
    
    def _draw_boxes(
        self,
        image: np.ndarray,
        boxes: np.ndarray,
        classes: np.ndarray,
        confidences: np.ndarray,
        class_names: List[str]
    ) -> np.ndarray:
        """
        ì´ë¯¸ì§€ì— ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        
        Returns:
            Annotated image
        """
        import cv2
        
        output = image.copy()
        
        # í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ
        colors = {
            'traffic_cone': (255, 165, 0),   # Orange
            'obstacle': (255, 0, 0),         # Red
            'robot_car': (0, 255, 0),        # Green
            'traffic_sign': (0, 0, 255),     # Blue
            'pedestrian': (255, 255, 0)      # Yellow
        }
        
        for box, cls_name, conf in zip(boxes, class_names, confidences):
            x1, y1, x2, y2 = map(int, box)
            color = colors.get(cls_name, (128, 128, 128))
            
            # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            cv2.rectangle(output, (x1, y1), (x2, y2), color, 2)
            
            # ë ˆì´ë¸” ê·¸ë¦¬ê¸°
            label = f"{cls_name} {conf:.2f}"
            (label_w, label_h), _ = cv2.getTextSize(
                label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1
            )
            cv2.rectangle(output, (x1, y1 - label_h - 10), 
                         (x1 + label_w, y1), color, -1)
            cv2.putText(output, label, (x1, y1 - 5),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        return output
    
    def get_performance_stats(self) -> Dict:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        if len(self._inference_times) == 0:
            return {
                "avg_inference_time_ms": 0.0,
                "avg_fps": 0.0,
                "avg_detections": 0.0
            }
        
        avg_time = np.mean(self._inference_times)
        avg_fps = 1000 / avg_time if avg_time > 0 else 0
        avg_det = np.mean(self._detection_counts)
        
        return {
            "avg_inference_time_ms": avg_time,
            "avg_fps": avg_fps,
            "avg_detections": avg_det,
            "total_frames": len(self._inference_times)
        }
    
    def reset_stats(self):
        """í†µê³„ ì´ˆê¸°í™”"""
        self._inference_times.clear()
        self._detection_counts.clear()
```

### 3.2 `YOLOTrainer` (í•™ìŠµ ê´€ë¦¬)

**íŒŒì¼:** `src/training/trainer.py`

```python
from ultralytics import YOLO
from typing import Dict, Optional
import yaml


class YOLOTrainer:
    """
    YOLOv8 í•™ìŠµ ê´€ë¦¬ì
    
    ì •í™•ë„ ìµœëŒ€í™” ì „ëµ:
        - 200 epochs (ì¶©ë¶„í•œ í•™ìŠµ)
        - Large model (YOLOv8l)
        - High resolution (640Ã—640)
        - Advanced augmentation
        - Patience: 50 (overfitting ë°©ì§€)
    """
    
    def __init__(
        self,
        model_name: str = 'yolov8l.pt',
        data_config: str = 'config/dataset.yaml',
        hyp_config: Optional[str] = None
    ):
        """
        Parameters:
            model_name: YOLOv8 model variant
            data_config: Dataset configuration path
            hyp_config: Hyperparameter config (optional)
        """
        self.model = YOLO(model_name)
        self.data_config = data_config
        self.hyp_config = hyp_config
    
    def train(
        self,
        epochs: int = 200,
        batch: int = 16,
        imgsz: int = 640,
        patience: int = 50,
        project: str = 'runs/train',
        name: str = 'exp',
        **kwargs
    ) -> Dict:
        """
        ëª¨ë¸ í•™ìŠµ
        
        Parameters:
            epochs: Training epochs
            batch: Batch size
            imgsz: Input image size
            patience: Early stopping patience
            project: Save directory
            name: Experiment name
            **kwargs: Additional YOLO arguments
        
        Returns:
            Training results (metrics)
        """
        # í•™ìŠµ ì‹¤í–‰
        results = self.model.train(
            data=self.data_config,
            epochs=epochs,
            batch=batch,
            imgsz=imgsz,
            patience=patience,
            project=project,
            name=name,
            
            # Optimizer
            optimizer='AdamW',
            lr0=0.001,
            lrf=0.01,
            momentum=0.937,
            weight_decay=0.0005,
            
            # Augmentation (ì •í™•ë„ í–¥ìƒ)
            hsv_h=0.015,
            hsv_s=0.7,
            hsv_v=0.4,
            degrees=10.0,
            translate=0.1,
            scale=0.5,
            fliplr=0.5,
            mosaic=1.0,
            mixup=0.1,
            
            # Training settings
            pretrained=True,
            freeze=10,  # Freeze first 10 layers
            save=True,
            save_period=10,
            verbose=True,
            
            # Device
            device=0 if torch.cuda.is_available() else 'cpu',
            
            **kwargs
        )
        
        return results
    
    def validate(
        self,
        weights: str,
        split: str = 'val'
    ) -> Dict:
        """
        ëª¨ë¸ ê²€ì¦
        
        Parameters:
            weights: Model weights path
            split: 'val' or 'test'
        
        Returns:
            Validation metrics
        """
        model = YOLO(weights)
        
        results = model.val(
            data=self.data_config,
            split=split,
            imgsz=640,
            batch=16,
            device=0 if torch.cuda.is_available() else 'cpu'
        )
        
        return {
            "mAP50": results.box.map50,
            "mAP50-95": results.box.map,
            "precision": results.box.p,
            "recall": results.box.r,
            "f1": results.box.f1
        }
```

### 3.3 `DatasetLoader` (ë°ì´í„° ê´€ë¦¬)

**íŒŒì¼:** `src/data/dataset.py`

```python
from pathlib import Path
from typing import Dict, List, Tuple
import yaml


class DatasetManager:
    """
    YOLO í¬ë§· ë°ì´í„°ì…‹ ê´€ë¦¬
    
    Directory Structure:
        datasets/
        â”œâ”€â”€ images/
        â”‚   â”œâ”€â”€ train/
        â”‚   â”œâ”€â”€ val/
        â”‚   â””â”€â”€ test/
        â””â”€â”€ labels/
            â”œâ”€â”€ train/  # .txt files
            â”œâ”€â”€ val/
            â””â”€â”€ test/
    
    Label Format (YOLO):
        Each .txt file contains:
        <class_id> <x_center> <y_center> <width> <height>
        
        All values normalized to [0, 1]:
        - x_center, y_center: Box center
        - width, height: Box dimensions
    """
    
    def __init__(self, dataset_root: str = 'datasets'):
        """
        Parameters:
            dataset_root: Dataset root directory
        """
        self.root = Path(dataset_root)
        self.image_dir = self.root / 'images'
        self.label_dir = self.root / 'labels'
    
    def verify_dataset(self) -> Dict:
        """
        ë°ì´í„°ì…‹ ë¬´ê²°ì„± ê²€ì¦
        
        Returns:
            {
                "train_images": int,
                "val_images": int,
                "test_images": int,
                "missing_labels": List[str],
                "empty_labels": List[str],
                "class_distribution": Dict[int, int]
            }
        """
        stats = {
            "train_images": len(list((self.image_dir / 'train').glob('*.jpg'))),
            "val_images": len(list((self.image_dir / 'val').glob('*.jpg'))),
            "test_images": len(list((self.image_dir / 'test').glob('*.jpg'))),
            "missing_labels": [],
            "empty_labels": [],
            "class_distribution": {}
        }
        
        # ê° split ê²€ì¦
        for split in ['train', 'val', 'test']:
            image_files = list((self.image_dir / split).glob('*.jpg'))
            
            for img_file in image_files:
                label_file = self.label_dir / split / f"{img_file.stem}.txt"
                
                # ë ˆì´ë¸” íŒŒì¼ ì¡´ì¬ í™•ì¸
                if not label_file.exists():
                    stats["missing_labels"].append(str(img_file))
                    continue
                
                # ë ˆì´ë¸” íŒŒì¼ ë‚´ìš© í™•ì¸
                with open(label_file, 'r') as f:
                    lines = f.readlines()
                
                if len(lines) == 0:
                    stats["empty_labels"].append(str(img_file))
                
                # í´ë˜ìŠ¤ ë¶„í¬ ê³„ì‚°
                for line in lines:
                    cls = int(line.split()[0])
                    stats["class_distribution"][cls] = \
                        stats["class_distribution"].get(cls, 0) + 1
        
        return stats
    
    def create_yaml_config(self, output_path: str = 'config/dataset.yaml'):
        """
        YOLO í•™ìŠµìš© YAML ìƒì„±
        """
        config = {
            'path': str(self.root.absolute()),
            'train': 'images/train',
            'val': 'images/val',
            'test': 'images/test',
            'nc': 5,
            'names': {
                0: 'traffic_cone',
                1: 'obstacle',
                2: 'robot_car',
                3: 'traffic_sign',
                4: 'pedestrian'
            }
        }
        
        with open(output_path, 'w') as f:
            yaml.dump(config, f, default_flow_style=False)
        
        print(f"âœ… Dataset config saved: {output_path}")
```

### 3.4 `MetricsCalculator` (í‰ê°€ ë©”íŠ¸ë¦­)

**íŒŒì¼:** `src/training/metrics.py`

```python
from typing import Dict, List
import numpy as np
from collections import defaultdict


class MetricsCalculator:
    """
    ê°ì²´ ê°ì§€ ë©”íŠ¸ë¦­ ê³„ì‚°
    
    Metrics:
        - mAP@0.5: Mean Average Precision at IoU=0.5
        - mAP@0.5:0.95: mAP averaged over IoU [0.5, 0.95]
        - Precision: TP / (TP + FP)
        - Recall: TP / (TP + FN)
        - F1-Score: 2 * P * R / (P + R)
    """
    
    @staticmethod
    def calculate_iou(box1: List[float], box2: List[float]) -> float:
        """
        ë‘ ë°•ìŠ¤ì˜ IoU ê³„ì‚°
        
        Parameters:
            box1, box2: [x1, y1, x2, y2]
        
        Returns:
            IoU: 0.0 ~ 1.0
        """
        # Intersection
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])
        
        inter_w = max(0, x2 - x1)
        inter_h = max(0, y2 - y1)
        inter_area = inter_w * inter_h
        
        # Union
        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
        union_area = box1_area + box2_area - inter_area
        
        if union_area == 0:
            return 0.0
        
        return inter_area / union_area
    
    @staticmethod
    def calculate_ap(
        precisions: List[float],
        recalls: List[float]
    ) -> float:
        """
        Average Precision ê³„ì‚° (11-point interpolation)
        
        Returns:
            AP: 0.0 ~ 1.0
        """
        # Recallì„ 0~1ë¡œ 11ê°œ í¬ì¸íŠ¸ì—ì„œ Precision ë³´ê°„
        ap = 0.0
        for r in np.linspace(0, 1, 11):
            # r ì´ìƒì˜ recallì—ì„œ ìµœëŒ€ precision
            p_max = 0.0
            for i, recall in enumerate(recalls):
                if recall >= r:
                    p_max = max(p_max, precisions[i])
            ap += p_max / 11
        
        return ap
    
    @staticmethod
    def calculate_map(
        predictions: List[Dict],
        ground_truths: List[Dict],
        iou_threshold: float = 0.5,
        num_classes: int = 5
    ) -> Dict:
        """
        mAP@IoU ê³„ì‚°
        
        Parameters:
            predictions: List of prediction dicts
            ground_truths: List of GT dicts
            iou_threshold: IoU threshold for TP
            num_classes: Number of classes
        
        Returns:
            {
                "mAP": float,
                "AP_per_class": Dict[int, float],
                "precision": float,
                "recall": float
            }
        """
        # í´ë˜ìŠ¤ë³„ AP ê³„ì‚°
        aps = []
        
        for cls in range(num_classes):
            # í•´ë‹¹ í´ë˜ìŠ¤ì˜ predictions & GTs í•„í„°ë§
            cls_preds = [
                p for p in predictions 
                if p['class'] == cls
            ]
            cls_gts = [
                gt for gt in ground_truths 
                if gt['class'] == cls
            ]
            
            if len(cls_gts) == 0:
                continue
            
            # Confidenceë¡œ ì •ë ¬
            cls_preds = sorted(cls_preds, key=lambda x: x['confidence'], reverse=True)
            
            # TP, FP ê³„ì‚°
            tp = np.zeros(len(cls_preds))
            fp = np.zeros(len(cls_preds))
            gt_matched = [False] * len(cls_gts)
            
            for i, pred in enumerate(cls_preds):
                max_iou = 0.0
                max_idx = -1
                
                for j, gt in enumerate(cls_gts):
                    if gt_matched[j]:
                        continue
                    
                    iou = MetricsCalculator.calculate_iou(
                        pred['box'], gt['box']
                    )
                    
                    if iou > max_iou:
                        max_iou = iou
                        max_idx = j
                
                if max_iou >= iou_threshold and max_idx >= 0:
                    tp[i] = 1
                    gt_matched[max_idx] = True
                else:
                    fp[i] = 1
            
            # Cumulative TP, FP
            tp_cumsum = np.cumsum(tp)
            fp_cumsum = np.cumsum(fp)
            
            # Precision & Recall
            precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-10)
            recalls = tp_cumsum / len(cls_gts)
            
            # AP ê³„ì‚°
            ap = MetricsCalculator.calculate_ap(
                precisions.tolist(), recalls.tolist()
            )
            aps.append(ap)
        
        # mAP
        mAP = np.mean(aps) if len(aps) > 0 else 0.0
        
        return {
            "mAP": mAP,
            "AP_per_class": {i: ap for i, ap in enumerate(aps)},
            "num_classes": len(aps)
        }
```

---

## 4. ì•Œê³ ë¦¬ì¦˜ ìƒì„¸

### 4.1 YOLOv8 Architecture

**Backbone: CSPDarknet53**

```
Input (640Ã—640Ã—3)
    â†“
Conv + BN + SiLU (64)
    â†“
CSP Block 1 (64 â†’ 128)   â†’ P1
    â†“
CSP Block 2 (128 â†’ 256)  â†’ P2
    â†“
CSP Block 3 (256 â†’ 512)  â†’ P3 (output)
    â†“
CSP Block 4 (512 â†’ 1024) â†’ P4 (output)
    â†“
CSP Block 5 (1024 â†’ 1024) â†’ P5 (output)
```

**Neck: PANet (Path Aggregation Network)**

```
P5 (1024) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“              â”‚
P4 (512) â”€â”€â”       â”‚
    â†“      â”‚ Concatâ”‚
P3 (256) â”€â”€â”¤       â”‚
           â”‚       â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚  Top-down path
    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Bottom-up path
    â†“
N3 (256) â†’ Head (Small objects)
N4 (512) â†’ Head (Medium objects)
N5 (1024) â†’ Head (Large objects)
```

**Head: Anchor-free Detection**

```
For each scale (N3, N4, N5):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Classification  â”‚ â†’ (C classes)
    â”‚ Head            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Box Regression  â”‚ â†’ (4 coordinates)
    â”‚ Head (DFL)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Objectness      â”‚ â†’ (1 score)
    â”‚ Head            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Total Predictions:**
- P3 (80Ã—80): 6,400 predictions
- P4 (40Ã—40): 1,600 predictions
- P5 (20Ã—20): 400 predictions
- **Total**: 8,400 predictions per image

### 4.2 Loss Function

**YOLOv8 Combined Loss:**

```
Total Loss = Î»_box * Box_Loss + Î»_cls * Class_Loss + Î»_dfl * DFL_Loss

where:
    Box_Loss = CIoU Loss (Complete IoU)
    Class_Loss = BCE Loss (Binary Cross Entropy)
    DFL_Loss = Distribution Focal Loss
    
    Î»_box = 7.5  (ë†’ê²Œ â†’ ì •í™•í•œ ë°•ìŠ¤)
    Î»_cls = 0.5  (ë‚®ê²Œ â†’ ë¹ ë¥¸ ìˆ˜ë ´)
    Î»_dfl = 1.5
```

**CIoU Loss (Complete IoU):**

```
CIoU = 1 - IoU + ÏÂ²(b, b_gt) / cÂ² + Î± * v

where:
    IoU: Intersection over Union
    ÏÂ²: Distance between box centers
    cÂ²: Diagonal length of enclosing box
    Î±: Trade-off parameter
    v: Aspect ratio consistency
```

**ì¥ì :**
- ë°•ìŠ¤ í¬ê¸°, ìœ„ì¹˜, ì¢…íš¡ë¹„ ëª¨ë‘ ê³ ë ¤
- ì •í™•í•œ ë°•ìŠ¤ íšŒê·€

### 4.3 Non-Maximum Suppression (NMS)

**ì•Œê³ ë¦¬ì¦˜:**

```python
def nms(boxes, scores, iou_threshold=0.45):
    """
    Class-wise NMS (ê° í´ë˜ìŠ¤ ë³„ë„ ì²˜ë¦¬)
    
    Parameters:
        boxes: (N, 4) [x1, y1, x2, y2]
        scores: (N,) confidence scores
        iou_threshold: IoU threshold
    
    Returns:
        keep_indices: ìœ ì§€í•  ë°•ìŠ¤ ì¸ë±ìŠ¤
    """
    # 1. Confidenceë¡œ ì •ë ¬
    indices = np.argsort(scores)[::-1]  # Descending
    
    keep = []
    
    while len(indices) > 0:
        # 2. ê°€ì¥ ë†’ì€ confidence ë°•ìŠ¤ ì„ íƒ
        current = indices[0]
        keep.append(current)
        
        if len(indices) == 1:
            break
        
        # 3. ë‚˜ë¨¸ì§€ ë°•ìŠ¤ì™€ IoU ê³„ì‚°
        ious = calculate_iou_batch(
            boxes[current:current+1],
            boxes[indices[1:]]
        )
        
        # 4. IoU < thresholdì¸ ë°•ìŠ¤ë§Œ ìœ ì§€
        indices = indices[1:][ious < iou_threshold]
    
    return keep
```

**ì •í™•ë„ í–¥ìƒì„ ìœ„í•œ ê°œì„ :**
- Class-wise NMS: ê° í´ë˜ìŠ¤ ë³„ë„ ì²˜ë¦¬
- Multi-scale NMS: ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ì—ì„œ ê°ì§€ëœ ë°•ìŠ¤ í†µí•©
- Soft-NMS (ì„ íƒì ): Hard suppression ëŒ€ì‹  confidence ê°ì†Œ

---

## 5. ì„¤ì • íŒŒì¼ ëª…ì„¸

### 5.1 Dataset Configuration

**íŒŒì¼:** `config/dataset.yaml`

```yaml
# Dataset Configuration for RC Track Object Detection
# YOLOv8 Format

# Paths
path: /Users/ahnhyunjun/Desktop/SEA_ME/-autonomous-driving_ML/03-object-detection/datasets
train: images/train
val: images/val
test: images/test

# Classes
nc: 5  # Number of classes

names:
  0: traffic_cone
  1: obstacle
  2: robot_car
  3: traffic_sign
  4: pedestrian

# Image settings
imgsz: 640  # Input image size (square)

# Annotation format
format: yolo  # YOLO txt format
```

### 5.2 Training Configuration

**íŒŒì¼:** `config/hyperparameters.yaml`

```yaml
# YOLOv8l Training Hyperparameters
# Focus: High Accuracy

# Model
model: yolov8l.pt
task: detect

# Training
epochs: 200
batch: 16
imgsz: 640
patience: 50  # Early stopping

# Optimizer
optimizer: AdamW
lr0: 0.001      # Initial learning rate
lrf: 0.01       # Final lr = lr0 * lrf = 0.00001
momentum: 0.937
weight_decay: 0.0005

# Loss weights (ì •í™•ë„ ìš°ì„ )
box: 7.5        # Box loss (ë†’ê²Œ)
cls: 0.5        # Class loss
dfl: 1.5        # Distribution Focal Loss

# Augmentation (aggressive for accuracy)
hsv_h: 0.015    # Hue
hsv_s: 0.7      # Saturation
hsv_v: 0.4      # Value (brightness)
degrees: 10.0   # Rotation
translate: 0.1  # Translation
scale: 0.5      # Scale (0.5Ã— ~ 1.5Ã—)
shear: 0.0      # No shear
perspective: 0.0  # No perspective
flipud: 0.0     # No vertical flip
fliplr: 0.5     # Horizontal flip
mosaic: 1.0     # Mosaic (4 images)
mixup: 0.1      # Mixup (blend 2 images)
copy_paste: 0.0  # No copy-paste

# Inference
conf: 0.25      # Confidence threshold
iou: 0.45       # NMS IoU threshold
max_det: 100    # Maximum detections

# Device
device: 0       # GPU 0 (or 'cpu')
workers: 8      # DataLoader workers

# Logging
project: runs/train
name: exp
exist_ok: False
pretrained: True
verbose: True
seed: 42

# Freeze
freeze: 10      # Freeze first 10 layers (backbone)
```

---

## 6. ë°ì´í„° íŒŒì´í”„ë¼ì¸

### 6.1 ë°ì´í„° ìˆ˜ì§‘

**ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸: `scripts/collect_data.py`**

```python
import cv2
import os
from datetime import datetime


class DataCollector:
    """
    RC íŠ¸ë™ ì´ë¯¸ì§€ ìˆ˜ì§‘
    
    ìˆ˜ì§‘ ì „ëµ:
        - ë‹¤ì–‘í•œ ì¡°ëª… (ë°ìŒ/ì–´ë‘ì›€/ê·¸ë¦¼ì)
        - ë‹¤ì–‘í•œ ê±°ë¦¬ (0.5m ~ 3m)
        - ë‹¤ì–‘í•œ ê°ë„ (ì •ë©´/ì¸¡ë©´/ëŒ€ê°)
        - ë‹¤ì–‘í•œ ê°ì²´ ë°°ì¹˜
    """
    
    def __init__(self, output_dir: str = 'datasets/raw'):
        """
        Parameters:
            output_dir: Output directory for collected images
        """
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # ì¹´ë©”ë¼ ì´ˆê¸°í™” (Pi Camera or USB)
        self.cap = cv2.VideoCapture(0)
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    
    def collect_images(self, num_images: int = 100):
        """
        ì´ë¯¸ì§€ ìˆ˜ì§‘ (í‚¤ë³´ë“œ ì¸í„°ë™ì…˜)
        
        í‚¤ ë°”ì¸ë”©:
            SPACE: ì´ë¯¸ì§€ ì €ì¥
            'q': ì¢…ë£Œ
            'p': ì¼ì‹œì •ì§€
        """
        count = 0
        paused = False
        
        print(f"ğŸ“¸ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì‹œì‘ (ëª©í‘œ: {num_images}ì¥)")
        print("SPACE: ì €ì¥ | 'p': ì¼ì‹œì •ì§€ | 'q': ì¢…ë£Œ")
        
        while count < num_images:
            if not paused:
                ret, frame = self.cap.read()
                
                if not ret:
                    print("âŒ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
                    break
                
                # í”„ë ˆì„ í‘œì‹œ
                cv2.putText(frame, f"Count: {count}/{num_images}", 
                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 
                           1, (0, 255, 0), 2)
                cv2.imshow('Data Collection', frame)
            
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord(' '):  # SPACE: ì €ì¥
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
                filename = f"img_{count:04d}_{timestamp}.jpg"
                filepath = os.path.join(self.output_dir, filename)
                
                cv2.imwrite(filepath, frame)
                print(f"âœ… Saved: {filename}")
                count += 1
            
            elif key == ord('p'):  # Pause
                paused = not paused
                print("â¸ï¸ Paused" if paused else "â–¶ï¸ Resumed")
            
            elif key == ord('q'):  # Quit
                break
        
        self.cap.release()
        cv2.destroyAllWindows()
        print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ: {count}ì¥")
```

### 6.2 ë°ì´í„° ë ˆì´ë¸”ë§

**ë„êµ¬: CVAT (Computer Vision Annotation Tool)**

**ì›Œí¬í”Œë¡œìš°:**

```
1. CVAT ì„¤ì¹˜ & ì‹¤í–‰
   docker run -p 8080:8080 cvat/server

2. í”„ë¡œì íŠ¸ ìƒì„±
   - Name: "RC Track Object Detection"
   - Labels: traffic_cone, obstacle, robot_car, traffic_sign, pedestrian

3. ì´ë¯¸ì§€ ì—…ë¡œë“œ
   - datasets/raw/ í´ë”ì˜ 1000ì¥ ì—…ë¡œë“œ

4. ë ˆì´ë¸”ë§ (ì•½ 40ì‹œê°„)
   - Bounding box ê·¸ë¦¬ê¸°
   - í´ë˜ìŠ¤ í• ë‹¹
   - í’ˆì§ˆ ê²€í† 

5. Export
   - Format: YOLO 1.1
   - Download annotations

6. í›„ì²˜ë¦¬
   - YOLO í¬ë§·ìœ¼ë¡œ ë³€í™˜
   - Train/val/test split
```

**ë ˆì´ë¸”ë§ í’ˆì§ˆ ê´€ë¦¬:**
```python
# scripts/validate_annotations.py

def validate_yolo_labels(label_dir: str):
    """
    YOLO ë ˆì´ë¸” ê²€ì¦
    
    ì²´í¬ì‚¬í•­:
        - í´ë˜ìŠ¤ ID [0, 4] ë²”ìœ„ ë‚´
        - ì¢Œí‘œ [0, 1] ë²”ìœ„ ë‚´
        - Width, height > 0
        - ì¤‘ë³µ ë°•ìŠ¤ ì—†ìŒ
    """
    errors = []
    
    for label_file in Path(label_dir).glob('*.txt'):
        with open(label_file, 'r') as f:
            lines = f.readlines()
        
        for i, line in enumerate(lines):
            parts = line.strip().split()
            
            if len(parts) != 5:
                errors.append(f"{label_file.name}:{i} - Invalid format")
                continue
            
            cls, x, y, w, h = map(float, parts)
            
            # ê²€ì¦
            if not (0 <= cls <= 4):
                errors.append(f"{label_file.name}:{i} - Invalid class {cls}")
            
            if not (0 <= x <= 1 and 0 <= y <= 1):
                errors.append(f"{label_file.name}:{i} - Invalid center ({x}, {y})")
            
            if not (0 < w <= 1 and 0 < h <= 1):
                errors.append(f"{label_file.name}:{i} - Invalid size ({w}, {h})")
    
    return errors
```

### 6.3 Train/Val/Test Split

**ë¶„í•  ìŠ¤í¬ë¦½íŠ¸: `scripts/split_dataset.py`**

```python
import shutil
import random
from pathlib import Path


def split_dataset(
    image_dir: str,
    label_dir: str,
    output_dir: str,
    split_ratio: Tuple[float, float, float] = (0.7, 0.15, 0.15),
    seed: int = 42
):
    """
    ë°ì´í„°ì…‹ì„ Train/Val/Testë¡œ ë¶„í• 
    
    Parameters:
        image_dir: Raw images directory
        label_dir: Raw labels directory
        output_dir: Output directory
        split_ratio: (train, val, test)
        seed: Random seed
    """
    random.seed(seed)
    
    # ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡
    image_files = list(Path(image_dir).glob('*.jpg'))
    random.shuffle(image_files)
    
    # ë¶„í•  ê³„ì‚°
    total = len(image_files)
    train_end = int(total * split_ratio[0])
    val_end = train_end + int(total * split_ratio[1])
    
    train_files = image_files[:train_end]
    val_files = image_files[train_end:val_end]
    test_files = image_files[val_end:]
    
    print(f"ğŸ“Š Dataset split:")
    print(f"  Train: {len(train_files)} ({len(train_files)/total*100:.1f}%)")
    print(f"  Val:   {len(val_files)} ({len(val_files)/total*100:.1f}%)")
    print(f"  Test:  {len(test_files)} ({len(test_files)/total*100:.1f}%)")
    
    # íŒŒì¼ ë³µì‚¬
    for split_name, files in [
        ('train', train_files),
        ('val', val_files),
        ('test', test_files)
    ]:
        img_out = Path(output_dir) / 'images' / split_name
        lbl_out = Path(output_dir) / 'labels' / split_name
        
        img_out.mkdir(parents=True, exist_ok=True)
        lbl_out.mkdir(parents=True, exist_ok=True)
        
        for img_file in files:
            # ì´ë¯¸ì§€ ë³µì‚¬
            shutil.copy(img_file, img_out / img_file.name)
            
            # ë ˆì´ë¸” ë³µì‚¬
            lbl_file = Path(label_dir) / f"{img_file.stem}.txt"
            if lbl_file.exists():
                shutil.copy(lbl_file, lbl_out / lbl_file.name)
    
    print("âœ… Split completed")
```

---

## 7. API ë¬¸ì„œ

### 7.1 í•™ìŠµ API

```python
# train.py

from src.training.trainer import YOLOTrainer

# Trainer ì´ˆê¸°í™”
trainer = YOLOTrainer(
    model_name='yolov8l.pt',
    data_config='config/dataset.yaml'
)

# í•™ìŠµ ì‹œì‘
results = trainer.train(
    epochs=200,
    batch=16,
    imgsz=640,
    patience=50,
    project='runs/train',
    name='yolov8l_accuracy'
)

# ê²°ê³¼ í™•ì¸
print(f"Best mAP@0.5: {results.box.map50:.4f}")
print(f"Best mAP@0.5:0.95: {results.box.map:.4f}")
```

### 7.2 ì¶”ë¡  API

```python
# detect.py

from src.detector import ObjectDetector
import cv2

# Detector ì´ˆê¸°í™”
detector = ObjectDetector(
    weights='runs/train/yolov8l_accuracy/weights/best.pt',
    device='cuda',
    conf_thres=0.25,
    iou_thres=0.45
)

# ë‹¨ì¼ ì´ë¯¸ì§€ ê°ì§€
image = cv2.imread('test_image.jpg')
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

results = detector.detect(image, return_image=True)

# ê²°ê³¼ ì¶œë ¥
print(f"Detected {results['num_detections']} objects")
for cls_name, conf in zip(results['class_names'], results['confidences']):
    print(f"  - {cls_name}: {conf:.2f}")

# ê²°ê³¼ ì‹œê°í™”
cv2.imshow('Detection', cv2.cvtColor(results['image_annotated'], cv2.COLOR_RGB2BGR))
cv2.waitKey(0)
```

### 7.3 í‰ê°€ API

```python
# validate.py

from src.training.trainer import YOLOTrainer

trainer = YOLOTrainer(data_config='config/dataset.yaml')

# Test set í‰ê°€
metrics = trainer.validate(
    weights='runs/train/yolov8l_accuracy/weights/best.pt',
    split='test'
)

print(f"mAP@0.5:      {metrics['mAP50']:.4f}")
print(f"mAP@0.5:0.95: {metrics['mAP50-95']:.4f}")
print(f"Precision:    {metrics['precision']:.4f}")
print(f"Recall:       {metrics['recall']:.4f}")
print(f"F1-Score:     {metrics['f1']:.4f}")
```

---

## 8. ì—ëŸ¬ ì²˜ë¦¬

### 8.1 ì˜ˆì™¸ ì •ì˜

```python
# src/utils/exceptions.py

class DetectionError(Exception):
    """Base exception for detection errors"""
    pass

class ModelLoadError(DetectionError):
    """ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"""
    pass

class InferenceError(DetectionError):
    """ì¶”ë¡  ì‹¤íŒ¨"""
    pass

class DataValidationError(DetectionError):
    """ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨"""
    pass
```

### 8.2 ì—ëŸ¬ ì²˜ë¦¬ íŒ¨í„´

```python
def detect_with_fallback(self, image: np.ndarray) -> Dict:
    """
    Graceful degradationì„ ì ìš©í•œ ê°ì§€
    """
    try:
        # GPU ì¶”ë¡  ì‹œë„
        return self.detect(image)
    
    except torch.cuda.OutOfMemoryError:
        # GPU ë©”ëª¨ë¦¬ ë¶€ì¡± â†’ CPU fallback
        print("âš ï¸ GPU OOM, falling back to CPU")
        self.model.to('cpu')
        return self.detect(image)
    
    except InferenceError as e:
        # ì¶”ë¡  ì‹¤íŒ¨ â†’ ë¹ˆ ê²°ê³¼ ë°˜í™˜
        print(f"âŒ Inference failed: {e}")
        return {
            "boxes": [],
            "classes": [],
            "confidences": [],
            "class_names": [],
            "num_detections": 0,
            "error": str(e)
        }
```

---

## 9. í†µí•© ì¸í„°í˜ì´ìŠ¤

### 9.1 Module 01, 02 í†µí•©

```python
# integration_example.py

from module_01 import LaneDetector
from module_02 import LaneKeepingAssist
from module_03 import ObjectDetector

# ì´ˆê¸°í™”
lane_detector = LaneDetector(weights='module_01/best.pth')
lkas = LaneKeepingAssist()
object_detector = ObjectDetector(weights='module_03/best.pt')

# í”„ë ˆì„ ì²˜ë¦¬
frame = capture_frame()

# 1. ì°¨ì„  ê°ì§€
lane_output = lane_detector.detect(frame)

# 2. ê°ì²´ ê°ì§€
object_output = object_detector.detect(frame)

# 3. ì¶©ëŒ ì²´í¬
obstacles_in_lane = check_lane_overlap(
    lane_mask=lane_output['lane_mask'],
    object_boxes=object_output['boxes']
)

# 4. ì œì–´ ê²°ì •
if obstacles_in_lane:
    # ê¸´ê¸‰ íšŒí”¼
    control = lkas.emergency_brake()
else:
    # ì •ìƒ ì£¼í–‰
    control = lkas.process_frame(lane_output, vehicle_state)
```

---

**ì‘ì„± ì™„ë£Œ!**  
ë‹¤ìŒ: ê²€ì¦ì„œ ì‘ì„±
