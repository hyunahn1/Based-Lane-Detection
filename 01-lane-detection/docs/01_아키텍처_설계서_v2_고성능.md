# RC카 자율주행 시스템 아키텍처 설계서 v2.0 (고성능 버전)

## 🎯 설계 철학

**핵심 전략**: "일단 최고 성능부터, 최적화는 나중에"

- **환경**: 단일 RC 트랙 (일반화 불필요)
- **하드웨어**: RTX 5090 (제약 없음)
- **데이터**: 199개 (증강으로 보완)
- **우선순위**: 성능 > 속도 > 메모리

---

## 1. 프로젝트 개요

### 1.1 목적
실내 RC 트랙에서 **최고 정확도**의 차선 인식 및 추종 시스템 개발

### 1.2 데이터 현황
- **이미지**: 199개 (640×480, RGB)
- **어노테이션**: 199개 (1:1 매칭, polyline)
- **환경**: 단일 실내 트랙, 고정 조명
- **클래스**: 단일 (lane)

### 1.3 하드웨어 환경

#### 학습 환경 (제약 없음)
- **GPU**: RTX 5090 (24GB VRAM)
- **CPU**: 고성능 멀티코어
- **RAM**: 충분
- **스토리지**: SSD 권장

#### 배포 환경 (나중에 고려)
- RC카 하드웨어 (추후 결정)
- 경량화/최적화는 Phase 2

### 1.4 전략적 결정

✅ **채택**:
- 최신 고성능 모델
- 큰 입력 해상도 (640×480 or higher)
- 대용량 배치 학습
- 앙상블/TTA (Test-Time Augmentation)
- 최신 프레임워크 활용

❌ **배제** (일단):
- 모델 경량화
- Quantization
- 실시간 제약
- 임베디드 고려

---

## 2. 모델 아키텍처

### 2.1 모델 선택지

#### 옵션 A: DeepLabV3+ (ResNet101) ⭐ 추천
```yaml
Backbone: ResNet101 (ImageNet pretrained)
Decoder: ASPP (Atrous Spatial Pyramid Pooling)
Input: 640×480
Output: 640×480 binary mask
Parameters: ~59M
FLOPs: ~200G

장점:
✅ 검증된 성능 (SOTA baseline)
✅ 사전학습 모델 풍부
✅ Segmentation 특화
✅ Multi-scale feature extraction

단점:
❌ 무거움 (RTX 5090에서는 문제없음)
❌ 추론 느림 (배포 시 최적화 필요)

예상 성능: IoU 0.85~0.90
```

#### 옵션 B: Segformer (MIT-B3)
```yaml
Backbone: Mix Transformer (hierarchical)
Decoder: Lightweight MLP
Input: 640×480
Output: 640×480
Parameters: ~47M

장점:
✅ Transformer 기반 (최신)
✅ 성능 우수
✅ 효율적 구조

단점:
❌ 데이터 요구량 높음 (199개는 부족)

예상 성능: IoU 0.80~0.88 (데이터 부족 시 불안정)
```

#### 옵션 C: HRNet + OCR
```yaml
Backbone: HRNet-W48
Head: Object-Contextual Representations
Input: 640×480
Parameters: ~70M

장점:
✅ High-resolution 유지
✅ 세밀한 경계 검출

단점:
❌ 복잡함
❌ 학습 까다로움

예상 성능: IoU 0.88~0.92 (best case)
```

### 2.2 최종 선택: **DeepLabV3+ (ResNet101)**

**선택 이유**:
1. ✅ 안정적이고 검증된 아키텍처
2. ✅ 사전학습 모델 풍부 (Transfer Learning 용이)
3. ✅ 199개 데이터에서도 좋은 성능
4. ✅ COCO로 변환 후 바로 학습 가능
5. ✅ PyTorch, MMSegmentation 지원 완벽

**모델 구조**:
```
Input Image (640×480×3)
    ↓
ResNet101 Backbone (ImageNet pretrained)
├─ Low-level features (256 channels)
└─ High-level features (2048 channels)
    ↓
ASPP Module (Multi-scale context)
├─ 1×1 conv
├─ 3×3 conv, rate=6
├─ 3×3 conv, rate=12
├─ 3×3 conv, rate=18
└─ Global Average Pooling
    ↓
Decoder (Concatenate + Refine)
    ↓
Output (640×480×1) - Lane Mask
```

---

## 2.3 데이터 파이프라인 및 분할 전략

```
training_data/
├── images/          (199개)
├── annotations/     (199개)
└── convert_coco.py
     ↓
[1단계: 원본 데이터 분할 - 데이터 누수 방지]
├── Train: 139개 (70%)
├── Val: 30개 (15%)
└── Test: 30개 (15%)
     ↓
[2단계: Train에만 증강 적용]
├── Train 증강: 139개 → 695개 (5배)
├── Val: 30개 (증강 X, 원본만)
└── Test: 30개 (증강 X, 원본만)
     ↓
[최종 데이터셋]
├── Train: 695개 (증강됨)
├── Val: 30개 (원본)
└── Test: 30개 (원본)
```

### 분할 전략의 전문가적 고민

#### 1. 분할 비율 선정 근거 (70/15/15)

| 비율 | 개수 | 선택 이유 |
|------|------|-----------|
| **Train 70%** | 139개 → 695개 | • 199개는 적은 편이므로 학습에 최대한 활용<br>• 증강 후 695개로 충분한 학습 가능<br>• 단일 환경이므로 다양성보다 수량 중요 |
| **Val 15%** | 30개 | • 통계적 유의성: 최소 30개 샘플 권장<br>• Early stopping, 하이퍼파라미터 튜닝용<br>• 오버피팅 모니터링에 충분<br>• 원본만 사용 → 실제 성능 반영 |
| **Test 15%** | 30개 | • 최종 성능 평가용 (한 번도 안 본 데이터)<br>• 30개로 IoU 95% 신뢰구간 ±0.05<br>• 편향 없는 객관적 평가 |

#### 2. 대안 전략 검토 및 배제 이유

| 전략 | 설명 | 채택 여부 | 이유 |
|------|------|-----------|------|
| **K-Fold CV** | 5-Fold Cross-Validation | ❌ 배제 | • 학습 시간 5배 증가 (RTX 5090이어도 40시간)<br>• 단일 환경에서 큰 이득 없음<br>• Hold-out으로 충분 |
| **Stratified Split** | 클래스별 균등 분할 | ❌ 불필요 | • 단일 클래스 (lane only)<br>• Stratification 의미 없음 |
| **80/10/10** | Train 더 많이 | ❌ 배제 | • Val 19개는 통계적으로 부족 (최소 30개)<br>• Test 19개도 신뢰구간 너무 넓음 |
| **60/20/20** | Val/Test 더 많이 | ❌ 배제 | • Train 119개는 학습에 부족<br>• 증강 후에도 595개밖에 안됨 |
| **Leave-One-Out** | 극단적 CV | ❌ 배제 | • 199번 학습 필요 (비현실적)<br>• 단일 샘플 검증은 분산 큼 |

#### 3. 시간적 의존성 고려 (중요!)

```yaml
문제 인식:
  - 연속된 비디오 프레임일 가능성 (frame_0013, 0016, 0017...)
  - 인접 프레임은 매우 유사 → 데이터 누수 위험
  
해결 방법:
  1. 프레임 번호 기준 균등 분할
     - Train: frame_0013~0139 (초반)
     - Val: frame_0140~0169 (중반)
     - Test: frame_0170~0252 (후반)
     ❌ 문제: 트랙 위치 편향
  
  2. ✅ 랜덤 셔플 + Gap 강제 (채택)
     - 프레임 번호 차이 ≥ 3인 것만 다른 세트에 배치
     - 예: frame_0013이 Train → frame_0014~0016은 Train에만
     - 유사 프레임 누수 방지
  
  3. 완전 랜덤 (위험)
     - 인접 프레임이 Train/Val에 섞임
     - 성능 과대평가 위험
```

#### 4. 증강 데이터 처리 전략

```yaml
핵심 원칙: "증강은 Train에만, Val/Test는 순수하게"

잘못된 방법 ❌:
  - 전체 199개 → 995개 증강 → 분할
  - 문제: Val/Test에 증강 데이터 포함
  - 결과: 실제 성능보다 높게 측정 (착시)
  
올바른 방법 ✅:
  1. 원본 199개를 먼저 분할
  2. Train 139개만 증강 (5배 → 695개)
  3. Val/Test는 원본 그대로
  
이유:
  - Val: 조기 종료 판단 (실제 성능 반영 필요)
  - Test: 최종 평가 (편향 없는 순수 데이터)
  - Train: 증강으로 다양성 확보
```

#### 5. 검증 전략의 신뢰도 분석

```python
# 통계적 신뢰구간 계산
import scipy.stats as stats
import numpy as np

def confidence_interval(n_samples, iou_mean, confidence=0.95):
    """
    n_samples개로 측정한 IoU의 신뢰구간
    """
    # 가정: IoU는 베타 분포 근사
    # 간단한 추정: 정규분포 근사
    std_error = 0.1 / np.sqrt(n_samples)  # 경험적
    z_score = stats.norm.ppf((1 + confidence) / 2)
    margin = z_score * std_error
    return margin

# Val 30개로 측정 시
val_margin = confidence_interval(30, 0.85)
print(f"Val IoU 0.85 ± {val_margin:.3f}")  # 약 ±0.036

# Test 30개로 측정 시
test_margin = confidence_interval(30, 0.85)
print(f"Test IoU 0.85 ± {test_margin:.3f}")  # 약 ±0.036

# 만약 Test 20개라면?
test20_margin = confidence_interval(20, 0.85)
print(f"Test IoU 0.85 ± {test20_margin:.3f}")  # 약 ±0.044 (너무 넓음)

# 결론: 30개는 신뢰구간 ±3.6%로 적절
```

#### 6. 실무적 결정 사항

| 결정 사항 | 선택 | 근거 |
|-----------|------|------|
| **분할 시점** | 원본 분할 → 증강 | 데이터 누수 방지, 순수 평가 |
| **분할 방법** | 층화 랜덤 (frame gap) | 시간적 의존성 고려 |
| **재현성** | Random Seed 고정 (42) | 실험 재현 가능 |
| **검증 빈도** | 5 epoch마다 | 학습 시간 vs 정보량 트레이드오프 |
| **최종 모델** | Best Val IoU | Early Stopping 기준 |
| **앙상블** | 다른 Seed 3개 | Seed 편향 제거 |

#### 7. 데이터 부족 완화 전략 (199개 → 효과적 활용)

```yaml
전략 1: Self-Training (준지도 학습)
  - 라벨 없는 167개 이미지 활용 가능
  - High confidence 예측 → Pseudo Label
  - Train에 추가 → 재학습
  - 예상 효과: IoU +2~3%
  
전략 2: Mixup (데이터 다양성)
  - Train 이미지 간 혼합
  - α=0.2 (약하게)
  - 예상 효과: 일반화 +1~2%
  
전략 3: Test-Time Augmentation
  - Test 시 여러 변형 예측 → 평균
  - 추론 시간 3~5배 증가
  - 예상 효과: IoU +2~3%
  
전략 4: Transfer Learning
  - ImageNet 사전학습 (필수)
  - COCO/ADE20K fine-tuning (선택)
  - 예상 효과: 수렴 속도 3배 빠름
```

---

## 3. 학습 전략

### 3.1 학습 방법론의 전문가적 고민

**전체 학습 철학**: "데이터 부족을 방법론으로 극복"

#### 핵심 질문과 답변

**Q1: 왜 70/15/15 비율인가?**
- **A**: 199개라는 제한된 데이터에서 최적 균형점
  - Train 70% (139개): 최소 학습 요구량 충족
  - Val 15% (30개): 통계적 유의성 최소 기준 (n≥30)
  - Test 15% (30개): 신뢰구간 ±3.6% (acceptable)
  - 60/20/20: Train 부족 (119개 → 증강 후 595개 부족)
  - 80/10/10: Val/Test 각 19개 (통계적으로 불안정)

**Q2: K-Fold를 왜 안 쓰는가?**
- **A**: 비용 대비 효과 낮음
  - 5-Fold CV = 학습 5번 = 40시간 (RTX 5090 기준)
  - 단일 트랙 환경 → 일반화보다 과적합 방지 중요
  - Hold-out (70/15/15)로 충분히 신뢰 가능
  - ✅ 대신: 다른 Seed로 3번 학습 → 앙상블

**Q3: 증강을 언제 적용하는가?**
- **A**: Train에만 적용 (Val/Test는 순수 원본)
  - ❌ 잘못: 전체 증강 → 분할 (Val/Test 오염)
  - ✅ 올바름: 분할 → Train만 증강
  - 이유: Val은 실제 성능 측정, Test는 최종 평가

**Q4: 시간적 의존성은 어떻게 처리?**
- **A**: Frame Gap 강제 (인접 프레임 분리)
  - 문제: frame_0013, 0016, 0017... (연속 프레임 가능)
  - 해결: 인접 프레임(gap<3)은 같은 세트에 배치
  - 구현: 프레임 번호 기준 블록 분할 후 랜덤 배치

**Q5: 검증 전략의 신뢰도는?**
- **A**: 30개 샘플로 IoU ±3.6% 신뢰구간 (95% CI)
  - 충분한 신뢰도 (실무 기준 ±5% 이내)
  - Val과 Test 모두 30개로 일관성 유지

#### 학습 전략 결정 트리

```
데이터 199개 → 충분한가?
    ↓ NO
전문가 전략 적용 필요
    ↓
1. 분할 비율 최적화
   └→ 70/15/15 (통계적 근거)
    ↓
2. 증강 전략
   └→ Train만 5배 증강
    ↓
3. Transfer Learning
   └→ ImageNet pretrained (필수)
    ↓
4. 정규화 기법
   └→ Dropout, Label Smoothing, EMA
    ↓
5. 검증 전략
   └→ Hold-out + Seed 앙상블
    ↓
6. 추가 기법
   └→ TTA, Self-Training (선택)
```

### 3.2 데이터 증강 (Aggressive)

**오프라인 증강** (학습 전):
```python
원본 Train 139개 → 증강 695개 (5배)

⚠️ 중요: Val/Test는 증강하지 않음 (원본 30개 유지)

증강 기법 (조합):
1. 회전: ±15° (과도한 회전은 트랙 특성 손실)
2. 이동: ±10%
3. 스케일: 0.9~1.1 (보수적)
4. 색상:
   - Brightness: ±20% (조명 변화 대응)
   - Contrast: ±20%
   - Saturation: ±15%
5. 노이즈 (약하게):
   - Gaussian Noise (σ=0.01~0.02)
   - Motion Blur (kernel=3~5)
6. ❌ 왜곡 제외:
   - Elastic Transform, Grid Distortion은 과도한 변형
   - 단일 트랙 환경에서 오히려 해로울 수 있음
```

**온라인 증강** (학습 중):
```python
- RandomCrop 95% (약간만 crop)
- ColorJitter (추가 색상 변화)
- RandomBrightnessContrast
- GaussianBlur (p=0.3)
```

**중요**: 수평 반전 ❌ (차선 방향성 유지)

### 3.3 Transfer Learning 전략

**전문가적 고민: "어떤 사전학습을 어떻게 활용할까?"**

#### 선택지 분석

| 사전학습 | 장점 | 단점 | 선택 |
|---------|------|------|------|
| **ImageNet** | • 1.2M 이미지<br>• 범용 특징<br>• 검증됨 | • 차선과 무관 | ✅ 필수 |
| **COCO** | • Segmentation 특화<br>• 80 클래스 | • 차선 없음 | ⚠️ 선택적 |
| **ADE20K** | • Scene 이해<br>• 150 클래스 | • 실내 트랙과 다름 | ❌ 불필요 |
| **CULane** | • 차선 특화!<br>• 88K 이미지 | • 실외 도로<br>• 도메인 차이 | ⚠️ 고려 |
| **From Scratch** | • 완전 학습 | • 199개로 불가능 | ❌ 절대 불가 |

**최종 선택**: ImageNet (필수) + 선택적 COCO

#### Fine-tuning 전략의 과학적 근거

```yaml
전략 1: Differential Learning Rate (채택) ⭐
  
  이유:
    - Backbone (ResNet101): 범용 특징 (edges, corners)
    - Decoder (ASPP): 태스크 특화 (차선 패턴)
    - Backbone은 천천히, Decoder는 빠르게
  
  설정:
    - Backbone LR: 1e-5 (ImageNet 지식 유지)
    - ASPP LR: 1e-4 (10배 빠름)
    - 이유: 하위 층은 이미 좋은 특징 학습됨
  
  근거 논문:
    - "Discriminative Learning Rates" (Howard & Ruder, 2018)
    - 실험적으로 10-100배 차이가 최적

전략 2: Gradual Unfreezing (대안)
  
  방법:
    - Phase 1 (Epoch 0-20): Backbone freeze, Decoder만 학습
    - Phase 2 (Epoch 21-50): 전체 학습 (Differential LR)
    - Phase 3 (Epoch 51+): LR 감소 (Fine-tuning)
  
  장점: 더 안정적 학습
  단점: 복잡함, 시간 증가
  판단: ❌ 불필요 (RTX 5090에서 안정적 학습 가능)

전략 3: Layer-wise LR Decay
  
  방법:
    - Layer 1: LR × 0.1
    - Layer 2: LR × 0.3
    - Layer 3: LR × 0.5
    - Layer 4: LR × 0.7
    - Decoder: LR × 1.0
  
  장점: 세밀한 제어
  단점: 하이퍼파라미터 과다
  판단: ❌ 오버엔지니어링
```

#### 실제 구현

```python
# MMSegmentation Config
optim_wrapper = dict(
    type='OptimWrapper',
    optimizer=dict(
        type='AdamW',
        lr=0.0001,  # Base LR (Decoder)
        betas=(0.9, 0.999),
        weight_decay=0.0001
    ),
    paramwise_cfg=dict(
        custom_keys={
            'backbone': dict(lr_mult=0.1),  # 1e-5 (10배 느림)
        }
    )
)

# Warmup 전략 (중요!)
param_scheduler = [
    dict(
        type='LinearLR',
        start_factor=1e-6,  # 매우 작게 시작
        by_epoch=False,
        begin=0,
        end=500  # 500 iteration warmup
    ),
    dict(
        type='CosineAnnealingLR',
        T_max=200,
        eta_min=1e-6,
        by_epoch=True,
        begin=0,
        end=200
    )
]

# 이유:
# - Warmup: 초기 불안정 방지 (pretrained → task 적응)
# - Cosine: 부드러운 LR 감소 (local minima 탈출)
```

### 3.4 정규화 전략 (Overfitting 방지)

**전문가적 고민: "199개로 어떻게 일반화할까?"**

#### 정규화 기법 조합

```yaml
기법 1: Dropout (필수) ✅
  위치: ASPP, Decoder
  비율: 0.1 (MMSeg 기본)
  근거: 파라미터 60M → 오버피팅 위험
  
기법 2: Label Smoothing (추천) ✅
  수식: y_smooth = (1-ε)y + ε/C
  ε: 0.1
  근거: Hard label (0 or 1) → Soft (0.05 or 0.95)
  효과: 모델 과신 방지, 일반화 향상
  
기법 3: Exponential Moving Average (강력 추천) ✅
  수식: θ_ema = α × θ_ema + (1-α) × θ
  α: 0.9999
  근거: 파라미터 평균화 → 안정적 추론
  효과: Test 성능 +1~2%
  
기법 4: Stochastic Depth (선택) ⚠️
  방법: ResNet block 랜덤 skip
  비율: 0.2 (20% 확률로 skip)
  근거: Dropout의 구조적 버전
  효과: 경미한 정규화
  
기법 5: Weight Decay (필수) ✅
  값: 1e-4
  근거: L2 regularization (표준)
  효과: 가중치 크기 제한
  
기법 6: Mixup (고려) ⚠️
  수식: x_mix = λx_i + (1-λ)x_j
  λ: Beta(0.2, 0.2)
  우려: Segmentation에서 경계 모호
  판단: 실험적 적용 (optional)
```

#### 정규화 효과 예측

```python
# 경험적 추정 (문헌 기반)
baseline_iou = 0.80  # 정규화 없을 때

improvements = {
    'Transfer Learning': +0.15,  # 가장 큰 영향
    'Label Smoothing': +0.02,
    'EMA': +0.02,
    'Dropout': +0.01,
    'Weight Decay': +0.01,
    'TTA': +0.03,
}

expected_iou = baseline_iou + sum(improvements.values())
# = 0.80 + 0.24 = 1.04 (불가능)
# 실제: 비선형 조합, 상한선 존재
# 현실적 기대: 0.85~0.88
```

### 3.3 손실 함수 (단순화 - 실용적)

```python
# ✅ 실제 적용 (MMSeg 표준)
Total Loss = CrossEntropy Loss + 3.0 × Dice Loss

1. CrossEntropy Loss (weight=1.0)
   - 픽셀 단위 분류
   - 클래스 불균형 대응 (class weights 적용 가능)

2. Dice Loss (weight=3.0)
   - IoU 직접 최적화
   - Segmentation 품질 향상
   - 더 높은 가중치 (Dice 강조)

⚠️ 복잡한 손실 함수 조합은 배제
이유:
- Focal, Tversky, Boundary Loss는 구현/튜닝 복잡
- 199개 데이터에서는 단순한 것이 더 안정적
- CrossEntropy + Dice로 충분 (검증된 조합)
```

### 3.4 하이퍼파라미터

```yaml
# 고성능 설정
optimizer: AdamW
  learning_rate: 1e-4 (backbone: 1e-5)
  weight_decay: 1e-4
  betas: [0.9, 0.999]

scheduler: CosineAnnealingWarmRestarts
  T_0: 10  # 첫 restart 주기
  T_mult: 2  # 주기 증가율
  eta_min: 1e-6
  warmup_epochs: 5

batch_size: 64 (RTX 5090에서 가능)
  - Mixed Precision (FP16) 사용
  - Gradient Accumulation if needed

epochs: 200 (충분히 수렴)
  early_stopping:
    patience: 30 (충분한 여유)
    monitor: val_iou
    mode: max

regularization:
  - Dropout: 0.1 (MMSeg 기본, ASPP/Decoder)
  - ❌ Label Smoothing: 구현 복잡 (배제)
  - ❌ Stochastic Depth: 구현 복잡 (배제)
  
⚠️ 단순함 유지: 과도한 트릭은 디버깅 어렵게 만듦
```

### 3.5 고급 학습 기법 (선택적)

```yaml
1. Exponential Moving Average (EMA)
   - 모델 파라미터 평균화
   - 더 안정적인 추론
   - decay: 0.9999

2. Progressive Resizing
   - Epoch 0-50: 320×240
   - Epoch 51-100: 480×360
   - Epoch 101+: 640×480
   - 빠른 초기 학습 + 세밀한 후기 학습

3. Mixup / CutMix (Optional)
   - 데이터 다양성 증가
   - α=0.2 (약하게)
   - Segmentation에는 주의 필요

4. Self-Training (Pseudo Labeling)
   - 라벨 없는 이미지 167개 활용
   - High confidence prediction → pseudo label
   - 추가 학습 데이터 확보
```

---

## 4. 추론 최적화

### 4.1 Test-Time Augmentation (TTA)

```python
# 앙상블 효과로 성능 향상
TTA 설정:
1. 원본
2. Brightness +10%
3. Brightness -10%
4. Contrast +10%
5. Contrast -10%

최종 예측 = Average(5개 예측) > 0.5
→ IoU +2~3% 향상 예상
→ 추론 시간 5배 증가 (학습 완료 후 사용)
```

### 4.2 앙상블

```python
# 여러 모델 학습 후 조합
Model 1: DeepLabV3+ (ResNet101)
Model 2: DeepLabV3+ (ResNet50) - 가벼운 버전
Model 3: DeepLabV3+ (ResNet101, 다른 seed)

최종 예측 = Weighted Average
  - Validation IoU 기반 가중치
  - Soft voting (probability averaging)
  
→ IoU +3~5% 향상 예상
```

### 4.3 후처리

```python
1. Conditional Random Field (CRF)
   - 경계 정제
   - 약 1-2% IoU 향상

2. Morphological Operations
   - Opening (노이즈 제거)
   - Closing (구멍 메우기)

3. Connected Component Analysis
   - 크기 기반 필터링
   - 작은 false positive 제거

4. Kalman Filter (시간적 평활화)
   - 비디오 입력 시 안정성 향상
```

---

## 5. 프레임워크 선택

### 5.1 MMSegmentation (강력 추천) ⭐⭐⭐

```yaml
장점:
✅ OpenMMLab의 공식 Segmentation 프레임워크
✅ 300+ 모델 사전 구현
✅ Config 기반 실험 관리
✅ 학습/평가 파이프라인 완성
✅ COCO 포맷 바로 지원
✅ 활발한 커뮤니티

사용 이유:
1. 3시간 안에 첫 결과 가능
2. DeepLabV3+ 바로 사용 가능
3. 하이퍼파라미터 튜닝 편함
4. Tensorboard 통합
5. 최신 기법 바로 적용 가능

예상 일정:
- Day 1: 설치 + 데이터 준비 + 첫 학습
- Day 2-3: 하이퍼파라미터 튜닝
- Day 4: 최종 학습 + 평가
```

**설치**:
```bash
pip install -U openmim
mim install mmengine
mim install "mmcv>=2.0.0"
mim install "mmsegmentation>=1.0.0"
```

### 5.2 대안: Detectron2

```yaml
장점:
✅ Facebook AI Research
✅ 검증된 안정성
✅ DeepLabV3+ 지원

단점:
❌ Segmentation보다 Detection 특화
❌ MMSeg보다 모델 적음
```

### 5.3 대안: PyTorch 직접 구현

```yaml
장점:
✅ 완전한 제어
✅ 학습 목적으로 좋음

단점:
❌ 5-7일 소요
❌ 버그 리스크
❌ "일단 돌아가게" 목표와 맞지 않음

추천: Phase 2 (성능 확인 후)
```

---

## 6. 구현 계획 (Fast Track)

### Phase 0: 환경 설정 (2시간)
```bash
1. MMSegmentation 설치
2. CUDA, PyTorch 확인
3. 데이터셋 확인
```

### Phase 1: 데이터 준비 (4시간)
```bash
1. COCO 포맷 변환 (이미 완료)
2. MMSeg 데이터셋 클래스 작성
3. 데이터 증강 파이프라인 구축
4. Train/Val/Test 분할 (139/30/30)
5. 증강 적용 (199 → 1,000)
```

### Phase 2: Baseline 학습 (6시간)
```bash
1. Config 파일 작성 (DeepLabV3+)
2. 학습 시작 (50 epoch 빠른 확인)
3. Tensorboard 모니터링
4. Validation IoU 확인

목표: IoU > 0.70 확인
```

### Phase 3: 본 학습 (1-2일)
```bash
1. 하이퍼파라미터 튜닝
2. 200 epoch 전체 학습
3. EMA, TTA 적용
4. 앙상블 (여러 모델 학습)

목표: IoU > 0.85
```

### Phase 4: 추론 및 평가 (4시간)
```bash
1. 테스트셋 평가
2. 시각화 (예측 vs GT)
3. 실패 케이스 분석
4. 추론 속도 측정
```

### Phase 5: 제어 통합 (1-2일)
```bash
1. 후처리 파이프라인
2. PID 제어기 구현
3. 시뮬레이션 테스트
4. 실차 준비
```

**총 예상 기간**: 3-5일 (first working version)

---

## 7. 성능 목표 (현실적 재조정)

### 7.1 정량적 지표

⚠️ **중요**: 아래 수치는 **실험 전 추정치**입니다. 실제 결과로 업데이트 필요.

| 지표 | 낙관적 목표 | 현실적 목표 | 최소 기준 | 근거 |
|------|-------------|-------------|-----------|------|
| Test IoU | 0.80-0.85 | **0.75-0.80** | 0.70 | 199개 데이터 제약 |
| Test Pixel Acc | ≥ 0.95 | ≥ 0.93 | 0.90 | 배경이 많음 |
| Precision | 0.80-0.85 | **0.75-0.80** | 0.70 | FP 최소화 |
| Recall | 0.80-0.85 | **0.75-0.80** | 0.70 | FN 최소화 |
| F1-Score | 0.80-0.85 | **0.75-0.80** | 0.70 | 균형 |

**실제 달성 가능 범위**:
- Best case (모든 조건 이상적): IoU 0.80-0.85
- Likely case (일반적): IoU 0.75-0.80 ⭐ **목표**
- Worst case (데이터 문제): IoU 0.65-0.75

**불확실성 요인**:
1. 프레임 간 유사성 (데이터 누수 위험)
2. 라벨 품질 (어노테이션 정확도)
3. 트랙 다양성 부족 (단일 환경)
4. 증강 효과 (과적합 가능성)

### 7.2 정성적 지표

- ✅ 직선/곡선 차선 완벽 검출
- ✅ 교차로/복잡한 패턴 대응
- ✅ 그림자/조명 변화 강건
- ✅ 경계 선명 (blurry하지 않음)

### 7.3 실차 목표

- **완주율**: ≥ 90% (10회 시도)
- **차선 이탈률**: ≤ 10%
- **평균 속도**: 최대 가능 속도

---

## 8. 리스크 관리

### 8.1 데이터 부족 (Medium)

**현황**: 199개 (충분하지 않지만 관리 가능)

**완화 전략**:
1. ✅ Aggressive 데이터 증강 (5배)
2. ✅ Transfer Learning (ImageNet)
3. ✅ Self-Training (pseudo labeling)
4. ✅ 단일 환경 (일반화 불필요)
5. ✅ Dropout, Label Smoothing

**잔존 리스크**: 
- 매우 극단적인 시나리오 (조명 완전 꺼짐 등)에서는 실패 가능
- → 실차 테스트로 확인

### 8.2 오버피팅 (Medium)

**징후**:
- Train IoU > 0.95
- Val IoU < 0.75
- Train-Val gap > 0.15

**대응**:
1. ✅ Dropout 증가 (0.3 → 0.5)
2. ✅ Weight Decay 증가
3. ✅ Early Stopping
4. ✅ 더 많은 증강
5. ✅ Label Smoothing

### 8.3 학습 불안정 (Low)

**징후**:
- Loss 발산
- NaN gradient
- Validation 요동

**대응**:
1. ✅ Learning Rate 감소
2. ✅ Gradient Clipping
3. ✅ Mixed Precision 확인
4. ✅ Batch Size 조정

---

## 9. 배포 전략 (Phase 2)

### 9.1 모델 경량화 (나중에)

```yaml
1. Knowledge Distillation
   Teacher: DeepLabV3+ (ResNet101)
   Student: DeepLabV3+ (MobileNetV2)
   → 10배 빠른 추론

2. Pruning
   - 중요도 낮은 채널 제거
   - 30-50% 파라미터 감소

3. Quantization
   - FP32 → INT8
   - 4배 빠른 추론

4. ONNX / TensorRT
   - 최적화된 추론 엔진
   - 2-3배 빠른 추론
```

### 9.2 임베디드 배포

```yaml
옵션 1: Jetson Nano
- TensorRT 사용
- INT8 quantization
- 목표: 15 FPS

옵션 2: Raspberry Pi 4
- TFLite 변환
- 목표: 10 FPS
- Coral TPU 가속 고려

옵션 3: PC 연결
- 제약 없음
- WiFi로 추론 결과 전송
```

---

## 10. 성공 기준

### 10.1 Phase 1 성공 (Baseline)
- ✅ Test IoU ≥ 0.75
- ✅ 학습 안정적 (수렴)
- ✅ 시각적으로 합리적

### 10.2 Phase 2 성공 (Optimized)
- ✅ Test IoU ≥ 0.85
- ✅ 실차 완주율 ≥ 90%
- ✅ 실시간 추론 가능 (경량화 후)

### 10.3 최종 성공
- ✅ 안정적 트랙 완주
- ✅ 다양한 조명 조건 대응
- ✅ 배포 가능한 형태

---

## 11. 다음 단계

### 즉시 시작 가능한 작업

```bash
# 1. MMSegmentation 설치
pip install -U openmim
mim install mmengine "mmcv>=2.0.0" "mmsegmentation>=1.0.0"

# 2. 데이터 변환 스크립트 작성
# 3. Config 파일 작성 (DeepLabV3+)
# 4. 학습 시작
```

### 예상 타임라인 (현실적 재조정)

⚠️ **실제 개발 기간은 예상보다 길어질 수 있음**

| Week | Day | 작업 | 예상 산출물 | 비고 |
|------|-----|------|-------------|------|
| 1 | 1-2 | 환경 설정 + 데이터 준비 | 학습 가능 데이터셋 | 디버깅 시간 포함 |
| 1 | 3-4 | Baseline 학습 (50 epoch) | IoU 0.65-0.75 모델 | 첫 결과 확인 |
| 1 | 5 | 분석 및 문제 파악 | 실패 케이스 분석 | - |
| 2 | 1-3 | 본 학습 (200 epoch) | IoU 0.70-0.80 모델 | 하이퍼파라미터 튜닝 |
| 2 | 4 | 평가 및 최적화 | TTA, 후처리 적용 | +2-3% 개선 |
| 2 | 5 | 제어 통합 시작 | 추론 파이프라인 | - |

**총 예상 기간**: 2주 (10-12일)
- ❌ 3-5일은 과도하게 낙관적
- ✅ 2주가 현실적 (디버깅, 재학습 포함)

---

**작성일**: 2026-01-29  
**버전**: 2.0.1 (고성능 - 현실적 재조정)  
**작성자**: AI Architecture Design  
**수정일**: 2026-01-29 (팩트체크 반영)

---

## ⚠️ 중요 고지사항

**이 문서의 수치와 목표는 실험 전 추정치입니다.**

### 확정된 사실 ✅
- 데이터: 199개 (Train 139, Val 30, Test 30)
- 증강: Train만 5배 → 695개
- 모델: DeepLabV3+ (ResNet101)
- 환경: RTX 5090

### 불확실한 추정치 ⚠️
- IoU 목표: 0.75-0.80 (현실적), 0.80-0.85 (낙관적)
- 개발 기간: 2주 (실제는 더 걸릴 수 있음)
- 배치 사이즈: 실측 필요 (16부터 시작 권장)

### 검증 필요 사항 🔍
- 프레임 간 유사성 (데이터 누수 가능성)
- 라벨 품질 (어노테이션 정확도)
- 실제 달성 가능 성능

**권장**: 작은 실험부터 시작하여 실측 결과로 문서 업데이트
