# RCì¹´ ììœ¨ì£¼í–‰ êµ¬í˜„ ëª…ì„¸ì„œ v2.0 (ê³ ì„±ëŠ¥ - MMSegmentation)

## ğŸ¯ êµ¬í˜„ ì „ëµ

**í”„ë ˆì„ì›Œí¬**: MMSegmentation (ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…)  
**ëª¨ë¸**: DeepLabV3+ (ResNet101)  
**í˜„ì‹¤ì  ëª©í‘œ**: 2ì£¼ ì•ˆì— IoU 0.75-0.80 ë‹¬ì„± (ì‹¤í—˜ ê¸°ë°˜ ì—…ë°ì´íŠ¸)

---

## 1. í”„ë¡œì íŠ¸ êµ¬ì¡° (MMSeg ê¸°ë°˜)

```
ads-skynet/hyunahn/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ 01_ì•„í‚¤í…ì²˜_ì„¤ê³„ì„œ_v2_ê³ ì„±ëŠ¥.md
â”‚   â”œâ”€â”€ 02_êµ¬í˜„_ëª…ì„¸ì„œ_v2_ê³ ì„±ëŠ¥.md
â”‚   â””â”€â”€ 03_ê²€ì¦ì„œ_v2_ê³ ì„±ëŠ¥.md
â”œâ”€â”€ training_data/
â”‚   â”œâ”€â”€ images/          # 199ê°œ ì›ë³¸
â”‚   â”œâ”€â”€ annotations/     # 199ê°œ JSON
â”‚   â””â”€â”€ convert_coco.py  # COCO ë³€í™˜ (ì™„ë£Œ)
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ images/          # ì¦ê°• í›„ ì´ë¯¸ì§€
â”‚   â”œâ”€â”€ masks/           # ì¦ê°• í›„ ë§ˆìŠ¤í¬
â”‚   â”œâ”€â”€ train.txt        # í•™ìŠµ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
â”‚   â”œâ”€â”€ val.txt          # ê²€ì¦ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
â”‚   â””â”€â”€ test.txt         # í…ŒìŠ¤íŠ¸ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ deeplabv3plus_r101_lane.py  # ë©”ì¸ Config
â”‚   â”œâ”€â”€ _base_/
â”‚   â”‚   â”œâ”€â”€ datasets/lane_dataset.py
â”‚   â”‚   â”œâ”€â”€ models/deeplabv3plus_r101.py
â”‚   â”‚   â””â”€â”€ schedules/schedule_200e.py
â”‚   â””â”€â”€ tta_config.py    # Test-Time Augmentation
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ prepare_dataset.py       # ë°ì´í„° ì¦ê°• ë° ë¶„í• 
â”‚   â”œâ”€â”€ coco_to_mmseg.py        # COCO â†’ MMSeg í¬ë§·
â”‚   â”œâ”€â”€ train.py                 # í•™ìŠµ ì‹¤í–‰
â”‚   â”œâ”€â”€ test.py                  # í‰ê°€ ì‹¤í–‰
â”‚   â””â”€â”€ visualize.py             # ê²°ê³¼ ì‹œê°í™”
â”œâ”€â”€ work_dirs/
â”‚   â””â”€â”€ deeplabv3plus_r101_lane/ # í•™ìŠµ ê²°ê³¼
â”‚       â”œâ”€â”€ checkpoints/
â”‚       â”œâ”€â”€ logs/
â”‚       â””â”€â”€ vis/
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ predictor.py     # ì¶”ë¡  ì—”ì§„
â”‚   â”œâ”€â”€ postprocess.py   # í›„ì²˜ë¦¬
â”‚   â””â”€â”€ demo.py          # ë°ëª¨ ì‹¤í–‰
â”œâ”€â”€ control/
â”‚   â”œâ”€â”€ path_planner.py
â”‚   â”œâ”€â”€ pid_controller.py
â”‚   â””â”€â”€ vehicle_interface.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## 2. í™˜ê²½ ì„¤ì •

### 2.1 ì˜ì¡´ì„± ì„¤ì¹˜

```bash
# requirements.txt
torch==2.0.1
torchvision==0.15.2
openmim
mmengine>=0.8.0
mmcv>=2.0.0
mmsegmentation>=1.0.0
opencv-python>=4.7.0
albumentations>=1.3.0
numpy>=1.24.0
pillow>=9.5.0
matplotlib>=3.7.0
seaborn>=0.12.0
tqdm>=4.65.0
tensorboard>=2.13.0
pycocotools>=2.0.6

# ì„¤ì¹˜ ëª…ë ¹
pip install -r requirements.txt

# ë˜ëŠ” mim ì‚¬ìš©
pip install -U openmim
mim install mmengine
mim install "mmcv>=2.0.0"
mim install "mmsegmentation>=1.0.0"
```

### 2.2 í™˜ê²½ í™•ì¸

```python
# check_env.py
import torch
import mmcv
import mmseg

print(f"PyTorch: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}")
print(f"MMCV: {mmcv.__version__}")
print(f"MMSeg: {mmseg.__version__}")

# ê¸°ëŒ€ ì¶œë ¥:
# PyTorch: 2.0.1
# CUDA available: True
# CUDA version: 11.8 or 12.1
# GPU: NVIDIA GeForce RTX 5090
# MMCV: 2.x.x
# MMSeg: 1.x.x
```

---

## 3. ë°ì´í„° ì¤€ë¹„

### 3.1 COCO â†’ MMSeg í¬ë§· ë³€í™˜

```python
# tools/coco_to_mmseg.py
import json
import numpy as np
import cv2
from pathlib import Path
from tqdm import tqdm
import shutil

def coco_to_mmseg(coco_json_path, image_dir, output_dir, split='train'):
    """
    COCO í¬ë§·ì„ MMSegmentation í¬ë§·ìœ¼ë¡œ ë³€í™˜
    
    MMSeg í¬ë§·:
    - images/: ì´ë¯¸ì§€ íŒŒì¼
    - masks/: PNG ë§ˆìŠ¤í¬ (0=background, 1=lane)
    - {split}.txt: íŒŒì¼ ë¦¬ìŠ¤íŠ¸
    """
    output_dir = Path(output_dir)
    images_out = output_dir / 'images'
    masks_out = output_dir / 'masks'
    images_out.mkdir(parents=True, exist_ok=True)
    masks_out.mkdir(parents=True, exist_ok=True)
    
    # COCO ë¡œë“œ
    with open(coco_json_path) as f:
        coco = json.load(f)
    
    file_list = []
    
    for img_info in tqdm(coco['images'], desc=f'Converting {split}'):
        img_id = img_info['id']
        img_filename = img_info['file_name']
        img_path = Path(image_dir) / img_filename
        
        # ì´ë¯¸ì§€ ë³µì‚¬
        out_img_path = images_out / img_filename
        shutil.copy(img_path, out_img_path)
        
        # ë§ˆìŠ¤í¬ ìƒì„±
        mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)
        
        # í•´ë‹¹ ì´ë¯¸ì§€ì˜ ì–´ë…¸í…Œì´ì…˜ ì°¾ê¸°
        for ann in coco['annotations']:
            if ann['image_id'] == img_id:
                # Polygon segmentation â†’ mask
                for seg in ann['segmentation']:
                    poly = np.array(seg).reshape(-1, 2).astype(np.int32)
                    cv2.fillPoly(mask, [poly], 1)  # lane class = 1
        
        # ë§ˆìŠ¤í¬ ì €ì¥ (PNG, 0-255)
        mask_filename = img_filename.replace('.jpg', '.png')
        mask_path = masks_out / mask_filename
        cv2.imwrite(str(mask_path), mask)
        
        # íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€ (ìƒëŒ€ ê²½ë¡œ)
        file_list.append(f"images/{img_filename} masks/{mask_filename}\n")
    
    # íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì €ì¥
    with open(output_dir / f'{split}.txt', 'w') as f:
        f.writelines(file_list)
    
    print(f"âœ… {split}: {len(file_list)} samples converted")

if __name__ == '__main__':
    # ì‹¤í–‰ ì˜ˆì‹œ
    coco_to_mmseg(
        coco_json_path='training_data/_annotations.coco.json',
        image_dir='training_data/images',
        output_dir='dataset',
        split='all'
    )
```

### 3.2 ë°ì´í„° ì¦ê°• ë° ë¶„í• 

```python
# tools/prepare_dataset.py
import albumentations as A
import cv2
import numpy as np
from pathlib import Path
import shutil
from tqdm import tqdm
import random

def create_augmentation_pipeline():
    """ê°•ë ¥í•œ ë°ì´í„° ì¦ê°• íŒŒì´í”„ë¼ì¸"""
    return A.Compose([
        # ê¸°í•˜í•™ì  ë³€í™˜
        A.ShiftScaleRotate(
            shift_limit=0.15,
            scale_limit=0.15,
            rotate_limit=20,
            border_mode=cv2.BORDER_CONSTANT,
            value=0,
            mask_value=0,
            p=0.8
        ),
        
        # ìƒ‰ìƒ ë³€í™˜
        A.ColorJitter(
            brightness=0.3,
            contrast=0.3,
            saturation=0.2,
            hue=0.05,
            p=0.8
        ),
        
        # ë¸”ëŸ¬ ë° ë…¸ì´ì¦ˆ
        A.OneOf([
            A.GaussianBlur(blur_limit=(3, 7), p=1.0),
            A.MotionBlur(blur_limit=7, p=1.0),
            A.MedianBlur(blur_limit=7, p=1.0),
        ], p=0.3),
        
        A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),
        
        # ì™œê³¡ (ì•½í•˜ê²Œ)
        A.OneOf([
            A.ElasticTransform(alpha=50, sigma=5, p=1.0),
            A.GridDistortion(num_steps=5, distort_limit=0.1, p=1.0),
        ], p=0.2),
        
        # ë°ê¸° ë³€í™”
        A.RandomBrightnessContrast(
            brightness_limit=0.3,
            contrast_limit=0.3,
            p=0.5
        ),
    ])

def augment_dataset(input_dir, output_dir, augment_factor=5, train_ratio=0.7, val_ratio=0.15):
    """
    ë°ì´í„°ì…‹ ì¦ê°• ë° ë¶„í• 
    
    Parameters:
    -----------
    input_dir : Path to dataset/ (images/, masks/, all.txt)
    output_dir : Path to augmented dataset
    augment_factor : ì¦ê°• ë°°ìˆ˜ (ì›ë³¸ í¬í•¨)
    train_ratio, val_ratio : ë¶„í•  ë¹„ìœ¨
    """
    input_dir = Path(input_dir)
    output_dir = Path(output_dir)
    
    images_in = input_dir / 'images'
    masks_in = input_dir / 'masks'
    
    images_out = output_dir / 'images'
    masks_out = output_dir / 'masks'
    images_out.mkdir(parents=True, exist_ok=True)
    masks_out.mkdir(parents=True, exist_ok=True)
    
    # ì›ë³¸ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
    with open(input_dir / 'all.txt') as f:
        original_files = [line.strip().split() for line in f]
    
    random.shuffle(original_files)
    n_total = len(original_files)
    n_train = int(n_total * train_ratio)
    n_val = int(n_total * val_ratio)
    
    train_files = original_files[:n_train]
    val_files = original_files[n_train:n_train + n_val]
    test_files = original_files[n_train + n_val:]
    
    print(f"Split: Train={len(train_files)}, Val={len(val_files)}, Test={len(test_files)}")
    
    augmentor = create_augmentation_pipeline()
    
    def process_split(file_list, split_name, apply_augmentation=True):
        """ë¶„í• ë³„ ì²˜ë¦¬"""
        output_list = []
        
        for img_rel, mask_rel in tqdm(file_list, desc=f'{split_name}'):
            img_path = input_dir / img_rel
            mask_path = input_dir / mask_rel
            
            image = cv2.imread(str(img_path))
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)
            
            # ì›ë³¸ ì €ì¥
            base_name = Path(img_rel).stem
            
            for aug_idx in range(augment_factor if apply_augmentation else 1):
                if aug_idx == 0:
                    # ì›ë³¸
                    aug_image = image
                    aug_mask = mask
                    suffix = ''
                else:
                    # ì¦ê°•
                    augmented = augmentor(image=image, mask=mask)
                    aug_image = augmented['image']
                    aug_mask = augmented['mask']
                    suffix = f'_aug{aug_idx}'
                
                # ì €ì¥
                out_img_name = f"{base_name}{suffix}.jpg"
                out_mask_name = f"{base_name}{suffix}.png"
                
                cv2.imwrite(
                    str(images_out / out_img_name),
                    cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR)
                )
                cv2.imwrite(
                    str(masks_out / out_mask_name),
                    aug_mask
                )
                
                output_list.append(f"images/{out_img_name} masks/{out_mask_name}\n")
        
        # íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì €ì¥
        with open(output_dir / f'{split_name}.txt', 'w') as f:
            f.writelines(output_list)
        
        print(f"âœ… {split_name}: {len(output_list)} samples")
    
    # Train: ì¦ê°• O
    process_split(train_files, 'train', apply_augmentation=True)
    
    # Val, Test: ì¦ê°• X
    process_split(val_files, 'val', apply_augmentation=False)
    process_split(test_files, 'test', apply_augmentation=False)

if __name__ == '__main__':
    augment_dataset(
        input_dir='dataset',
        output_dir='dataset_augmented',
        augment_factor=5,  # 5ë°° ì¦ê°• (ì›ë³¸ + 4ë°°)
        train_ratio=0.7,
        val_ratio=0.15
    )
```

---

## 4. MMSegmentation Config

### 4.1 Dataset Config

```python
# configs/_base_/datasets/lane_dataset.py

dataset_type = 'CustomDataset'
data_root = 'dataset_augmented/'

# í´ë˜ìŠ¤ ì •ì˜
classes = ('background', 'lane')
palette = [[0, 0, 0], [255, 255, 255]]  # ë°°ê²½: ê²€ì •, ì°¨ì„ : í°ìƒ‰

# Train pipeline
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', reduce_zero_label=False),
    dict(type='Resize', scale=(640, 480), keep_ratio=True),
    dict(type='RandomFlip', prob=0.0),  # ìˆ˜í‰ ë°˜ì „ ê¸ˆì§€
    dict(type='PhotoMetricDistortion'),  # ì¶”ê°€ ìƒ‰ìƒ ë³€í™˜
    dict(type='PackSegInputs')
]

# Val/Test pipeline
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='Resize', scale=(640, 480), keep_ratio=True),
    dict(type='LoadAnnotations', reduce_zero_label=False),
    dict(type='PackSegInputs')
]

# Train dataloader
train_dataloader = dict(
    batch_size=16,  # âš ï¸ ì‹œì‘ì€ 16, GPU ë©”ëª¨ë¦¬ í™•ì¸ í›„ ì¦ê°€
    num_workers=8,
    persistent_workers=True,
    sampler=dict(type='DefaultSampler', shuffle=True),
    dataset=dict(
        type=dataset_type,
        data_root=data_root,
        data_prefix=dict(img_path='images', seg_map_path='masks'),
        ann_file='train.txt',
        pipeline=train_pipeline
    )
)

# Val dataloader
val_dataloader = dict(
    batch_size=1,
    num_workers=4,
    persistent_workers=True,
    sampler=dict(type='DefaultSampler', shuffle=False),
    dataset=dict(
        type=dataset_type,
        data_root=data_root,
        data_prefix=dict(img_path='images', seg_map_path='masks'),
        ann_file='val.txt',
        pipeline=test_pipeline
    )
)

# Test dataloader
test_dataloader = val_dataloader

# Evaluator
val_evaluator = dict(type='IoUMetric', iou_metrics=['mIoU', 'mDice'])
test_evaluator = val_evaluator
```

### 4.2 Model Config

```python
# configs/_base_/models/deeplabv3plus_r101.py

norm_cfg = dict(type='SyncBN', requires_grad=True)
data_preprocessor = dict(
    type='SegDataPreProcessor',
    mean=[123.675, 116.28, 103.53],
    std=[58.395, 57.12, 57.375],
    bgr_to_rgb=True,
    pad_val=0,
    seg_pad_val=255
)

model = dict(
    type='EncoderDecoder',
    data_preprocessor=data_preprocessor,
    pretrained='open-mmlab://resnet101_v1c',
    backbone=dict(
        type='ResNetV1c',
        depth=101,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        dilations=(1, 1, 2, 4),
        strides=(1, 2, 1, 1),
        norm_cfg=norm_cfg,
        norm_eval=False,
        style='pytorch',
        contract_dilation=True
    ),
    decode_head=dict(
        type='DepthwiseSeparableASPPHead',
        in_channels=2048,
        in_index=3,
        channels=512,
        dilations=(1, 12, 24, 36),
        c1_in_channels=256,
        c1_channels=48,
        dropout_ratio=0.1,
        num_classes=2,  # background + lane
        norm_cfg=norm_cfg,
        align_corners=False,
        loss_decode=[
            dict(type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            dict(type='DiceLoss', loss_weight=3.0)  # Dice ê°•ì¡°
        ]
    ),
    auxiliary_head=dict(
        type='FCNHead',
        in_channels=1024,
        in_index=2,
        channels=256,
        num_convs=1,
        concat_input=False,
        dropout_ratio=0.1,
        num_classes=2,
        norm_cfg=norm_cfg,
        align_corners=False,
        loss_decode=dict(type='CrossEntropyLoss', use_sigmoid=False, loss_weight=0.4)
    ),
    train_cfg=dict(),
    test_cfg=dict(mode='whole')
)
```

### 4.3 Training Schedule

```python
# configs/_base_/schedules/schedule_200e.py

# Optimizer
optim_wrapper = dict(
    type='OptimWrapper',
    optimizer=dict(
        type='AdamW',
        lr=0.0001,
        betas=(0.9, 0.999),
        weight_decay=0.0001
    ),
    paramwise_cfg=dict(
        custom_keys={
            'backbone': dict(lr_mult=0.1),  # Backbone 10ë°° ëŠë¦¬ê²Œ
        }
    )
)

# Learning rate scheduler
param_scheduler = [
    dict(
        type='LinearLR',
        start_factor=1e-6,
        by_epoch=False,
        begin=0,
        end=500
    ),  # Warmup
    dict(
        type='CosineAnnealingLR',
        T_max=200,
        eta_min=1e-6,
        by_epoch=True,
        begin=0,
        end=200
    )
]

# Training config
train_cfg = dict(
    type='EpochBasedTrainLoop',
    max_epochs=200,
    val_interval=5  # 5 epochë§ˆë‹¤ validation
)

val_cfg = dict(type='ValLoop')
test_cfg = dict(type='TestLoop')

# Hooks
default_hooks = dict(
    timer=dict(type='IterTimerHook'),
    logger=dict(type='LoggerHook', interval=50, log_metric_by_epoch=True),
    param_scheduler=dict(type='ParamSchedulerHook'),
    checkpoint=dict(
        type='CheckpointHook',
        by_epoch=True,
        interval=10,
        save_best='mIoU',
        rule='greater'
    ),
    sampler_seed=dict(type='DistributedSamplerSeedHook'),
    visualization=dict(type='SegVisualizationHook', draw=True, interval=20)
)
```

### 4.4 Main Config

```python
# configs/deeplabv3plus_r101_lane.py

_base_ = [
    './_base_/models/deeplabv3plus_r101.py',
    './_base_/datasets/lane_dataset.py',
    './_base_/schedules/schedule_200e.py',
    './_base_/default_runtime.py'
]

# Mixed Precision Training (FP16)
optim_wrapper = dict(
    type='AmpOptimWrapper',
    loss_scale='dynamic'
)

# Work directory
work_dir = './work_dirs/deeplabv3plus_r101_lane'

# Random seed
randomness = dict(seed=42)
```

---

## 5. í•™ìŠµ ì‹¤í–‰

### 5.1 í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

```bash
# tools/train.sh
#!/bin/bash

# Single GPU
python tools/train.py \
    configs/deeplabv3plus_r101_lane.py \
    --work-dir work_dirs/deeplabv3plus_r101_lane \
    --amp  # Mixed Precision

# Multi-GPU (if available)
# bash tools/dist_train.sh \
#     configs/deeplabv3plus_r101_lane.py \
#     4  # number of GPUs
```

### 5.2 í•™ìŠµ ëª¨ë‹ˆí„°ë§

```python
# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
tensorboard --logdir=work_dirs/deeplabv3plus_r101_lane

# ë¸Œë¼ìš°ì €ì—ì„œ: http://localhost:6006
# í™•ì¸ í•­ëª©:
# - loss/train, loss/val
# - mIoU/val
# - learning_rate
# - ì˜ˆì¸¡ ì‹œê°í™”
```

---

## 6. í‰ê°€ ë° ì¶”ë¡ 

### 6.1 í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

```python
# tools/test.py
from mmseg.apis import init_model, inference_model
from mmseg.utils import register_all_modules
import mmcv
import numpy as np
from pathlib import Path

def evaluate_model(config_path, checkpoint_path, test_data_root):
    """ëª¨ë¸ í‰ê°€"""
    register_all_modules()
    
    # ëª¨ë¸ ë¡œë“œ
    model = init_model(config_path, checkpoint_path, device='cuda:0')
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    with open(Path(test_data_root) / 'test.txt') as f:
        test_files = [line.strip().split()[0] for line in f]
    
    ious = []
    
    for img_rel in test_files:
        img_path = Path(test_data_root) / img_rel
        mask_path = img_path.parent.parent / 'masks' / (img_path.stem + '.png')
        
        # ì¶”ë¡ 
        result = inference_model(model, str(img_path))
        pred_mask = result.pred_sem_seg.data[0].cpu().numpy()
        
        # Ground truth
        gt_mask = mmcv.imread(str(mask_path), flag='grayscale')
        
        # IoU ê³„ì‚°
        intersection = np.logical_and(pred_mask == 1, gt_mask == 1).sum()
        union = np.logical_or(pred_mask == 1, gt_mask == 1).sum()
        iou = intersection / (union + 1e-6)
        ious.append(iou)
    
    mean_iou = np.mean(ious)
    print(f"Test mIoU: {mean_iou:.4f}")
    return mean_iou

if __name__ == '__main__':
    evaluate_model(
        config_path='configs/deeplabv3plus_r101_lane.py',
        checkpoint_path='work_dirs/deeplabv3plus_r101_lane/best_mIoU_epoch_XXX.pth',
        test_data_root='dataset_augmented'
    )
```

### 6.2 ì¶”ë¡  ì—”ì§„

```python
# inference/predictor.py
import torch
import torch.nn as nn
from mmseg.apis import init_model, inference_model
import cv2
import numpy as np

class LanePredictor:
    """ê³ ì„±ëŠ¥ ì°¨ì„  ê²€ì¶œ ì¶”ë¡ ê¸°"""
    
    def __init__(self, config_path, checkpoint_path, device='cuda:0', use_tta=False):
        self.device = device
        self.use_tta = use_tta
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = init_model(config_path, checkpoint_path, device=device)
        self.model.eval()
    
    @torch.no_grad()
    def predict(self, image):
        """
        Parameters:
        -----------
        image : np.ndarray, (H, W, 3), RGB
        
        Returns:
        --------
        mask : np.ndarray, (H, W), binary {0, 1}
        confidence : float
        """
        if not self.use_tta:
            # ë‹¨ì¼ ì˜ˆì¸¡
            result = inference_model(self.model, image)
            mask = result.pred_sem_seg.data[0].cpu().numpy().astype(np.uint8)
            confidence = result.pred_sem_seg.data[0].float().mean().item()
        else:
            # Test-Time Augmentation
            masks = []
            
            # ì›ë³¸
            result = inference_model(self.model, image)
            masks.append(result.pred_sem_seg.data[0].cpu().numpy())
            
            # ë°ê¸° ë³€í˜•
            for brightness_delta in [-20, 20]:
                aug_image = np.clip(image.astype(np.float32) + brightness_delta, 0, 255).astype(np.uint8)
                result = inference_model(self.model, aug_image)
                masks.append(result.pred_sem_seg.data[0].cpu().numpy())
            
            # ëŒ€ë¹„ ë³€í˜•
            for contrast_factor in [0.9, 1.1]:
                aug_image = np.clip(image.astype(np.float32) * contrast_factor, 0, 255).astype(np.uint8)
                result = inference_model(self.model, aug_image)
                masks.append(result.pred_sem_seg.data[0].cpu().numpy())
            
            # ì•™ìƒë¸” (í‰ê· )
            mask = (np.mean(masks, axis=0) > 0.5).astype(np.uint8)
            confidence = np.mean(masks)
        
        return mask, confidence
    
    def predict_video(self, video_path, output_path):
        """ë¹„ë””ì˜¤ ì²˜ë¦¬"""
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            # BGR â†’ RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # ì¶”ë¡ 
            mask, conf = self.predict(frame_rgb)
            
            # ì‹œê°í™”
            overlay = frame.copy()
            overlay[mask == 1] = [0, 255, 0]  # ì´ˆë¡ìƒ‰
            result = cv2.addWeighted(frame, 0.7, overlay, 0.3, 0)
            
            out.write(result)
        
        cap.release()
        out.release()
```

---

## 7. ì œì–´ ì‹œìŠ¤í…œ

### 7.1 í›„ì²˜ë¦¬

```python
# inference/postprocess.py
import cv2
import numpy as np
from scipy import ndimage

def postprocess_mask(mask, min_area=100):
    """
    ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬
    
    1. Morphological operations
    2. Connected component filtering
    3. Polynomial fitting
    """
    # Opening (ë…¸ì´ì¦ˆ ì œê±°)
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
    
    # Closing (êµ¬ë© ë©”ìš°ê¸°)
    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
    
    # Connected components
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask, connectivity=8)
    
    # ì‘ì€ ì˜ì—­ ì œê±°
    filtered_mask = np.zeros_like(mask)
    for i in range(1, num_labels):
        area = stats[i, cv2.CC_STAT_AREA]
        if area >= min_area:
            filtered_mask[labels == i] = 1
    
    return filtered_mask

def extract_lane_polylines(mask, num_lanes=3):
    """ì°¨ì„  polyline ì¶”ì¶œ"""
    # Connected components
    num_labels, labels = cv2.connectedComponents(mask)
    
    polylines = []
    
    for label_id in range(1, min(num_labels, num_lanes + 1)):
        component = (labels == label_id).astype(np.uint8) * 255
        
        # Skeleton
        skeleton = cv2.ximgproc.thinning(component)
        
        # ì¢Œí‘œ ì¶”ì¶œ
        points = np.column_stack(np.where(skeleton > 0))[:, ::-1]  # (x, y)
        
        if len(points) > 5:
            # Y ê¸°ì¤€ ì •ë ¬
            points = points[points[:, 1].argsort()]
            
            # Polynomial fitting
            x, y = points[:, 0], points[:, 1]
            coeffs = np.polyfit(y, x, deg=2)
            poly = np.poly1d(coeffs)
            
            y_samples = np.linspace(y.min(), y.max(), 20)
            x_samples = poly(y_samples)
            
            polyline = np.column_stack([x_samples, y_samples])
            polylines.append(polyline)
    
    return polylines
```

### 7.2 PID ì œì–´ (ê¸°ì¡´ê³¼ ë™ì¼)

```python
# control/pid_controller.py
# (êµ¬í˜„ ëª…ì„¸ì„œ v1ê³¼ ë™ì¼)
```

---

## 8. ì‹¤í–‰ ê°€ì´ë“œ

### 8.1 ì „ì²´ ì›Œí¬í”Œë¡œìš°

```bash
# Step 1: í™˜ê²½ í™•ì¸
python check_env.py

# Step 2: COCO â†’ MMSeg ë³€í™˜
cd training_data
python convert_coco.py  # COCO JSON ìƒì„±
cd ..
python tools/coco_to_mmseg.py

# Step 3: ë°ì´í„° ì¦ê°• ë° ë¶„í• 
python tools/prepare_dataset.py

# Step 4: í•™ìŠµ
bash tools/train.sh

# Step 5: Tensorboard ëª¨ë‹ˆí„°ë§
tensorboard --logdir=work_dirs/deeplabv3plus_r101_lane

# Step 6: í‰ê°€
python tools/test.py

# Step 7: ì¶”ë¡  í…ŒìŠ¤íŠ¸
python inference/demo.py --image test.jpg
```

### 8.2 ì˜ˆìƒ í•™ìŠµ ì‹œê°„ (RTX 5090)

| Epoch | ë°°ì¹˜ 64 | ë°°ì¹˜ 32 |
|-------|---------|---------|
| 50 | ~2ì‹œê°„ | ~3ì‹œê°„ |
| 100 | ~4ì‹œê°„ | ~6ì‹œê°„ |
| 200 | ~8ì‹œê°„ | ~12ì‹œê°„ |

---

## 9. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 9.1 Out of Memory
```bash
# Configì—ì„œ batch_size ì¤„ì´ê¸°
batch_size=32  # ë˜ëŠ” 16

# Gradient Accumulation ì‚¬ìš©
# configsì— ì¶”ê°€:
optim_wrapper = dict(
    accumulative_counts=2  # ì‹¤ì§ˆì  ë°°ì¹˜ = 32*2 = 64
)
```

### 9.2 í•™ìŠµ ë¶ˆì•ˆì •
```python
# Learning rate ë‚®ì¶”ê¸°
optimizer=dict(lr=5e-5)  # 1e-4 â†’ 5e-5

# Gradient clipping
optim_wrapper = dict(
    clip_grad=dict(max_norm=1.0)
)
```

---

**ì‘ì„±ì¼**: 2026-01-29  
**ë²„ì „**: 2.0 (MMSegmentation)  
**ë‹¤ìŒ**: ê²€ì¦ì„œ v2
