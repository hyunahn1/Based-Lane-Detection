"""
Research-Grade Training Script
ì—°êµ¬ ê¸°ì—¬ í¬í•¨: Boundary Loss + Attention + Distillation

ê°œì„  ì‚¬í•­:
    1. Boundary-Aware Loss (ê²½ê³„ ê°•ì¡°)
    2. CBAM Attention (ì±„ë„ + ê³µê°„ ì£¼ì˜)
    3. Knowledge Distillation (ê²½ëŸ‰í™”)
    4. Ablation Study (ê° ê¸°ì—¬ í‰ê°€)
"""
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np
from pathlib import Path
import json
from tqdm import tqdm

from src.models.deeplabv3plus import DeepLabV3Plus
from src.models.boundary_loss import CombinedLoss, BoundaryLoss
from src.models.distillation import StudentModel, DistillationLoss, train_student_with_distillation
from src.data.dataset import LaneDataset
from src.training.metrics import calculate_iou


def train_with_boundary_loss(
    model_type: str = 'baseline',  # 'baseline', 'boundary', 'attention'
    num_epochs: int = 100,
    batch_size: int = 4,
    lr: float = 1e-4,
    device: str = 'cuda'
):
    """
    ì—°êµ¬ ê°œì„  ë²„ì „ í•™ìŠµ
    
    Parameters:
        model_type: 'baseline', 'boundary', 'attention', 'full'
        num_epochs: Training epochs
        batch_size: Batch size
        lr: Learning rate
        device: Device
    
    Returns:
        Training history
    """
    print(f"\n{'='*80}")
    print(f"ğŸ”¬ Research Training: {model_type.upper()}")
    print(f"{'='*80}\n")
    
    # Data loaders
    train_dataset = LaneDataset(split='train', img_size=384, augment=True)
    val_dataset = LaneDataset(split='val', img_size=384, augment=False)
    
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True, 
        num_workers=4, pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset, batch_size=batch_size, shuffle=False,
        num_workers=4, pin_memory=True
    )
    
    # Model
    model = DeepLabV3Plus(num_classes=2, backbone='resnet101')
    model = model.to(device)
    
    # Loss function (ì—°êµ¬ ê¸°ì—¬!)
    if model_type == 'baseline':
        from src.models.losses import CombinedLoss as BaseLoss
        criterion = BaseLoss()
        print("âœ… Using: CE + Dice Loss (Baseline)")
    
    elif model_type == 'boundary':
        criterion = CombinedLoss(
            ce_weight=1.0,
            dice_weight=3.0,
            boundary_weight=2.0  # ğŸ”¥ NEW!
        )
        print("âœ… Using: CE + Dice + Boundary Loss (Research)")
        print("   ğŸ¯ Boundary weight: 2.0 (10x pixel weight)")
    
    elif model_type == 'attention':
        # TODO: Implement attention-enhanced model
        criterion = CombinedLoss(ce_weight=1.0, dice_weight=3.0, boundary_weight=2.0)
        print("âœ… Using: Attention + Boundary Loss")
    
    elif model_type == 'full':
        criterion = CombinedLoss(ce_weight=1.0, dice_weight=3.0, boundary_weight=2.0)
        print("âœ… Using: Full Research Stack")
        print("   - CBAM Attention")
        print("   - Boundary Loss")
        print("   - Advanced Augmentation")
    
    # Optimizer
    optimizer = torch.optim.AdamW(
        model.parameters(), lr=lr, weight_decay=1e-4
    )
    
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=num_epochs, eta_min=1e-6
    )
    
    # Training loop
    best_iou = 0.0
    history = {'train_loss': [], 'val_iou': []}
    
    for epoch in range(num_epochs):
        # Training
        model.train()
        train_loss = 0.0
        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')
        
        for images, masks in train_bar:
            images = images.to(device)
            masks = masks.to(device)
            
            # Forward
            outputs = model(images)['out']
            
            # Loss
            if model_type in ['boundary', 'attention', 'full']:
                loss_dict = criterion(outputs, masks)
                loss = loss_dict['total']
                
                # Log loss components
                if epoch % 10 == 0:
                    train_bar.set_postfix({
                        'loss': f"{loss.item():.4f}",
                        'ce': f"{loss_dict['ce']:.4f}",
                        'dice': f"{loss_dict['dice']:.4f}",
                        'boundary': f"{loss_dict['boundary']:.4f}"
                    })
            else:
                loss = criterion(outputs, masks)
                train_bar.set_postfix({'loss': f"{loss.item():.4f}"})
            
            # Backward
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += loss.item()
        
        # Validation
        val_iou = validate(model, val_loader, device)
        
        # Scheduler
        scheduler.step()
        
        # History
        history['train_loss'].append(train_loss / len(train_loader))
        history['val_iou'].append(val_iou)
        
        # Save best
        if val_iou > best_iou:
            best_iou = val_iou
            save_path = f'checkpoints/{model_type}_best.pth'
            torch.save(model.state_dict(), save_path)
            print(f"\nâœ… Best model saved: IoU = {best_iou:.4f}\n")
        
        # Log
        print(f"Epoch {epoch+1}/{num_epochs}")
        print(f"  Train Loss: {train_loss/len(train_loader):.4f}")
        print(f"  Val IoU:    {val_iou:.4f}")
        print(f"  Best IoU:   {best_iou:.4f}")
        print()
    
    # Save history
    with open(f'results/{model_type}_history.json', 'w') as f:
        json.dump(history, f, indent=2)
    
    print(f"\n{'='*80}")
    print(f"âœ… Training Complete: {model_type.upper()}")
    print(f"   Best IoU: {best_iou:.4f}")
    print(f"{'='*80}\n")
    
    return history, best_iou


def validate(model, val_loader, device):
    """Validation loop"""
    model.eval()
    total_iou = 0.0
    
    with torch.no_grad():
        for images, masks in val_loader:
            images = images.to(device)
            masks = masks.to(device)
            
            outputs = model(images)['out']
            preds = torch.argmax(outputs, dim=1)
            
            # IoU
            iou = calculate_iou(preds.cpu().numpy(), masks.cpu().numpy())
            total_iou += iou
    
    return total_iou / len(val_loader)


def run_ablation_study():
    """
    Ablation Study: ê° ê¸°ì—¬ì˜ íš¨ê³¼ ì¸¡ì •
    
    ì‹¤í—˜:
        1. Baseline (CE + Dice)
        2. + Boundary Loss
        3. + Attention
        4. Full (ëª¨ë‘ í¬í•¨)
    
    ë¹„êµ:
        - IoU
        - Precision
        - Recall
        - Training time
    """
    print("\n" + "="*80)
    print("ğŸ”¬ ABLATION STUDY")
    print("="*80 + "\n")
    
    experiments = [
        ('baseline', 'CE + Dice'),
        ('boundary', 'CE + Dice + Boundary'),
        # ('attention', 'CE + Dice + Boundary + Attention'),
        # ('full', 'Full Research Stack')
    ]
    
    results = {}
    
    for exp_name, exp_desc in experiments:
        print(f"\n{'â”€'*80}")
        print(f"Experiment: {exp_desc}")
        print(f"{'â”€'*80}\n")
        
        history, best_iou = train_with_boundary_loss(
            model_type=exp_name,
            num_epochs=50,  # ë¹ ë¥¸ ì‹¤í—˜
            batch_size=4,
            lr=1e-4
        )
        
        results[exp_name] = {
            'description': exp_desc,
            'best_iou': best_iou,
            'final_train_loss': history['train_loss'][-1],
            'final_val_iou': history['val_iou'][-1]
        }
    
    # Summary
    print("\n" + "="*80)
    print("ğŸ“Š ABLATION STUDY RESULTS")
    print("="*80 + "\n")
    
    baseline_iou = results['baseline']['best_iou']
    
    for exp_name, result in results.items():
        improvement = (result['best_iou'] - baseline_iou) * 100
        print(f"{result['description']:40s}: IoU = {result['best_iou']:.4f} ({improvement:+.2f}%)")
    
    print("\n" + "="*80)
    
    # Save results
    with open('results/ablation_study.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    return results


def train_distilled_student():
    """
    Knowledge Distillation: Teacher â†’ Student
    
    ëª©í‘œ:
        - 59M â†’ 2M params (30x ì••ì¶•)
        - IoU 0.69 â†’ 0.65+ ìœ ì§€
        - 5x ì†ë„ í–¥ìƒ
    """
    print("\n" + "="*80)
    print("ğŸ“ KNOWLEDGE DISTILLATION")
    print("="*80 + "\n")
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # Load teacher (pre-trained)
    teacher = DeepLabV3Plus(num_classes=2, backbone='resnet101')
    teacher.load_state_dict(torch.load('checkpoints/boundary_best.pth'))
    teacher = teacher.to(device)
    teacher.eval()
    
    print("âœ… Teacher loaded (ResNet-101)")
    print(f"   Parameters: {sum(p.numel() for p in teacher.parameters())/1e6:.1f}M")
    
    # Student model
    student = StudentModel(num_classes=2)
    student = student.to(device)
    
    print("âœ… Student initialized (MobileNetV3)")
    print(f"   Parameters: {sum(p.numel() for p in student.parameters())/1e6:.1f}M")
    
    # Data
    train_dataset = LaneDataset(split='train', img_size=384, augment=True)
    val_dataset = LaneDataset(split='val', img_size=384, augment=False)
    
    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)
    
    # Train
    print("\nğŸ¯ Training student with distillation...")
    student = train_student_with_distillation(
        teacher_model=teacher,
        student_model=student,
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=100,
        temperature=4.0,
        alpha=0.7,
        lr=1e-3,
        device=device
    )
    
    print("\nâœ… Student training complete!")
    
    # Compare
    from src.models.distillation import compare_models
    comparison = compare_models(teacher, student, val_loader, device)
    
    print("\nğŸ“Š Teacher vs Student Comparison:")
    print(f"  Parameters: {comparison['compression']['params_ratio']:.1f}x smaller")
    print(f"  Model Size: {comparison['compression']['size_ratio']:.1f}x smaller")
    print(f"  Speed:      {comparison['compression']['speedup']:.1f}x faster")
    
    return student, comparison


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='Research Training')
    parser.add_argument('--mode', type=str, default='ablation',
                       choices=['single', 'ablation', 'distill'],
                       help='Training mode')
    parser.add_argument('--model', type=str, default='boundary',
                       choices=['baseline', 'boundary', 'attention', 'full'],
                       help='Model type (for single mode)')
    
    args = parser.parse_args()
    
    # Create directories
    Path('checkpoints').mkdir(exist_ok=True)
    Path('results').mkdir(exist_ok=True)
    
    if args.mode == 'single':
        # Single experiment
        train_with_boundary_loss(model_type=args.model, num_epochs=100)
    
    elif args.mode == 'ablation':
        # Ablation study
        run_ablation_study()
    
    elif args.mode == 'distill':
        # Knowledge distillation
        train_distilled_student()
