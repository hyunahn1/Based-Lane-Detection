"""
Module 08 Basic Functionality Test
ë¹ ë¥¸ ê²€ì¦: Environment + Agent
"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

import numpy as np
import torch

print("="*80)
print("Module 08: Reinforcement Learning - Basic Test")
print("="*80)

# Test 1: Environment
print("\n[Test 1] Environment Initialization")
try:
    from src.environment import RCTrackEnv
    
    env = RCTrackEnv(track_type='easy')
    obs, info = env.reset()
    
    print(f"  âœ… Environment created")
    print(f"     Track: {env.track_type}")
    print(f"     Max steps: {env.max_steps}")
    
    # Check observation
    assert 'image' in obs
    assert obs['image'].shape == (3, 84, 84)
    print(f"  âœ… Observation space: OK")
    print(f"     Image: {obs['image'].shape}")
    print(f"     Velocity: {obs['velocity']}")
    print(f"     Lateral offset: {obs['lateral_offset']}")
    
    # Check action
    action = env.action_space.sample()
    assert action.shape == (2,)
    print(f"  âœ… Action space: OK")
    print(f"     Action shape: {action.shape}")
    print(f"     Action range: [{env.action_space.low}, {env.action_space.high}]")
    
    print("  âœ… PASS: Environment initialization")
    
except Exception as e:
    print(f"  âŒ FAIL: {e}")
    import traceback
    traceback.print_exc()

# Test 2: Environment Step
print("\n[Test 2] Environment Step")
try:
    env = RCTrackEnv()
    obs, _ = env.reset()
    
    action = np.array([0.5, 0.5])  # steering=0.5, throttle=0.5
    next_obs, reward, terminated, truncated, info = env.step(action)
    
    print(f"  âœ… Step executed")
    print(f"     Action: {action}")
    print(f"     Reward: {reward:.4f}")
    print(f"     Terminated: {terminated}")
    print(f"     Car position: x={info['car_x']:.2f}, y={info['car_y']:.2f}")
    
    # Run a few steps
    for _ in range(10):
        action = env.action_space.sample()
        obs, reward, terminated, truncated, _ = env.step(action)
        if terminated or truncated:
            break
    
    print(f"  âœ… PASS: Environment step works")
    
except Exception as e:
    print(f"  âŒ FAIL: {e}")
    import traceback
    traceback.print_exc()

# Test 3: PPO Agent
print("\n[Test 3] PPO Agent Initialization")
try:
    from src.agent import PPOAgent
    
    env = RCTrackEnv()
    agent = PPOAgent(
        obs_space=env.observation_space,
        action_space=env.action_space,
        device='cpu'
    )
    
    print(f"  âœ… Agent created")
    
    # Count parameters
    total_params = sum(p.numel() for p in agent.policy.parameters())
    trainable_params = sum(p.numel() for p in agent.policy.parameters() if p.requires_grad)
    
    print(f"     Total params: {total_params:,}")
    print(f"     Trainable params: {trainable_params:,}")
    
    print("  âœ… PASS: PPO Agent initialization")
    
except Exception as e:
    print(f"  âŒ FAIL: {e}")
    import traceback
    traceback.print_exc()

# Test 4: Action Selection
print("\n[Test 4] Agent Action Selection")
try:
    env = RCTrackEnv()
    agent = PPOAgent(env.observation_space, env.action_space)
    
    obs, _ = env.reset()
    action, log_prob, value = agent.select_action(obs)
    
    print(f"  âœ… Action selected")
    print(f"     Action: {action}")
    print(f"     Log prob: {log_prob:.4f}")
    print(f"     Value: {value:.4f}")
    
    assert action.shape == (2,)
    assert isinstance(log_prob, (int, float))
    assert isinstance(value, (int, float))
    
    print("  âœ… PASS: Action selection works")
    
except Exception as e:
    print(f"  âŒ FAIL: {e}")
    import traceback
    traceback.print_exc()

# Test 5: Training Loop (Mini)
print("\n[Test 5] Mini Training Loop (10 steps)")
try:
    env = RCTrackEnv()
    agent = PPOAgent(env.observation_space, env.action_space)
    
    trajectories = []
    obs, _ = env.reset()
    
    for step in range(10):
        action, log_prob, value = agent.select_action(obs)
        next_obs, reward, terminated, truncated, info = env.step(action)
        
        trajectories.append({
            'obs': obs,
            'action': action,
            'reward': reward,
            'next_obs': next_obs,
            'done': terminated or truncated,
            'log_prob': log_prob,
            'value': value
        })
        
        obs = next_obs
        if terminated or truncated:
            obs, _ = env.reset()
    
    print(f"  âœ… Collected {len(trajectories)} transitions")
    
    # PPO update
    stats = agent.update(trajectories, num_epochs=1, batch_size=10)
    
    print(f"  âœ… PPO update executed")
    print(f"     Policy loss: {stats['policy_loss']:.4f}")
    print(f"     Value loss: {stats['value_loss']:.4f}")
    
    print("  âœ… PASS: Mini training loop works")
    
except Exception as e:
    print(f"  âŒ FAIL: {e}")
    import traceback
    traceback.print_exc()

# Test 6: Episode Rollout
print("\n[Test 6] Full Episode Rollout")
try:
    env = RCTrackEnv()
    agent = PPOAgent(env.observation_space, env.action_space)
    
    obs, _ = env.reset()
    episode_reward = 0
    steps = 0
    
    for _ in range(100):
        action, _, _ = agent.select_action(obs, deterministic=True)
        obs, reward, terminated, truncated, info = env.step(action)
        
        episode_reward += reward
        steps += 1
        
        if terminated or truncated:
            break
    
    print(f"  âœ… Episode completed")
    print(f"     Steps: {steps}")
    print(f"     Total reward: {episode_reward:.2f}")
    print(f"     Final position: x={info['car_x']:.2f}, y={info['car_y']:.2f}")
    print(f"     Goal reached: {info.get('goal_reached', False)}")
    
    print("  âœ… PASS: Full episode rollout")
    
except Exception as e:
    print(f"  âŒ FAIL: {e}")
    import traceback
    traceback.print_exc()

# Summary
print("\n" + "="*80)
print("ğŸ“Š Test Summary")
print("="*80)
print("""
âœ… Test 1: Environment initialization
âœ… Test 2: Environment step
âœ… Test 3: PPO Agent initialization
âœ… Test 4: Action selection
âœ… Test 5: Mini training loop
âœ… Test 6: Full episode rollout

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… Module 08 ê¸°ë³¸ ê¸°ëŠ¥ ëª¨ë‘ ì •ìƒ ì‘ë™!

í•µì‹¬ í™•ì¸ ì‚¬í•­:
  1. âœ… Gymnasium í™˜ê²½ ì‘ë™
  2. âœ… Observation/Action spaces ì •ìƒ
  3. âœ… PPO Agent ìƒì„± ê°€ëŠ¥
  4. âœ… Action selection ì‘ë™
  5. âœ… PPO update ê°€ëŠ¥
  6. âœ… Episode rollout ì •ìƒ

ë‹¤ìŒ ë‹¨ê³„:
  - ì‹¤ì œ í•™ìŠµ (train.py)
  - Curiosity module ì¶”ê°€
  - ì„±ëŠ¥ í‰ê°€
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
""")
print("="*80)
