# Module 08: Reinforcement Learning for Autonomous Driving
## 아키텍처 설계서 v1.0

**작성일:** 2026-01-30  
**버전:** 1.0  
**목표:** 2026년 최신 RL 기술로 자율주행 학습

---

## 1. 개요

### 1.1 목적
- **자율주행 정책을 강화학습으로 학습**
- 기존 rule-based (PID, MPC) → Learning-based
- 환경과 상호작용하며 스스로 학습

### 1.2 범위
- **시뮬레이션 환경:** Gymnasium 기반 커스텀 환경
- **알고리즘:** PPO (Proximal Policy Optimization)
- **추가 기법:** Curiosity-driven exploration, World Model
- **최종 목표:** RC 트랙에서 자율주행 정책 학습

### 1.3 핵심 차별화 (2026년 최신 기술)
1. **PPO** - OpenAI 표준 알고리즘
2. **Curiosity Module** - Intrinsic motivation (2024-2025 트렌드)
3. **World Model** - 환경 예측 모델 통합
4. **Offline RL** - 실제 데이터로 fine-tuning (안전성)
5. **Multi-modal state** - Vision + IMU + Previous actions

---

## 2. 시스템 아키텍처

### 2.1 전체 구조

```
┌─────────────────────────────────────────────────────────────┐
│                    RL Training System                        │
├─────────────────────────────────────────────────────────────┤
│                                                               │
│  ┌──────────────┐        ┌──────────────┐                   │
│  │ Environment  │◄──────►│    Agent     │                   │
│  │              │  obs   │              │                   │
│  │  - Track     │  ────► │  - Actor     │                   │
│  │  - RC Car    │        │  - Critic    │                   │
│  │  - Sensors   │  reward│  - PPO       │                   │
│  │              │◄────── │              │                   │
│  │              │  action│              │                   │
│  │              │◄────── │              │                   │
│  └──────────────┘        └──────────────┘                   │
│         │                        │                           │
│         │                        ▼                           │
│         │                ┌──────────────┐                   │
│         │                │ Curiosity    │                   │
│         │                │ Module       │                   │
│         │                │ (ICM)        │                   │
│         │                └──────────────┘                   │
│         │                        │                           │
│         ▼                        ▼                           │
│  ┌──────────────┐        ┌──────────────┐                   │
│  │ World Model  │        │ Replay       │                   │
│  │ (Dynamics)   │        │ Buffer       │                   │
│  └──────────────┘        └──────────────┘                   │
│                                                               │
└─────────────────────────────────────────────────────────────┘
```

### 2.2 컴포넌트 상세

#### A. Environment (Gymnasium 기반)
**역할:** 시뮬레이션 환경 제공

**State (관측):**
```python
state = {
    'image': (3, 84, 84),           # Front camera (CNN 입력)
    'velocity': float,               # 속도 (m/s)
    'steering': float,               # 현재 조향각 (deg)
    'lateral_offset': float,         # 차선 중심 오차 (m)
    'heading_error': float,          # 방향 오차 (rad)
    'distance_to_obstacle': float,   # 장애물 거리 (m)
    'prev_actions': (5,)             # 이전 5개 action
}
```

**Action (제어):**
```python
action = {
    'steering': float,  # -45° ~ +45°
    'throttle': float   # 0.0 ~ 1.0
}
```

**Reward (보상):**
```python
reward = w1 * speed_reward          # 빠르게 가기
       + w2 * centering_reward      # 중앙 유지
       + w3 * smoothness_reward     # 부드러운 제어
       - w4 * collision_penalty     # 충돌 방지
       - w5 * lane_departure_penalty # 차선 이탈 방지
       + curiosity_bonus            # 탐험 보너스
```

---

#### B. Agent (PPO)
**역할:** 정책 학습 및 행동 결정

**네트워크 구조:**

**1) Actor (정책 네트워크):**
```
Input: state
  │
  ├─► CNN (for image) ──► [256]
  │                          │
  ├─► MLP (for scalars) ──► [128]
  │                          │
  └─► Concat ───────────────┴──► [384]
                                   │
                                   ▼
                              [256] → [128]
                                   │
                                   ├─► μ (mean steering, throttle)
                                   └─► σ (std steering, throttle)
                                   
Output: π(a|s) = N(μ, σ²)
```

**2) Critic (가치 네트워크):**
```
Input: state
  │
  ├─► CNN (shared with Actor)
  │
  └─► MLP ──► [256] → [128] → [1]
                                │
                              V(s)
```

**PPO 알고리즘:**
```python
# Clipped surrogate objective
L^CLIP(θ) = E[min(
    r_t(θ) * A_t,
    clip(r_t(θ), 1-ε, 1+ε) * A_t
)]

where:
  r_t(θ) = π_θ(a_t|s_t) / π_θ_old(a_t|s_t)  # probability ratio
  A_t = advantage estimate (GAE)
  ε = 0.2  # clip parameter
```

---

#### C. Curiosity Module (ICM)
**역할:** Intrinsic motivation으로 탐험 유도

**구조 (Intrinsic Curiosity Module):**
```
┌────────────────────────────────────┐
│   Forward Model (Prediction)       │
│   φ(s_t), a_t → φ̂(s_{t+1})        │
└────────────────────────────────────┘
         │
         ├─► Prediction Error
         │   r_i = ||φ̂(s_{t+1}) - φ(s_{t+1})||²
         │
┌────────────────────────────────────┐
│   Inverse Model (Feature Learning) │
│   φ(s_t), φ(s_{t+1}) → â_t         │
└────────────────────────────────────┘
```

**최종 보상:**
```python
r_total = r_extrinsic + β * r_intrinsic
        = r_env + β * prediction_error
```

---

#### D. World Model
**역할:** 환경 dynamics 모델링 (미래 예측)

**구조:**
```
Input: (s_t, a_t)
  │
  ├─► Encoder ──► latent z_t
  │
  └─► Transition Model: (z_t, a_t) → z_{t+1}
                                        │
                                        ▼
                                   Decoder
                                        │
                                        ▼
                                  s_{t+1} (predicted)
```

**활용:**
- Planning: 미래 trajectory 예측
- Model-based RL: Imagined rollouts
- Safety: 위험 상황 사전 탐지

---

#### E. Replay Buffer
**역할:** 경험 저장 및 샘플링

**저장 데이터:**
```python
transition = (s_t, a_t, r_t, s_{t+1}, done, log_prob, value)
```

**샘플링 전략:**
- On-policy (PPO): 최근 trajectories
- Prioritized replay (optional): 중요한 경험 우선

---

## 3. 학습 파이프라인

### 3.1 학습 프로세스

```
1. Collect Trajectories
   └─► Agent interacts with environment (N steps)
   
2. Compute Returns & Advantages
   └─► GAE (Generalized Advantage Estimation)
   
3. PPO Update (K epochs)
   └─► Actor update (policy)
   └─► Critic update (value function)
   
4. Curiosity Update
   └─► Forward/Inverse model training
   
5. World Model Update
   └─► Dynamics prediction training
   
6. Evaluate & Log
   └─► Test on validation tracks
```

### 3.2 Hyperparameters

```yaml
# PPO
learning_rate: 3e-4
gamma: 0.99           # discount factor
gae_lambda: 0.95      # GAE parameter
clip_epsilon: 0.2     # PPO clip
value_coef: 0.5       # value loss weight
entropy_coef: 0.01    # exploration bonus

# Training
batch_size: 2048      # transitions per update
num_epochs: 10        # PPO epochs per batch
num_envs: 8           # parallel environments
max_timesteps: 1e7    # total training steps

# Curiosity
curiosity_beta: 0.2   # intrinsic reward weight
curiosity_lr: 1e-3

# World Model
world_model_lr: 1e-4
latent_dim: 256
```

---

## 4. 평가 메트릭

### 4.1 학습 성능
- **Episode Reward:** 평균 누적 보상
- **Success Rate:** 트랙 완주율
- **Convergence Speed:** 수렴까지 timesteps

### 4.2 주행 품질
- **Lap Time:** 트랙 완주 시간
- **Lateral Error (RMS):** 차선 중심 오차
- **Smoothness:** 조향/가속 변화율
- **Collision Rate:** 충돌 빈도

### 4.3 탐험 효율
- **State Coverage:** 방문한 상태 공간 범위
- **Intrinsic Reward:** Curiosity 기여도

---

## 5. 기술 스택

### 5.1 Core Libraries
```python
torch >= 2.0.0              # Deep learning
gymnasium >= 0.29.0         # RL environment
stable-baselines3 >= 2.2.0  # RL algorithms (PPO)
```

### 5.2 Simulation
```python
pygame >= 2.5.0             # 2D visualization
pymunk >= 6.6.0             # Physics engine
opencv-python >= 4.9.0      # Image processing
```

### 5.3 Utilities
```python
tensorboard >= 2.15.0       # Logging
wandb >= 0.16.0             # Experiment tracking
matplotlib, seaborn          # Plotting
```

---

## 6. 시뮬레이션 환경 설계

### 6.1 트랙 설계

**3가지 난이도:**

**Easy Track:**
- 직선 + 완만한 곡선
- 장애물 없음
- 길이: 100m

**Medium Track:**
- S-curve + 급커브
- 고정 장애물 3개
- 길이: 150m

**Hard Track:**
- 복잡한 곡선 + 좁은 구간
- 동적 장애물 (moving cones)
- 길이: 200m

### 6.2 물리 시뮬레이션

**RC Car Dynamics:**
```python
# Kinematic bicycle model
x_dot = v * cos(θ)
y_dot = v * sin(θ)
θ_dot = (v / L) * tan(δ)
v_dot = throttle * a_max - friction * v²

where:
  L = wheelbase (0.25m)
  δ = steering angle
  v = velocity
  θ = heading
```

---

## 7. 하드웨어 통합 계획 (추후)

### 7.1 Sim-to-Real Transfer 전략

**1) Domain Randomization:**
- 트랙 마찰계수 랜덤화
- 카메라 노이즈/밝기 변화
- 물리 파라미터 변동

**2) System Identification:**
- 실제 RC카 dynamics 측정
- 시뮬레이터 파라미터 튜닝

**3) Progressive Transfer:**
- 시뮬레이션 학습 → 안전한 환경 테스트 → 실제 트랙

### 7.2 하드웨어 요구사항

**센서:**
- ✅ Camera (front-facing)
- ✅ IMU (가속도, 자이로)
- ✅ Wheel encoders (속도)

**컴퓨팅:**
- Raspberry Pi 4 (8GB) 또는 Jetson Nano
- 추론 속도: 최소 10 Hz

**통신:**
- PWM for steering/throttle
- ROS2 (optional)

---

## 8. 연구 기여 (Novel Contributions)

### 8.1 기술적 기여

1. **Curiosity-driven RC car RL**
   - RC 트랙에서 intrinsic motivation 적용 (신규)
   
2. **Multi-modal state fusion**
   - Vision + IMU + proprioception 통합
   
3. **World Model for safe RL**
   - 충돌 예측 및 회피

4. **Offline-to-Online RL**
   - 실제 데이터로 pre-train → 시뮬 fine-tune

### 8.2 예상 성과

**시뮬레이션:**
- Lap time: < 30초 (Hard track)
- Success rate: > 95%
- 수렴 속도: < 5M timesteps

**실제 RC카:**
- Lap time: < 40초
- Collision rate: < 5%
- Sim-to-real gap: < 20%

---

## 9. 리스크 및 대응

### 9.1 리스크

| 리스크 | 영향 | 대응 |
|--------|------|------|
| 학습 불안정 | High | PPO hyperparameter tuning |
| Sim-to-real gap | High | Domain randomization |
| 수렴 느림 | Medium | Curiosity module |
| Overfitting | Medium | Early stopping, validation |

---

## 10. 마일스톤

### Phase 1: Environment (3일)
- ✅ Gymnasium 환경 구축
- ✅ 물리 시뮬레이션
- ✅ 3가지 트랙 설계

### Phase 2: PPO Agent (4일)
- ✅ Actor-Critic 네트워크
- ✅ PPO 알고리즘 구현
- ✅ Baseline 학습

### Phase 3: Curiosity (2일)
- ✅ ICM 구현
- ✅ Intrinsic reward 통합

### Phase 4: World Model (2일)
- ✅ Dynamics prediction
- ✅ Planning 통합

### Phase 5: 학습 & 평가 (3일)
- ✅ 3가지 트랙 학습
- ✅ Ablation studies
- ✅ 성능 측정

---

**작성자:** AI Research Team  
**승인:** Pending  
**다음 문서:** 02_구현_명세서.md
