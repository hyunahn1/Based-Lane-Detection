# Module 08: Reinforcement Learning - 검증서 v1.0

**작성일:** 2026-01-30  
**버전:** 1.0  
**목표:** PPO + Curiosity 학습 및 성능 검증

---

## 1. 검증 전략

### 1.1 검증 목표
1. **학습 수렴 검증** - PPO가 안정적으로 학습하는가?
2. **성능 목표 달성** - 트랙 완주율, lap time 목표 달성?
3. **Curiosity 효과** - Intrinsic motivation이 탐험을 개선하는가?
4. **Ablation Study** - 각 컴포넌트의 기여도는?
5. **Sim-to-Real 준비** - 하드웨어 통합 가능성?

---

## 2. 테스트 환경

### 2.1 시뮬레이션 환경

**3가지 트랙:**

| Track | 길이 | 곡선 | 장애물 | 난이도 |
|-------|------|------|--------|--------|
| Easy | 100m | 완만 | 0개 | ⭐ |
| Medium | 150m | S-curve | 3개 | ⭐⭐⭐ |
| Hard | 200m | 급커브 | 5개 (moving) | ⭐⭐⭐⭐⭐ |

### 2.2 평가 메트릭

#### A. 학습 성능
```python
metrics = {
    'episode_reward': List[float],      # 누적 보상
    'success_rate': float,              # 완주율 (%)
    'convergence_steps': int,           # 수렴까지 timesteps
    'training_time': float              # 학습 시간 (hours)
}
```

#### B. 주행 품질
```python
driving_metrics = {
    'lap_time': float,                  # 완주 시간 (seconds)
    'lateral_error_rms': float,         # 차선 오차 RMS (m)
    'heading_error_rms': float,         # 방향 오차 RMS (rad)
    'smoothness': float,                # 제어 부드러움 (std)
    'collision_rate': float,            # 충돌 빈도 (%)
    'avg_velocity': float               # 평균 속도 (m/s)
}
```

#### C. 탐험 효율
```python
exploration_metrics = {
    'state_coverage': float,            # 방문한 상태 비율 (%)
    'intrinsic_reward': float,          # Curiosity 기여도
    'entropy': float                    # 정책 엔트로피
}
```

---

## 3. 성능 목표 (KPI)

### 3.1 Easy Track (Baseline)

| Metric | Target | Acceptable |
|--------|--------|------------|
| **Success Rate** | ≥ 98% | ≥ 95% |
| **Lap Time** | ≤ 20s | ≤ 25s |
| **Lateral Error (RMS)** | ≤ 0.05m | ≤ 0.08m |
| **Collision Rate** | ≤ 1% | ≤ 3% |
| **Convergence** | ≤ 2M steps | ≤ 3M steps |

### 3.2 Medium Track

| Metric | Target | Acceptable |
|--------|--------|------------|
| **Success Rate** | ≥ 90% | ≥ 80% |
| **Lap Time** | ≤ 35s | ≤ 45s |
| **Lateral Error (RMS)** | ≤ 0.08m | ≤ 0.12m |
| **Collision Rate** | ≤ 5% | ≤ 10% |
| **Convergence** | ≤ 5M steps | ≤ 7M steps |

### 3.3 Hard Track

| Metric | Target | Acceptable |
|--------|--------|------------|
| **Success Rate** | ≥ 80% | ≥ 70% |
| **Lap Time** | ≤ 50s | ≤ 60s |
| **Lateral Error (RMS)** | ≤ 0.10m | ≤ 0.15m |
| **Collision Rate** | ≤ 10% | ≤ 15% |
| **Convergence** | ≤ 10M steps | ≤ 15M steps |

---

## 4. 테스트 케이스

### 4.1 Unit Tests (pytest)

#### Test 1: Environment Initialization
```python
def test_environment_init():
    """환경 초기화 검증"""
    env = RCTrackEnv(track_type='easy')
    obs, info = env.reset()
    
    # Check observation space
    assert 'image' in obs
    assert obs['image'].shape == (3, 84, 84)
    assert 'velocity' in obs
    assert 'steering' in obs
    
    # Check action space
    action = env.action_space.sample()
    assert action.shape == (2,)
    
    print("✅ PASS: Environment initialization")
```

#### Test 2: Agent Action Selection
```python
def test_agent_action():
    """Agent 행동 선택 검증"""
    env = RCTrackEnv()
    agent = PPOAgent(env.observation_space, env.action_space)
    
    obs, _ = env.reset()
    action, log_prob, value = agent.select_action(obs)
    
    assert action.shape == (2,)
    assert isinstance(log_prob, float)
    assert isinstance(value, float)
    
    print("✅ PASS: Agent action selection")
```

#### Test 3: Curiosity Module
```python
def test_curiosity():
    """Curiosity 모듈 검증"""
    env = RCTrackEnv()
    curiosity = IntrinsicCuriosityModule(
        env.observation_space,
        action_dim=2
    )
    
    obs, _ = env.reset()
    action = env.action_space.sample()
    next_obs, _, _, _, _ = env.step(action)
    
    # Convert to tensors
    obs_t = torch.FloatTensor(obs['image']).unsqueeze(0)
    next_obs_t = torch.FloatTensor(next_obs['image']).unsqueeze(0)
    action_t = torch.FloatTensor(action).unsqueeze(0)
    
    # Compute intrinsic reward
    r_i = curiosity.compute_intrinsic_reward(obs_t, next_obs_t, action_t)
    
    assert r_i.item() >= 0
    
    print("✅ PASS: Curiosity module")
```

---

### 4.2 Integration Tests

#### Test 4: PPO Training Loop
```python
def test_ppo_training():
    """PPO 학습 루프 검증 (short run)"""
    env = RCTrackEnv(track_type='easy')
    agent = PPOAgent(env.observation_space, env.action_space)
    
    trajectories = []
    obs, _ = env.reset()
    
    # Collect 100 transitions
    for _ in range(100):
        action, log_prob, value = agent.select_action(obs)
        next_obs, reward, terminated, truncated, _ = env.step(action)
        
        trajectories.append({
            'obs': obs,
            'action': action,
            'reward': reward,
            'next_obs': next_obs,
            'done': terminated or truncated,
            'log_prob': log_prob,
            'value': value
        })
        
        obs = next_obs
        if terminated or truncated:
            obs, _ = env.reset()
    
    # Update
    stats = agent.update(trajectories, num_epochs=1)
    
    assert 'policy_loss' in stats
    assert 'value_loss' in stats
    
    print("✅ PASS: PPO training loop")
```

---

### 4.3 Performance Tests

#### Test 5: Easy Track Performance
```python
def test_easy_track_performance():
    """Easy track 성능 검증"""
    # Load trained agent
    agent = PPOAgent.load('checkpoints/easy_track_best.pt')
    env = RCTrackEnv(track_type='easy')
    
    # Run 100 episodes
    success_count = 0
    lap_times = []
    
    for episode in range(100):
        obs, _ = env.reset()
        done = False
        step_count = 0
        
        while not done:
            action, _, _ = agent.select_action(obs, deterministic=True)
            obs, _, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            step_count += 1
        
        if info.get('goal_reached'):
            success_count += 1
            lap_times.append(step_count * env.dt)
    
    success_rate = success_count / 100
    avg_lap_time = np.mean(lap_times) if lap_times else float('inf')
    
    print(f"Success Rate: {success_rate:.2%}")
    print(f"Avg Lap Time: {avg_lap_time:.2f}s")
    
    assert success_rate >= 0.95, f"Success rate {success_rate:.2%} < 95%"
    assert avg_lap_time <= 25.0, f"Lap time {avg_lap_time:.2f}s > 25s"
    
    print("✅ PASS: Easy track performance")
```

---

### 4.4 Ablation Study Tests

#### Test 6: PPO vs PPO+Curiosity
```python
def test_ablation_curiosity():
    """Curiosity 효과 검증"""
    # Train PPO baseline
    agent_baseline = train_ppo(use_curiosity=False, max_steps=1e6)
    
    # Train PPO + Curiosity
    agent_curiosity = train_ppo(use_curiosity=True, max_steps=1e6)
    
    # Evaluate
    baseline_performance = evaluate(agent_baseline)
    curiosity_performance = evaluate(agent_curiosity)
    
    print("Baseline:")
    print(f"  Success Rate: {baseline_performance['success_rate']:.2%}")
    print(f"  Convergence: {baseline_performance['convergence_steps']}")
    
    print("With Curiosity:")
    print(f"  Success Rate: {curiosity_performance['success_rate']:.2%}")
    print(f"  Convergence: {curiosity_performance['convergence_steps']}")
    
    # Expectation: Curiosity improves exploration
    improvement = (
        curiosity_performance['success_rate'] - 
        baseline_performance['success_rate']
    )
    
    print(f"Improvement: {improvement:+.2%}")
    
    print("✅ PASS: Curiosity ablation")
```

---

## 5. 벤치마크 비교

### 5.1 Baseline Comparisons

| Method | Easy Track | Medium Track | Hard Track | Convergence |
|--------|------------|--------------|------------|-------------|
| **Random Policy** | 0% | 0% | 0% | N/A |
| **PID (Module 02)** | 95% | 70% | 40% | N/A |
| **MPC (Module 02)** | 98% | 85% | 60% | N/A |
| **PPO (Ours)** | **99%** | **90%** | **80%** | 5M |
| **PPO + Curiosity** | **99%** | **92%** | **85%** | **3M** ⚡ |

**예상 결과:**
- ✅ RL이 rule-based보다 우수
- ✅ Curiosity가 수렴 속도 40% 개선
- ✅ 복잡한 트랙에서 더 큰 차이

---

### 5.2 Ablation Study Results (예상)

| Configuration | Success Rate | Lap Time | Convergence |
|---------------|--------------|----------|-------------|
| PPO Baseline | 85% | 45s | 7M steps |
| + Curiosity | **90%** | 40s | **5M steps** |
| + World Model | **92%** | **38s** | 4.5M steps |
| Full (+ Offline RL) | **95%** | **35s** | **3M steps** |

---

## 6. 안전성 검증

### 6.1 Failure Cases

**시나리오:**
1. 급커브 고속 진입
2. 장애물 회피 실패
3. 차선 이탈 후 복귀 실패

**테스트:**
```python
def test_safety_scenarios():
    """안전 시나리오 검증"""
    agent = load_agent('best_model.pt')
    
    scenarios = [
        'sharp_turn_high_speed',
        'sudden_obstacle',
        'lane_departure'
    ]
    
    for scenario in scenarios:
        env = create_safety_scenario(scenario)
        obs, _ = env.reset()
        
        collision = False
        for _ in range(100):
            action, _, _ = agent.select_action(obs)
            obs, _, terminated, _, info = env.step(action)
            
            if info.get('collision'):
                collision = True
                break
        
        print(f"{scenario}: {'FAIL' if collision else 'PASS'}")
```

---

### 6.2 Robustness Tests

**Perturbation Tests:**
```python
def test_robustness():
    """강건성 검증 (노이즈, 지연 등)"""
    agent = load_agent('best_model.pt')
    
    perturbations = {
        'sensor_noise': [0.0, 0.05, 0.1],
        'action_delay': [0, 1, 2],  # timesteps
        'vision_blur': [0, 3, 5]    # blur kernel size
    }
    
    for perturb_type, values in perturbations.items():
        for value in values:
            env = RCTrackEnv(perturbation={perturb_type: value})
            success_rate = evaluate(agent, env, num_episodes=50)
            
            print(f"{perturb_type}={value}: {success_rate:.2%}")
```

---

## 7. 시각화 및 분석

### 7.1 학습 곡선
```python
def plot_learning_curves():
    """학습 곡선 시각화"""
    # Load training logs
    logs = load_tensorboard_logs('runs/ppo_curiosity')
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    # Episode reward
    axes[0, 0].plot(logs['episode_reward'])
    axes[0, 0].set_title('Episode Reward')
    
    # Success rate (rolling average)
    axes[0, 1].plot(rolling_average(logs['success_rate'], window=100))
    axes[0, 1].set_title('Success Rate')
    
    # Policy loss
    axes[1, 0].plot(logs['policy_loss'])
    axes[1, 0].set_title('Policy Loss')
    
    # Intrinsic reward
    axes[1, 1].plot(logs['intrinsic_reward'])
    axes[1, 1].set_title('Intrinsic Reward')
    
    plt.tight_layout()
    plt.savefig('results/learning_curves.png')
```

### 7.2 Trajectory Visualization
```python
def visualize_trajectory():
    """주행 궤적 시각화"""
    agent = load_agent('best_model.pt')
    env = RCTrackEnv(track_type='medium', render_mode='rgb_array')
    
    obs, _ = env.reset()
    trajectory = []
    
    for _ in range(1000):
        action, _, _ = agent.select_action(obs, deterministic=True)
        obs, _, terminated, truncated, _ = env.step(action)
        
        trajectory.append(env.car.position)
        
        if terminated or truncated:
            break
    
    # Plot
    trajectory = np.array(trajectory)
    plt.plot(trajectory[:, 0], trajectory[:, 1], 'b-', label='Agent')
    plt.plot(*env.track.centerline.T, 'r--', label='Centerline')
    plt.legend()
    plt.title('Trajectory Comparison')
    plt.savefig('results/trajectory.png')
```

---

## 8. 하드웨어 통합 준비

### 8.1 Sim-to-Real Validation

**체크리스트:**

- [ ] **Dynamics Match**
  - 시뮬레이션 vs 실제 RC car 비교
  - 조향/가속 응답 시간 측정
  - 마찰 계수 calibration

- [ ] **Sensor Validation**
  - 카메라 해상도/FPS 확인
  - IMU 정확도 측정
  - Latency 측정 (< 100ms)

- [ ] **Safety Mechanisms**
  - Emergency stop 구현
  - Failsafe mode (충돌 감지 → 정지)
  - Human override

- [ ] **Performance on Hardware**
  - 추론 속도: > 10 Hz
  - 메모리 사용: < 4GB
  - 배터리 수명: > 30분

---

### 8.2 Domain Randomization Test

```python
def test_domain_randomization():
    """Domain randomization 효과 검증"""
    # Train with randomization
    agent_rand = train_with_randomization(
        friction_range=(0.5, 1.5),
        noise_std=0.1,
        delay_range=(0, 2)
    )
    
    # Train without randomization
    agent_no_rand = train_without_randomization()
    
    # Test on varied conditions
    test_conditions = [
        {'friction': 0.7, 'noise': 0.05},
        {'friction': 1.3, 'noise': 0.15},
        {'friction': 1.0, 'noise': 0.20}
    ]
    
    for condition in test_conditions:
        env = RCTrackEnv(**condition)
        
        success_rand = evaluate(agent_rand, env)
        success_no_rand = evaluate(agent_no_rand, env)
        
        print(f"Condition {condition}:")
        print(f"  With randomization: {success_rand:.2%}")
        print(f"  Without randomization: {success_no_rand:.2%}")
```

---

## 9. 검증 성공 기준

### 9.1 필수 (Mandatory)

✅ **학습 수렴**
- Easy track: 수렴 확인 (< 3M steps)
- Medium track: 수렴 확인 (< 7M steps)

✅ **성능 목표**
- Easy: Success rate ≥ 95%
- Medium: Success rate ≥ 80%

✅ **코드 품질**
- Unit tests 통과율: 100%
- Integration tests 통과율: 100%

---

### 9.2 권장 (Recommended)

⭐ **고급 기능**
- Hard track 성공률: ≥ 70%
- Curiosity 효과 검증: 수렴 속도 20%+ 개선

⭐ **하드웨어 준비**
- Sim-to-real 체크리스트: 80%+ 완료
- Domain randomization 검증

---

## 10. 테스트 실행 가이드

### 10.1 Quick Test (5분)
```bash
# Unit tests
pytest tests/test_environment.py -v
pytest tests/test_agent.py -v

# Quick training
python train.py --config config/quick_test.yaml --max_steps 10000
```

### 10.2 Full Test (3시간)
```bash
# Full training (Easy track)
python train.py --track easy --max_steps 3000000

# Evaluation
python evaluate.py --checkpoint checkpoints/easy_best.pt --num_episodes 100

# Ablation study
python ablation_study.py --configs config/ablation/
```

### 10.3 Hardware Test (하드웨어 준비 후)
```bash
# Deploy to RC car
python deploy.py --model best_model.pt --device jetson

# Real-world evaluation
python test_hardware.py --track real_track --safety_mode on
```

---

## 11. 예상 결과

### 11.1 학습 성능 (시뮬레이션)

**Easy Track:**
- Success rate: **99%** ✅
- Lap time: **18s** ✅
- Convergence: **2M steps** ✅

**Medium Track:**
- Success rate: **92%** ✅
- Lap time: **38s** ✅
- Convergence: **5M steps** ✅

**Hard Track:**
- Success rate: **85%** ✅
- Lap time: **48s** ✅
- Convergence: **8M steps** ✅

---

### 11.2 연구 기여 검증

1. ✅ **Curiosity 효과**
   - 수렴 속도: 40% 개선
   - 탐험 효율: 60% 향상

2. ✅ **PPO vs MPC**
   - 복잡한 환경: PPO 20% 우수
   - 학습 가능: Adaptive behavior

3. ✅ **Sim-to-Real**
   - Domain randomization으로 gap 감소
   - 실제 RC car 테스트 준비 완료

---

**작성자:** AI Validation Team  
**승인:** Pending  
**다음 단계:** 구현 시작 및 팩트체크
