# Module 08: Reinforcement Learning - 구현 명세서 v1.0

**작성일:** 2026-01-30  
**버전:** 1.0  
**목표:** PPO + Curiosity + World Model 상세 구현

---

## 1. 디렉토리 구조

```
08-reinforcement-learning/
├── src/
│   ├── environment/
│   │   ├── __init__.py
│   │   ├── rc_track_env.py        # Gymnasium environment
│   │   ├── physics.py              # RC car dynamics
│   │   ├── track.py                # Track generation
│   │   └── rendering.py            # Visualization
│   │
│   ├── agent/
│   │   ├── __init__.py
│   │   ├── ppo_agent.py           # PPO implementation
│   │   ├── networks.py             # Actor-Critic networks
│   │   └── gae.py                  # Advantage estimation
│   │
│   ├── policy/
│   │   ├── __init__.py
│   │   ├── actor.py                # Policy network
│   │   └── critic.py               # Value network
│   │
│   ├── curiosity/
│   │   ├── __init__.py
│   │   ├── icm.py                  # Intrinsic Curiosity Module
│   │   └── forward_inverse.py      # Forward/Inverse models
│   │
│   ├── world_model/
│   │   ├── __init__.py
│   │   ├── dynamics.py             # World model
│   │   └── planner.py              # Model-based planning
│   │
│   ├── replay/
│   │   ├── __init__.py
│   │   └── buffer.py               # Replay buffer
│   │
│   └── utils/
│       ├── __init__.py
│       ├── logger.py               # TensorBoard/wandb
│       ├── config.py               # Hyperparameters
│       └── metrics.py              # Evaluation metrics
│
├── tests/
│   ├── test_environment.py
│   ├── test_agent.py
│   └── test_curiosity.py
│
├── config/
│   ├── ppo_config.yaml            # PPO hyperparameters
│   ├── env_config.yaml            # Environment settings
│   └── curiosity_config.yaml      # Curiosity settings
│
├── train.py                        # Training script
├── evaluate.py                     # Evaluation script
├── visualize.py                    # Visualization tool
└── README.md
```

---

## 2. 핵심 클래스 상세

### 2.1 Environment (`rc_track_env.py`)

```python
import gymnasium as gym
import numpy as np
from typing import Dict, Tuple, Optional

class RCTrackEnv(gym.Env):
    """
    RC Track Gymnasium Environment
    
    Observation Space:
        - image: Box(0, 255, shape=(3, 84, 84), dtype=uint8)
        - velocity: Box(low=0, high=3.0, dtype=float32)
        - steering: Box(low=-45, high=45, dtype=float32)
        - lateral_offset: Box(low=-0.5, high=0.5, dtype=float32)
        - heading_error: Box(low=-π, high=π, dtype=float32)
        - distance_to_obstacle: Box(low=0, high=10.0, dtype=float32)
        - prev_actions: Box(low=-1, high=1, shape=(5, 2), dtype=float32)
    
    Action Space:
        - steering: Box(low=-1, high=1, dtype=float32)  # normalized to [-45, 45]
        - throttle: Box(low=0, high=1, dtype=float32)
    """
    
    metadata = {'render_modes': ['human', 'rgb_array']}
    
    def __init__(
        self,
        track_type: str = 'easy',
        render_mode: Optional[str] = None,
        max_steps: int = 1000,
        dt: float = 0.1  # 10 Hz
    ):
        super().__init__()
        
        self.track_type = track_type
        self.render_mode = render_mode
        self.max_steps = max_steps
        self.dt = dt
        
        # Spaces
        self.observation_space = gym.spaces.Dict({
            'image': gym.spaces.Box(0, 255, shape=(3, 84, 84), dtype=np.uint8),
            'velocity': gym.spaces.Box(0, 3.0, shape=(1,), dtype=np.float32),
            'steering': gym.spaces.Box(-45, 45, shape=(1,), dtype=np.float32),
            'lateral_offset': gym.spaces.Box(-0.5, 0.5, shape=(1,), dtype=np.float32),
            'heading_error': gym.spaces.Box(-np.pi, np.pi, shape=(1,), dtype=np.float32),
            'distance_to_obstacle': gym.spaces.Box(0, 10.0, shape=(1,), dtype=np.float32),
            'prev_actions': gym.spaces.Box(-1, 1, shape=(5, 2), dtype=np.float32)
        })
        
        self.action_space = gym.spaces.Box(
            low=np.array([-1.0, 0.0]),
            high=np.array([1.0, 1.0]),
            dtype=np.float32
        )
        
        # Initialize components
        self.track = Track(track_type)
        self.car = RCCar()
        self.camera = Camera()
        
        # State
        self.step_count = 0
        self.prev_actions = np.zeros((5, 2))
    
    def reset(
        self,
        seed: Optional[int] = None,
        options: Optional[dict] = None
    ) -> Tuple[Dict, Dict]:
        super().reset(seed=seed)
        
        # Reset car
        self.car.reset(self.track.start_pos, self.track.start_heading)
        self.step_count = 0
        self.prev_actions = np.zeros((5, 2))
        
        obs = self._get_obs()
        info = self._get_info()
        
        return obs, info
    
    def step(self, action: np.ndarray) -> Tuple[Dict, float, bool, bool, Dict]:
        """
        Execute one timestep
        
        Returns:
            obs, reward, terminated, truncated, info
        """
        # Denormalize action
        steering = action[0] * 45.0  # [-1, 1] → [-45, 45] degrees
        throttle = action[1]  # [0, 1]
        
        # Apply action to car
        self.car.update(steering, throttle, self.dt)
        
        # Update prev actions
        self.prev_actions = np.roll(self.prev_actions, -1, axis=0)
        self.prev_actions[-1] = action
        
        # Check termination
        collision = self.track.check_collision(self.car.position)
        off_track = self.track.is_off_track(self.car.position)
        goal_reached = self.track.check_goal(self.car.position)
        
        terminated = collision or off_track or goal_reached
        truncated = self.step_count >= self.max_steps
        
        # Compute reward
        reward = self._compute_reward(collision, off_track, goal_reached)
        
        # Get observation
        obs = self._get_obs()
        info = self._get_info()
        info['collision'] = collision
        info['off_track'] = off_track
        info['goal_reached'] = goal_reached
        
        self.step_count += 1
        
        return obs, reward, terminated, truncated, info
    
    def _get_obs(self) -> Dict:
        """Get current observation"""
        # Capture camera image
        image = self.camera.capture(self.car, self.track)
        
        # Get car state
        lateral_offset = self.track.get_lateral_offset(self.car.position)
        heading_error = self.track.get_heading_error(self.car.heading)
        distance_to_obstacle = self.track.get_nearest_obstacle_distance(
            self.car.position
        )
        
        obs = {
            'image': image,
            'velocity': np.array([self.car.velocity], dtype=np.float32),
            'steering': np.array([self.car.steering], dtype=np.float32),
            'lateral_offset': np.array([lateral_offset], dtype=np.float32),
            'heading_error': np.array([heading_error], dtype=np.float32),
            'distance_to_obstacle': np.array([distance_to_obstacle], dtype=np.float32),
            'prev_actions': self.prev_actions.copy()
        }
        
        return obs
    
    def _compute_reward(
        self,
        collision: bool,
        off_track: bool,
        goal_reached: bool
    ) -> float:
        """
        Reward function
        
        Components:
            1. Speed reward: 빠르게 가기
            2. Centering reward: 중앙 유지
            3. Smoothness reward: 부드러운 제어
            4. Collision penalty: 충돌 방지
            5. Lane departure penalty: 차선 이탈 방지
            6. Goal bonus: 목표 도달
        """
        reward = 0.0
        
        # 1. Speed reward
        reward += self.car.velocity * 0.5
        
        # 2. Centering reward
        lateral_offset = self.track.get_lateral_offset(self.car.position)
        centering = np.exp(-5 * abs(lateral_offset))
        reward += centering * 1.0
        
        # 3. Smoothness reward (penalize large changes)
        if len(self.prev_actions) > 1:
            action_diff = np.linalg.norm(
                self.prev_actions[-1] - self.prev_actions[-2]
            )
            smoothness = np.exp(-action_diff)
            reward += smoothness * 0.2
        
        # 4. Collision penalty
        if collision:
            reward -= 100.0
        
        # 5. Off-track penalty
        if off_track:
            reward -= 50.0
        
        # 6. Goal bonus
        if goal_reached:
            reward += 200.0
        
        return reward
    
    def render(self):
        """Render environment"""
        if self.render_mode == 'human':
            return self._render_human()
        elif self.render_mode == 'rgb_array':
            return self._render_rgb()
    
    def close(self):
        """Clean up"""
        pass
```

---

### 2.2 PPO Agent (`ppo_agent.py`)

```python
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Dict, List, Tuple
import numpy as np

from .networks import ActorCritic
from .gae import compute_gae

class PPOAgent:
    """
    Proximal Policy Optimization Agent
    
    Features:
        - Clipped surrogate objective
        - GAE for advantage estimation
        - Value function clipping
        - Entropy bonus for exploration
    """
    
    def __init__(
        self,
        obs_space,
        action_space,
        lr: float = 3e-4,
        gamma: float = 0.99,
        gae_lambda: float = 0.95,
        clip_epsilon: float = 0.2,
        value_coef: float = 0.5,
        entropy_coef: float = 0.01,
        max_grad_norm: float = 0.5,
        device: str = 'cuda'
    ):
        self.obs_space = obs_space
        self.action_space = action_space
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_epsilon = clip_epsilon
        self.value_coef = value_coef
        self.entropy_coef = entropy_coef
        self.max_grad_norm = max_grad_norm
        self.device = device
        
        # Actor-Critic network
        self.policy = ActorCritic(obs_space, action_space).to(device)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        
        # Stats
        self.train_stats = {
            'policy_loss': [],
            'value_loss': [],
            'entropy': [],
            'kl_div': []
        }
    
    def select_action(
        self,
        obs: Dict[str, np.ndarray],
        deterministic: bool = False
    ) -> Tuple[np.ndarray, float, float]:
        """
        Select action given observation
        
        Returns:
            action, log_prob, value
        """
        with torch.no_grad():
            obs_tensor = self._obs_to_tensor(obs)
            
            # Forward pass
            action_mean, action_std, value = self.policy(obs_tensor)
            
            if deterministic:
                action = action_mean
            else:
                # Sample from Gaussian
                dist = torch.distributions.Normal(action_mean, action_std)
                action = dist.sample()
                log_prob = dist.log_prob(action).sum(dim=-1)
            
            action = action.cpu().numpy()[0]
            log_prob = log_prob.cpu().numpy()[0] if not deterministic else 0.0
            value = value.cpu().numpy()[0, 0]
        
        # Clip action to valid range
        action = np.clip(action, self.action_space.low, self.action_space.high)
        
        return action, log_prob, value
    
    def update(
        self,
        trajectories: List[Dict],
        num_epochs: int = 10,
        batch_size: int = 64
    ) -> Dict:
        """
        PPO update
        
        Args:
            trajectories: List of {obs, action, reward, next_obs, done, log_prob, value}
            num_epochs: Number of optimization epochs
            batch_size: Mini-batch size
        
        Returns:
            Training statistics
        """
        # Extract data
        obs_batch = self._stack_obs([t['obs'] for t in trajectories])
        actions = torch.FloatTensor([t['action'] for t in trajectories]).to(self.device)
        old_log_probs = torch.FloatTensor([t['log_prob'] for t in trajectories]).to(self.device)
        values = torch.FloatTensor([t['value'] for t in trajectories]).to(self.device)
        rewards = torch.FloatTensor([t['reward'] for t in trajectories]).to(self.device)
        dones = torch.FloatTensor([t['done'] for t in trajectories]).to(self.device)
        
        # Compute returns and advantages
        returns, advantages = compute_gae(
            rewards, values, dones,
            gamma=self.gamma,
            lam=self.gae_lambda
        )
        
        # Normalize advantages
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # PPO epochs
        num_samples = len(trajectories)
        indices = np.arange(num_samples)
        
        for epoch in range(num_epochs):
            np.random.shuffle(indices)
            
            for start in range(0, num_samples, batch_size):
                end = start + batch_size
                batch_indices = indices[start:end]
                
                # Get batch
                obs_b = {k: v[batch_indices] for k, v in obs_batch.items()}
                actions_b = actions[batch_indices]
                old_log_probs_b = old_log_probs[batch_indices]
                returns_b = returns[batch_indices]
                advantages_b = advantages[batch_indices]
                
                # Forward pass
                action_mean, action_std, values_pred = self.policy(obs_b)
                
                # Policy loss
                dist = torch.distributions.Normal(action_mean, action_std)
                log_probs = dist.log_prob(actions_b).sum(dim=-1)
                entropy = dist.entropy().mean()
                
                # Probability ratio
                ratio = torch.exp(log_probs - old_log_probs_b)
                
                # Clipped surrogate loss
                surr1 = ratio * advantages_b
                surr2 = torch.clamp(ratio, 1-self.clip_epsilon, 1+self.clip_epsilon) * advantages_b
                policy_loss = -torch.min(surr1, surr2).mean()
                
                # Value loss (clipped)
                value_loss = 0.5 * (returns_b - values_pred.squeeze()).pow(2).mean()
                
                # Total loss
                loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy
                
                # Optimization step
                self.optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)
                self.optimizer.step()
                
                # Log stats
                with torch.no_grad():
                    kl_div = (old_log_probs_b - log_probs).mean()
                
                self.train_stats['policy_loss'].append(policy_loss.item())
                self.train_stats['value_loss'].append(value_loss.item())
                self.train_stats['entropy'].append(entropy.item())
                self.train_stats['kl_div'].append(kl_div.item())
        
        # Return average stats
        stats = {
            'policy_loss': np.mean(self.train_stats['policy_loss'][-num_epochs:]),
            'value_loss': np.mean(self.train_stats['value_loss'][-num_epochs:]),
            'entropy': np.mean(self.train_stats['entropy'][-num_epochs:]),
            'kl_div': np.mean(self.train_stats['kl_div'][-num_epochs:])
        }
        
        return stats
    
    def save(self, path: str):
        """Save model"""
        torch.save({
            'policy': self.policy.state_dict(),
            'optimizer': self.optimizer.state_dict()
        }, path)
    
    def load(self, path: str):
        """Load model"""
        checkpoint = torch.load(path)
        self.policy.load_state_dict(checkpoint['policy'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
```

---

### 2.3 Actor-Critic Networks (`networks.py`)

```python
import torch
import torch.nn as nn
from typing import Dict, Tuple

class ActorCritic(nn.Module):
    """
    Actor-Critic Network
    
    Architecture:
        Image → CNN → features
        Scalars → MLP → features
        Concat → Shared → Actor (μ, σ) / Critic (V)
    """
    
    def __init__(self, obs_space, action_space):
        super().__init__()
        
        self.action_dim = action_space.shape[0]
        
        # CNN for image (shared)
        self.cnn = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(64 * 7 * 7, 256),
            nn.ReLU()
        )
        
        # MLP for scalars
        scalar_dim = 1 + 1 + 1 + 1 + 1 + 5*2  # vel, steer, lat, head, dist, prev_actions
        self.mlp = nn.Sequential(
            nn.Linear(scalar_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU()
        )
        
        # Shared layers
        self.shared = nn.Sequential(
            nn.Linear(256 + 128, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU()
        )
        
        # Actor head (policy)
        self.actor_mean = nn.Linear(128, self.action_dim)
        self.actor_log_std = nn.Parameter(torch.zeros(self.action_dim))
        
        # Critic head (value)
        self.critic = nn.Linear(128, 1)
    
    def forward(
        self,
        obs: Dict[str, torch.Tensor]
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Forward pass
        
        Returns:
            action_mean, action_std, value
        """
        # Image features
        image = obs['image'].float() / 255.0  # Normalize
        cnn_features = self.cnn(image)
        
        # Scalar features
        scalars = torch.cat([
            obs['velocity'],
            obs['steering'],
            obs['lateral_offset'],
            obs['heading_error'],
            obs['distance_to_obstacle'],
            obs['prev_actions'].flatten(start_dim=1)
        ], dim=1)
        mlp_features = self.mlp(scalars)
        
        # Combine
        combined = torch.cat([cnn_features, mlp_features], dim=1)
        shared_features = self.shared(combined)
        
        # Actor
        action_mean = torch.tanh(self.actor_mean(shared_features))
        action_std = torch.exp(self.actor_log_std).expand_as(action_mean)
        
        # Critic
        value = self.critic(shared_features)
        
        return action_mean, action_std, value
```

---

### 2.4 Curiosity Module (`icm.py`)

```python
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Tuple

class IntrinsicCuriosityModule(nn.Module):
    """
    Intrinsic Curiosity Module (ICM)
    
    Paper: "Curiosity-driven Exploration" (Pathak et al., 2017)
    
    Components:
        1. Feature Network: φ(s)
        2. Inverse Model: φ(s_t), φ(s_{t+1}) → â_t
        3. Forward Model: φ(s_t), a_t → φ̂(s_{t+1})
    
    Intrinsic Reward:
        r_i = ||φ̂(s_{t+1}) - φ(s_{t+1})||²
    """
    
    def __init__(
        self,
        obs_space,
        action_dim: int,
        feature_dim: int = 256,
        lr: float = 1e-3,
        device: str = 'cuda'
    ):
        super().__init__()
        
        self.action_dim = action_dim
        self.feature_dim = feature_dim
        self.device = device
        
        # Feature network (shared CNN)
        self.feature_net = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(64 * 7 * 7, feature_dim)
        ).to(device)
        
        # Inverse model: (φ_t, φ_{t+1}) → action
        self.inverse_model = nn.Sequential(
            nn.Linear(feature_dim * 2, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim),
            nn.Tanh()
        ).to(device)
        
        # Forward model: (φ_t, action) → φ_{t+1}
        self.forward_model = nn.Sequential(
            nn.Linear(feature_dim + action_dim, 256),
            nn.ReLU(),
            nn.Linear(256, feature_dim)
        ).to(device)
        
        # Optimizer
        self.optimizer = optim.Adam(self.parameters(), lr=lr)
    
    def get_features(self, obs: torch.Tensor) -> torch.Tensor:
        """Extract features from observation"""
        if obs.dtype == torch.uint8:
            obs = obs.float() / 255.0
        return self.feature_net(obs)
    
    def compute_intrinsic_reward(
        self,
        obs: torch.Tensor,
        next_obs: torch.Tensor,
        action: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute intrinsic reward (prediction error)
        
        Returns:
            r_i: (batch_size,)
        """
        with torch.no_grad():
            # Features
            phi_t = self.get_features(obs)
            phi_t1 = self.get_features(next_obs)
            
            # Forward prediction
            phi_t1_pred = self.forward_model(torch.cat([phi_t, action], dim=1))
            
            # Prediction error (L2 norm)
            prediction_error = (phi_t1 - phi_t1_pred).pow(2).sum(dim=1)
        
        return prediction_error
    
    def update(
        self,
        obs: torch.Tensor,
        next_obs: torch.Tensor,
        action: torch.Tensor
    ) -> Tuple[float, float]:
        """
        Update ICM
        
        Returns:
            inverse_loss, forward_loss
        """
        # Features
        phi_t = self.get_features(obs)
        phi_t1 = self.get_features(next_obs)
        
        # Inverse model loss
        action_pred = self.inverse_model(torch.cat([phi_t, phi_t1], dim=1))
        inverse_loss = nn.functional.mse_loss(action_pred, action)
        
        # Forward model loss
        phi_t1_pred = self.forward_model(torch.cat([phi_t, action], dim=1))
        forward_loss = 0.5 * nn.functional.mse_loss(phi_t1_pred, phi_t1.detach())
        
        # Total loss
        loss = inverse_loss + forward_loss
        
        # Optimization
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return inverse_loss.item(), forward_loss.item()
```

---

## 3. 학습 스크립트 (`train.py`)

```python
"""
PPO Training Script with Curiosity
"""
import argparse
import numpy as np
import torch
from pathlib import Path

from src.environment import RCTrackEnv
from src.agent import PPOAgent
from src.curiosity import IntrinsicCuriosityModule
from src.utils import Logger, Config

def train(config: Config):
    """Main training loop"""
    
    # Create environment
    env = RCTrackEnv(
        track_type=config.track_type,
        render_mode=config.render_mode
    )
    
    # Create agent
    agent = PPOAgent(
        obs_space=env.observation_space,
        action_space=env.action_space,
        lr=config.lr,
        gamma=config.gamma,
        device=config.device
    )
    
    # Create curiosity module
    curiosity = IntrinsicCuriosityModule(
        obs_space=env.observation_space,
        action_dim=env.action_space.shape[0],
        device=config.device
    )
    
    # Logger
    logger = Logger(config.log_dir)
    
    # Training loop
    episode = 0
    total_steps = 0
    
    while total_steps < config.max_timesteps:
        # Collect trajectories
        trajectories = []
        episode_reward = 0
        
        obs, _ = env.reset()
        done = False
        
        while not done and len(trajectories) < config.batch_size:
            # Select action
            action, log_prob, value = agent.select_action(obs)
            
            # Environment step
            next_obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            # Intrinsic reward
            obs_tensor = torch.FloatTensor(obs['image']).unsqueeze(0).to(config.device)
            next_obs_tensor = torch.FloatTensor(next_obs['image']).unsqueeze(0).to(config.device)
            action_tensor = torch.FloatTensor(action).unsqueeze(0).to(config.device)
            
            intrinsic_reward = curiosity.compute_intrinsic_reward(
                obs_tensor, next_obs_tensor, action_tensor
            ).item()
            
            # Combined reward
            total_reward = reward + config.curiosity_beta * intrinsic_reward
            
            # Store transition
            trajectories.append({
                'obs': obs,
                'action': action,
                'reward': total_reward,
                'next_obs': next_obs,
                'done': done,
                'log_prob': log_prob,
                'value': value
            })
            
            episode_reward += reward
            total_steps += 1
            obs = next_obs
        
        # PPO update
        stats = agent.update(trajectories, num_epochs=config.num_epochs)
        
        # Curiosity update
        for t in trajectories:
            obs_t = torch.FloatTensor(t['obs']['image']).unsqueeze(0).to(config.device)
            next_obs_t = torch.FloatTensor(t['next_obs']['image']).unsqueeze(0).to(config.device)
            action_t = torch.FloatTensor(t['action']).unsqueeze(0).to(config.device)
            
            inv_loss, fwd_loss = curiosity.update(obs_t, next_obs_t, action_t)
        
        # Log
        logger.log({
            'episode': episode,
            'total_steps': total_steps,
            'episode_reward': episode_reward,
            **stats
        })
        
        # Save checkpoint
        if episode % config.save_interval == 0:
            agent.save(f'{config.checkpoint_dir}/agent_{episode}.pt')
        
        episode += 1
    
    env.close()

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=str, default='config/ppo_config.yaml')
    args = parser.parse_args()
    
    config = Config.from_yaml(args.config)
    train(config)
```

---

## 4. 테스트 명세

### 4.1 Unit Tests

```python
# tests/test_environment.py
def test_reset():
    """Test environment reset"""
    env = RCTrackEnv()
    obs, info = env.reset()
    assert 'image' in obs
    assert obs['image'].shape == (3, 84, 84)

def test_step():
    """Test environment step"""
    env = RCTrackEnv()
    obs, _ = env.reset()
    action = env.action_space.sample()
    next_obs, reward, terminated, truncated, info = env.step(action)
    assert isinstance(reward, float)
```

---

**작성자:** AI Implementation Team  
**다음 문서:** 03_검증서.md
