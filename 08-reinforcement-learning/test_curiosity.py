"""
Curiosity Module ÌÖåÏä§Ìä∏
ICM (Intrinsic Curiosity Module) Í≤ÄÏ¶ù
"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

import numpy as np
import torch

print("="*80)
print("Module 08: Curiosity Module - Fact Check")
print("="*80)

# Test 1: Module Import
print("\n[Test 1] Curiosity Module Import")
try:
    from src.curiosity import IntrinsicCuriosityModule
    from src.environment import RCTrackEnv
    
    print("  ‚úÖ Import successful")
    
except Exception as e:
    print(f"  ‚ùå FAIL: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# Test 2: ICM Initialization
print("\n[Test 2] ICM Initialization")
try:
    icm = IntrinsicCuriosityModule(
        feature_dim=256,
        action_dim=2,
        device='cpu'
    )
    
    print(f"  ‚úÖ ICM created")
    
    # Count parameters
    feature_params = sum(p.numel() for p in icm.feature_net.parameters())
    inverse_params = sum(p.numel() for p in icm.inverse_model.parameters())
    forward_params = sum(p.numel() for p in icm.forward_model.parameters())
    total_params = sum(p.numel() for p in icm.parameters())
    
    print(f"     Feature Network: {feature_params:,} params")
    print(f"     Inverse Model: {inverse_params:,} params")
    print(f"     Forward Model: {forward_params:,} params")
    print(f"     Total: {total_params:,} params")
    
    print("  ‚úÖ PASS: ICM initialization")
    
except Exception as e:
    print(f"  ‚ùå FAIL: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# Test 3: Feature Encoding
print("\n[Test 3] Feature Encoding")
try:
    icm = IntrinsicCuriosityModule(device='cpu')
    
    # Dummy image
    image = torch.randint(0, 256, (2, 3, 84, 84), dtype=torch.uint8)
    
    # Encode
    features = icm.feature_net(image.float() / 255.0)
    
    print(f"  ‚úÖ Feature encoding")
    print(f"     Input: {image.shape}")
    print(f"     Output: {features.shape}")
    
    assert features.shape == (2, 256)
    assert not torch.isnan(features).any()
    
    print("  ‚úÖ PASS: Feature encoding")
    
except Exception as e:
    print(f"  ‚ùå FAIL: {e}")
    import traceback
    traceback.print_exc()

# Test 4: Inverse Model
print("\n[Test 4] Inverse Model")
try:
    icm = IntrinsicCuriosityModule(device='cpu')
    
    # Dummy features
    phi_t = torch.randn(2, 256)
    phi_t1 = torch.randn(2, 256)
    
    # Predict action
    action_pred = icm.inverse_model(phi_t, phi_t1)
    
    print(f"  ‚úÖ Inverse model")
    print(f"     Input: œÜ(s_t) {phi_t.shape}, œÜ(s_t+1) {phi_t1.shape}")
    print(f"     Output: √¢_t {action_pred.shape}")
    
    assert action_pred.shape == (2, 2)
    assert not torch.isnan(action_pred).any()
    
    print("  ‚úÖ PASS: Inverse model")
    
except Exception as e:
    print(f"  ‚ùå FAIL: {e}")
    import traceback
    traceback.print_exc()

# Test 5: Forward Model
print("\n[Test 5] Forward Model")
try:
    icm = IntrinsicCuriosityModule(device='cpu')
    
    # Dummy inputs
    phi_t = torch.randn(2, 256)
    action = torch.randn(2, 2)
    
    # Predict next state
    phi_t1_pred = icm.forward_model(phi_t, action)
    
    print(f"  ‚úÖ Forward model")
    print(f"     Input: œÜ(s_t) {phi_t.shape}, a_t {action.shape}")
    print(f"     Output: œÜÃÇ(s_t+1) {phi_t1_pred.shape}")
    
    assert phi_t1_pred.shape == (2, 256)
    assert not torch.isnan(phi_t1_pred).any()
    
    print("  ‚úÖ PASS: Forward model")
    
except Exception as e:
    print(f"  ‚ùå FAIL: {e}")
    import traceback
    traceback.print_exc()

# Test 6: Intrinsic Reward Computation
print("\n[Test 6] Intrinsic Reward Computation")
try:
    env = RCTrackEnv()
    icm = IntrinsicCuriosityModule(device='cpu')
    
    obs, _ = env.reset()
    action = env.action_space.sample()
    next_obs, _, _, _, _ = env.step(action)
    
    # Convert to tensors
    obs_tensor = torch.FloatTensor(obs['image']).unsqueeze(0)
    next_obs_tensor = torch.FloatTensor(next_obs['image']).unsqueeze(0)
    action_tensor = torch.FloatTensor(action).unsqueeze(0)
    
    # Compute intrinsic reward
    intrinsic_reward = icm.compute_intrinsic_reward(
        obs_tensor, next_obs_tensor, action_tensor
    )
    
    print(f"  ‚úÖ Intrinsic reward computed")
    print(f"     Reward: {intrinsic_reward.item():.4f}")
    print(f"     Shape: {intrinsic_reward.shape}")
    
    assert intrinsic_reward.shape == (1,)
    assert intrinsic_reward.item() >= 0
    assert not torch.isnan(intrinsic_reward).any()
    
    print("  ‚úÖ PASS: Intrinsic reward computation")
    
except Exception as e:
    print(f"  ‚ùå FAIL: {e}")
    import traceback
    traceback.print_exc()

# Test 7: ICM Update
print("\n[Test 7] ICM Update (Learning)")
try:
    env = RCTrackEnv()
    icm = IntrinsicCuriosityModule(device='cpu')
    
    # Collect transitions
    obs, _ = env.reset()
    
    obs_list = []
    next_obs_list = []
    action_list = []
    
    for _ in range(10):
        action = env.action_space.sample()
        next_obs, _, terminated, truncated, _ = env.step(action)
        
        obs_list.append(obs['image'])
        next_obs_list.append(next_obs['image'])
        action_list.append(action)
        
        obs = next_obs
        if terminated or truncated:
            obs, _ = env.reset()
    
    # Batch
    obs_batch = torch.FloatTensor(np.array(obs_list))
    next_obs_batch = torch.FloatTensor(np.array(next_obs_list))
    action_batch = torch.FloatTensor(np.array(action_list))
    
    # Update ICM
    inv_loss, fwd_loss = icm.update(obs_batch, next_obs_batch, action_batch)
    
    print(f"  ‚úÖ ICM update executed")
    print(f"     Inverse loss: {inv_loss:.4f}")
    print(f"     Forward loss: {fwd_loss:.4f}")
    
    assert isinstance(inv_loss, float)
    assert isinstance(fwd_loss, float)
    assert inv_loss >= 0
    assert fwd_loss >= 0
    
    print("  ‚úÖ PASS: ICM update")
    
except Exception as e:
    print(f"  ‚ùå FAIL: {e}")
    import traceback
    traceback.print_exc()

# Test 8: Curiosity Effect (New vs Familiar)
print("\n[Test 8] Curiosity Effect (New vs Familiar)")
try:
    env = RCTrackEnv()
    icm = IntrinsicCuriosityModule(device='cpu')
    
    obs, _ = env.reset()
    
    # Same transition repeated
    action = np.array([0.5, 0.5])
    
    rewards = []
    for step in range(20):
        next_obs, _, terminated, truncated, _ = env.step(action)
        
        obs_tensor = torch.FloatTensor(obs['image']).unsqueeze(0)
        next_obs_tensor = torch.FloatTensor(next_obs['image']).unsqueeze(0)
        action_tensor = torch.FloatTensor(action).unsqueeze(0)
        
        # Intrinsic reward
        r_i = icm.compute_intrinsic_reward(obs_tensor, next_obs_tensor, action_tensor)
        rewards.append(r_i.item())
        
        # Update ICM
        icm.update(obs_tensor, next_obs_tensor, action_tensor)
        
        obs = next_obs
        if terminated or truncated:
            obs, _ = env.reset()
    
    initial_reward = np.mean(rewards[:5])
    final_reward = np.mean(rewards[-5:])
    
    print(f"  ‚úÖ Curiosity effect observed")
    print(f"     Initial reward (new): {initial_reward:.4f}")
    print(f"     Final reward (familiar): {final_reward:.4f}")
    print(f"     Reduction: {(1 - final_reward/initial_reward)*100:.1f}%")
    
    # Curiosity should decrease over time
    assert final_reward < initial_reward, "Curiosity should decrease for repeated experiences"
    
    print("  ‚úÖ PASS: Curiosity decreases for familiar states")
    
except Exception as e:
    print(f"  ‚ùå FAIL: {e}")
    import traceback
    traceback.print_exc()

# Test 9: Integration with Environment
print("\n[Test 9] Integration Test (PPO + Curiosity)")
try:
    from src.agent import PPOAgent
    
    env = RCTrackEnv()
    agent = PPOAgent(env.observation_space, env.action_space, device='cpu')
    icm = IntrinsicCuriosityModule(device='cpu')
    
    obs, _ = env.reset()
    
    total_extrinsic = 0
    total_intrinsic = 0
    
    for step in range(50):
        # Agent action
        action, log_prob, value = agent.select_action(obs)
        next_obs, reward, terminated, truncated, _ = env.step(action)
        
        # Intrinsic reward
        obs_tensor = torch.FloatTensor(obs['image']).unsqueeze(0)
        next_obs_tensor = torch.FloatTensor(next_obs['image']).unsqueeze(0)
        action_tensor = torch.FloatTensor(action).unsqueeze(0)
        
        intrinsic_reward = icm.compute_intrinsic_reward(
            obs_tensor, next_obs_tensor, action_tensor
        ).item()
        
        # Combined reward
        combined_reward = reward + 0.2 * intrinsic_reward
        
        total_extrinsic += reward
        total_intrinsic += intrinsic_reward
        
        obs = next_obs
        if terminated or truncated:
            break
    
    print(f"  ‚úÖ PPO + Curiosity integration")
    print(f"     Steps: {step + 1}")
    print(f"     Total extrinsic reward: {total_extrinsic:.2f}")
    print(f"     Total intrinsic reward: {total_intrinsic:.2f}")
    print(f"     Combined: {total_extrinsic + 0.2 * total_intrinsic:.2f}")
    
    print("  ‚úÖ PASS: Integration works")
    
except Exception as e:
    print(f"  ‚ùå FAIL: {e}")
    import traceback
    traceback.print_exc()

# Summary
print("\n" + "="*80)
print("üìä Curiosity Module Test Summary")
print("="*80)
print("""
‚úÖ Test 1: Module import
‚úÖ Test 2: ICM initialization
‚úÖ Test 3: Feature encoding
‚úÖ Test 4: Inverse model
‚úÖ Test 5: Forward model
‚úÖ Test 6: Intrinsic reward computation
‚úÖ Test 7: ICM update (learning)
‚úÖ Test 8: Curiosity effect (new vs familiar)
‚úÖ Test 9: Integration with PPO agent

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚úÖ Curiosity Module ÏôÑÏ†Ñ ÏûëÎèô!

ÌïµÏã¨ Í≤ÄÏ¶ù Ìï≠Î™©:
  1. ‚úÖ Feature Network (CNN encoding)
  2. ‚úÖ Inverse Model (action prediction)
  3. ‚úÖ Forward Model (next state prediction)
  4. ‚úÖ Intrinsic Reward (prediction error)
  5. ‚úÖ ICM Learning (loss Í≥ÑÏÇ∞ Î∞è update)
  6. ‚úÖ Curiosity Decay (Î∞òÎ≥µ Í≤ΩÌóòÏãú Í∞êÏÜå)
  7. ‚úÖ PPO Integration (combined reward)

ÏõêÎ¶¨ ÌôïÏù∏:
  - ÏÉàÎ°úÏö¥ Í≤ΩÌóò ‚Üí ÏòàÏ∏° Ïò§Ï∞® ÌÅº ‚Üí ÎÜíÏùÄ intrinsic reward ‚úÖ
  - Î∞òÎ≥µ Í≤ΩÌóò ‚Üí ÏòàÏ∏° Ïò§Ï∞® ÏûëÏùå ‚Üí ÎÇÆÏùÄ intrinsic reward ‚úÖ
  - ICM ÌïôÏäµÏúºÎ°ú ÏòàÏ∏° Ï†ïÌôïÎèÑ Ìñ•ÏÉÅ ‚úÖ

Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ Í∞ÄÏπò:
  ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê 2026ÎÖÑ ÏµúÏã† RL Í∏∞Î≤ï
  ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Exploration Î¨∏Ï†ú Ìï¥Í≤∞
  ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ÌïôÏà†/Ïã§Î¨¥ Î™®Îëê Ï†ÅÏö© Í∞ÄÎä•

Module 08 ÏôÑÏ†ÑÏ≤¥ ÏôÑÏÑ±! üéâ
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
""")
print("="*80)
